[
["index.html", "Script repository (Hench et al. supplement) 1 Intro 1.1 Analysis 1.2 Prerequesites 1.3 Figures", " Script repository (Hench et al. supplement) Kosmas Hench 2020-06-19 1 Intro This repository contains the complete workflow used in the paper “Ancestral variation, hybridization and modularity fuel a marine radiation”. The individual chapters of this documentation follow the separate main steps of the workflow, which each refer to an individual prefix in the git x.x references of the papers method section. The individual steps partly depend on each other - especially git 1 - git 3 should be executed in order and before the other steps. 1.1 Analysis A documentation of the data preparation and the data analysis (git 1.x - 10.x) can be found at: git 1.x: Genotyping git 2.x: Genotyping all base pairs git 3.x: Analysis (FST &amp; GxP) git 4.x: Analysis (dXY &amp; \\(\\pi\\)) git 5.x: Analysis (phylogeny &amp; topolgy weighting) git 6.x: Analysis (\\(\\rho\\)) git 7.x: Analysis (population trees) git 8.x: Analysis (demographic history) git 9.x: Analysis (hybridization) git 10.x: Analysis (admixture) 1.2 Prerequesites All scripts assume two variables to be set within the bash environment: $BASE_DIR is assumed to point to the base folder of this repository $SFTWR is a folder that contains all the software dependencies that are used within the scripts The analysis is controlled using the workflow manager nextflow and uses slightly different configurations across the individual pipelines. The exact commands used to execute the analysis during the development of the publication are stored within the aliases set within sh/nextflow_alias.sh. Furthermore, external dependencies need to be downloaded and deployed at the expected places (s. README.md at the ressources folder). 1.3 Figures The creation of the figures is bundled in a single script (git 11) which can be executed once all nextflow scripts have successfully run. cd $BASE_DIR bash sh/create_figures.sh This is basically just a collection that will run all scripts located under $BASE_DIR/R/fig. Under this location, you will find one R script per figure (and suppl. figure). So if you are only interested in a single figure - that is the place to start looking. Furthermore, a more detailed documentation exists for all the figure scripts used for the manuscript: F1, F2, F3, F4 and F5 as well as for all the supplementary figures: SF1, SF2, SF3, SF4, SF5, SF6, SF7, SF8 and SF9. "],
["git-1-genotyping-i-snps-only.html", "2 (git 1) Genotyping I (SNPs only) 2.1 Summary 2.2 Details of genotyping.nf", " 2 (git 1) Genotyping I (SNPs only) This pipeline can be executed as follows: cd $BASE_DIR/nf/01_genotyping source ../sh/nextflow_alias.sh nf_run_gatk 2.1 Summary The genotyping procedure is controlled by the nextflow script genotyping.nf (located under $BASE_DIR/nf/01_genotyping/). It takes the analysis from the raw sequencing data to the genotyped and phased SNPs. Below is an overview of the steps involved in the genotyping process. (The green dot indicates the raw data input, red arrows depict output that is exported for further use.) 2.2 Details of genotyping.nf 2.2.1 Data preparation The nextflow script starts with a small header and then opens the analysis by reading a table with meta data about the samples. The table is parsed and the values are stored in nextflow variables. #!/usr/bin/env nextflow /* =============================================================== Disclaimer: This pipeline needs a lot of time &amp; memory to run: All in all we used roughly 10 TB and ran for about 1 Month (mainly due to limited bandwidth on the cluster durint the &quot;receive_tuple step) =============================================================== */ // git 1.1 /* open the pipeline based on the metadata spread sheet that includes all information necessary to assign read groups to the sequencing data, split the spread sheet by row and feed it into a channel */ Channel .fromPath(&#39;../../metadata/file_info.txt&#39;) .splitCsv(header:true, sep:&quot;\\t&quot;) .map{ row -&gt; [ id:row.id, label:row.label, file_fwd:row.file_fwd, file_rev:row.file_rev, flowcell_id_fwd:row.flowcell_id_fwd, lane_fwd:row.lane_fwd, company:row.company] } .set { samples_ch } Below is a little preview of the table containing the sample meta data: id label spec geo date coord_N coord_W company .. 16_21-30 16_21-30nigpan nig pan 2016 NA NA duke … 16_21-30 16_21-30nigpan nig pan 2016 NA NA duke … 16_31-40 16_31-40unipan uni pan 2016 NA NA duke … 16_31-40 16_31-40unipan uni pan 2016 NA NA duke … 17996 17996indbel ind bel 2004-07-27 16.801 -88.079 novogene … 17997 17997indbel ind bel 2004-07-27 16.801 -88.079 novogene … … … … … … … … … The first step to prepare the data for the GATK best practices, is to convert the sample sequences from *.fq to *.bam format to assign read groups: // git 1.2 /* for every sequencing file, convert into ubam format and assign read groups */ process split_samples { label &#39;L_20g2h_split_samples&#39; input: val x from samples_ch output: set val( &quot;${x.label}.${x.lane_fwd}&quot; ), file( &quot;${x.label}.${x.lane_fwd}.ubam.bam&quot; ) into ubams_mark, ubams_merge script: &quot;&quot;&quot; echo -e &quot;---------------------------------&quot; echo -e &quot;Label:\\t\\t${x.label}\\nFwd:\\t\\t${x.file_fwd}\\nRev:\\t\\t${x.file_rev}&quot; echo -e &quot;Flowcell:\\t${x.flowcell_id_fwd}\\nLane:\\t\\t${x.lane_fwd}&quot; echo -e &quot;Read group:\\t${x.flowcell_id_fwd}.${x.lane_fwd}\\nCompany:\\t${x.company}&quot; mkdir -p \\$BASE_DIR/temp_files gatk --java-options &quot;-Xmx20G&quot; \\ FastqToSam \\ -SM=${x.label} \\ -F1=\\$BASE_DIR/data/seqdata/${x.file_fwd} \\ -F2=\\$BASE_DIR/data/seqdata/${x.file_rev} \\ -O=${x.label}.${x.lane_fwd}.ubam.bam \\ -RG=${x.label}.${x.lane_fwd} \\ -LB=${x.label}&quot;.lib1&quot; \\ -PU=${x.flowcell_id_fwd}.${x.lane_fwd} \\ -PL=Illumina \\ -CN=${x.company} \\ --TMP_DIR=\\$BASE_DIR/temp_files; &quot;&quot;&quot; } The second step is marking the Illumina adapters. // git 1.3 /* for every ubam file, mark Illumina adapters */ process mark_adapters { label &#39;L_20g2h_mark_adapters&#39; tag &quot;${sample}&quot; input: set val( sample ), file( input ) from ubams_mark output: set val( sample ), file( &quot;*.adapter.bam&quot;) into adapter_bams file &quot;*.adapter.metrics.txt&quot; into adapter_metrics script: &quot;&quot;&quot; gatk --java-options &quot;-Xmx18G&quot; \\ MarkIlluminaAdapters \\ -I=${input} \\ -O=${sample}.adapter.bam \\ -M=${sample}.adapter.metrics.txt \\ -TMP_DIR=\\$BASE_DIR/temp_files; &quot;&quot;&quot; } We need to pass on the unaligned .bam file and the file containing the adapter information together, so the output of the first two processes are matched by the combined sample and sequencing lane information. // git 1.4 adapter_bams .combine(ubams_merge, by:0) .set {merge_input} For the actual mapping, the sequences are transformed back into .fq format, aligned using bwa and merged back with their original read group information. // git 1.5 /* this step includes a 3 step pipeline: * - re-transformatikon into fq format * - mapping aginst the reference genome_file * - merging with the basuch ubams to include read group information */ process map_and_merge { label &#39;L_75g24h8t_map_and_merge&#39; tag &quot;${sample}&quot; input: set val( sample ), file( adapter_bam_input ), file( ubam_input ) from merge_input output: set val( sample ), file( &quot;*.mapped.bam&quot; ) into mapped_bams script: &quot;&quot;&quot; set -o pipefail gatk --java-options &quot;-Xmx68G&quot; \\ SamToFastq \\ -I=${adapter_bam_input} \\ -FASTQ=/dev/stdout \\ -INTERLEAVE=true \\ -NON_PF=true \\ -TMP_DIR=\\$BASE_DIR/temp_files | \\ bwa mem -M -t 8 -p \\$BASE_DIR/ressources/HP_genome_unmasked_01.fa /dev/stdin | gatk --java-options &quot;-Xmx68G&quot; \\ MergeBamAlignment \\ --VALIDATION_STRINGENCY SILENT \\ --EXPECTED_ORIENTATIONS FR \\ --ATTRIBUTES_TO_RETAIN X0 \\ -ALIGNED_BAM=/dev/stdin \\ -UNMAPPED_BAM=${ubam_input} \\ -OUTPUT=${sample}.mapped.bam \\ --REFERENCE_SEQUENCE=\\$BASE_DIR/ressources/HP_genome_unmasked_01.fa.gz \\ -PAIRED_RUN true \\ --SORT_ORDER &quot;unsorted&quot; \\ --IS_BISULFITE_SEQUENCE false \\ --ALIGNED_READS_ONLY false \\ --CLIP_ADAPTERS false \\ --MAX_RECORDS_IN_RAM 2000000 \\ --ADD_MATE_CIGAR true \\ --MAX_INSERTIONS_OR_DELETIONS -1 \\ --PRIMARY_ALIGNMENT_STRATEGY MostDistant \\ --UNMAPPED_READ_STRATEGY COPY_TO_TAG \\ --ALIGNER_PROPER_PAIR_FLAGS true \\ --UNMAP_CONTAMINANT_READS true \\ -TMP_DIR=\\$BASE_DIR/temp_files &quot;&quot;&quot; } Next, the duplicates are being marked. // git 1.6 /* for every mapped sample,sort and mark duplicates * (intermediate step is required to create .bai file) */ process mark_duplicates { label &#39;L_32g30h_mark_duplicates&#39; publishDir &quot;../../1_genotyping/0_sorted_bams/&quot;, mode: &#39;symlink&#39; tag &quot;${sample}&quot; input: set val( sample ), file( input ) from mapped_bams output: set val { sample - ~/\\.(\\d+)/ }, val( sample ), file( &quot;*.dedup.bam&quot;) into dedup_bams file &quot;*.dedup.metrics.txt&quot; into dedup_metrics script: &quot;&quot;&quot; set -o pipefail gatk --java-options &quot;-Xmx30G&quot; \\ SortSam \\ -I=${input} \\ -O=/dev/stdout \\ --SORT_ORDER=&quot;coordinate&quot; \\ --CREATE_INDEX=false \\ --CREATE_MD5_FILE=false \\ -TMP_DIR=\\$BASE_DIR/temp_files \\ | \\ gatk --java-options &quot;-Xmx30G&quot; \\ SetNmAndUqTags \\ --INPUT=/dev/stdin \\ --OUTPUT=intermediate.bam \\ --CREATE_INDEX=true \\ --CREATE_MD5_FILE=true \\ -TMP_DIR=\\$BASE_DIR/temp_files \\ --REFERENCE_SEQUENCE=\\$BASE_DIR/ressources/HP_genome_unmasked_01.fa.gz gatk --java-options &quot;-Xmx30G&quot; \\ MarkDuplicates \\ -I=intermediate.bam \\ -O=${sample}.dedup.bam \\ -M=${sample}.dedup.metrics.txt \\ -MAX_FILE_HANDLES=1000 \\ -TMP_DIR=\\$BASE_DIR/temp_files rm intermediate* &quot;&quot;&quot; } As a preparation for the actual genotyping, the .bam files are being indexed. // git 1.7 /* index al bam files */ process index_bam { label &#39;L_32g1h_index_bam&#39; tag &quot;${sample}&quot; input: set val( sample ), val( sample_lane ), file( input ) from dedup_bams output: set val( sample ), val( sample_lane ), file( input ), file( &quot;*.bai&quot;) into ( indexed_bams, pir_bams ) script: &quot;&quot;&quot; gatk --java-options &quot;-Xmx30G&quot; \\ BuildBamIndex \\ -INPUT=${input} &quot;&quot;&quot; } At this point the preparation of the sequencing is done and we can start with the genotyping. (The output of the data preparation is split and one copy is later also used to prepare the read aware phasing in the process called extractPirs.) 2.2.2 Genotying Since some of our samples were split over several lanes, we now need to collect all .bam files for each sample. // git 1.8 /* collect all bam files for each sample */ indexed_bams .groupTuple() .set {tubbled} Now, we can create the genotype likelihoods for each individual sample. // git 1.9 /* create one *.g.vcf file per sample */ process receive_tuple { label &#39;L_36g47h_receive_tuple&#39; publishDir &quot;../../1_genotyping/1_gvcfs/&quot;, mode: &#39;symlink&#39; tag &quot;${sample}&quot; input: set sample, sample_lane, bam, bai from tubbled output: file( &quot;*.g.vcf.gz&quot;) into gvcfs file( &quot;*.vcf.gz.tbi&quot;) into tbis script: &quot;&quot;&quot; INPUT=\\$(echo ${bam} | sed &#39;s/\\\\[/-I /g; s/\\\\]//g; s/,/ -I/g&#39;) gatk --java-options &quot;-Xmx35g&quot; HaplotypeCaller \\ -R=\\$BASE_DIR/ressources/HP_genome_unmasked_01.fa \\ \\$INPUT \\ -O ${sample}.g.vcf.gz \\ -ERC GVCF &quot;&quot;&quot; } The individual genotype likelihoods are collected and combined for the entire data set. // git 1.10 /* collect and combine all *.g.vcf files */ process gather_gvcfs { label &#39;L_O88g90h_gather_gvcfs&#39; publishDir &quot;../../1_genotyping/1_gvcfs/&quot;, mode: &#39;symlink&#39; echo true input: file( gvcf ) from gvcfs.collect() file( tbi ) from tbis.collect() output: set file( &quot;cohort.g.vcf.gz&quot; ), file( &quot;cohort.g.vcf.gz.tbi&quot; ) into ( gcvf_snps, gvcf_acs, gvcf_indel ) script: &quot;&quot;&quot; GVCF=\\$(echo &quot; ${gvcf}&quot; | sed &#39;s/ /-V /g; s/vcf.gz/vcf.gz /g&#39;) gatk --java-options &quot;-Xmx85g&quot; \\ CombineGVCFs \\ -R=\\$BASE_DIR/ressources/HP_genome_unmasked_01.fa \\ \\$GVCF \\ -O cohort.g.vcf.gz &quot;&quot;&quot; } All samples are jointly genotyped. // git 1.11 /* actual genotyping step (varinat sites only) */ process joint_genotype_snps { label &#39;L_O88g90h_joint_genotype&#39; publishDir &quot;../../1_genotyping/2_raw_vcfs/&quot;, mode: &#39;symlink&#39; input: set file( vcf ), file( tbi ) from gcvf_snps output: set file( &quot;raw_var_sites.vcf.gz&quot; ), file( &quot;raw_var_sites.vcf.gz.tbi&quot; ) into ( raw_var_sites, raw_var_sites_to_metrics ) script: &quot;&quot;&quot; gatk --java-options &quot;-Xmx85g&quot; \\ GenotypeGVCFs \\ -R=\\$BASE_DIR/ressources/HP_genome_unmasked_01.fa \\ -V=${vcf} \\ -O=intermediate.vcf.gz gatk --java-options &quot;-Xmx85G&quot; \\ SelectVariants \\ -R=\\$BASE_DIR/ressources/HP_genome_unmasked_01.fa \\ -V=intermediate.vcf.gz \\ --select-type-to-include=SNP \\ -O=raw_var_sites.vcf.gz rm intermediate.* &quot;&quot;&quot; } The output of this process is split and used to collect the genotype metrics to inform the hard filtering of SNPs and to pass on the genotypes to the process called filterSNPs. At this point we create a channel containing all 24 hamlet linkage groups (LGs). This is used later (in the process called extractPirs) since all LGs are phased separately and only located at this part of the script for historical reasons (sorry :/). // git 1.12 /* generate a LG channel */ Channel .from( (&#39;01&#39;..&#39;09&#39;) + (&#39;10&#39;..&#39;19&#39;) + (&#39;20&#39;..&#39;24&#39;) ) .into{ LG_ids1; LG_ids2 } The metrics of the raw genotypes are collected. // git 1.13 /* produce metrics table to determine filtering thresholds - ups forgot to extract SNPS first*/ process joint_genotype_metrics { label &#39;L_28g5h_genotype_metrics&#39; publishDir &quot;../../1_genotyping/2_raw_vcfs/&quot;, mode: &#39;move&#39; input: set file( vcf ), file( tbi ) from raw_var_sites_to_metrics output: file( &quot;${vcf}.table.txt&quot; ) into raw_metrics script: &quot;&quot;&quot; gatk --java-options &quot;-Xmx25G&quot; \\ VariantsToTable \\ --variant=${vcf} \\ --output=${vcf}.table.txt \\ -F=CHROM -F=POS -F=MQ \\ -F=QD -F=FS -F=MQRankSum -F=ReadPosRankSum \\ --show-filtered &quot;&quot;&quot; } Based on the thresholds derived from the genotype metrics, the genotypes are first tagged and then filtered. After this, the data is filtered for missingness and only bi-allelic SNPs are selected. // git 1.14 /* filter snps basaed on locus annotations, missingness and type (bi-allelic only) */ process filterSNPs { label &#39;L_78g10h_filter_Snps&#39; publishDir &quot;../../1_genotyping/3_gatk_filtered/&quot;, mode: &#39;symlink&#39; input: set file( vcf ), file( tbi ) from raw_var_sites output: set file( &quot;filterd_bi-allelic.vcf.gz&quot; ), file( &quot;filterd_bi-allelic.vcf.gz.tbi&quot; ) into filtered_snps script: &quot;&quot;&quot; gatk --java-options &quot;-Xmx75G&quot; \\ VariantFiltration \\ -R=\\$BASE_DIR/ressources/HP_genome_unmasked_01.fa \\ -V ${vcf} \\ -O=intermediate.vcf.gz \\ --filter-expression &quot;QD &lt; 2.5&quot; \\ --filter-name &quot;filter_QD&quot; \\ --filter-expression &quot;FS &gt; 25.0&quot; \\ --filter-name &quot;filter_FS&quot; \\ --filter-expression &quot;MQ &lt; 52.0 || MQ &gt; 65.0&quot; \\ --filter-name &quot;filter_MQ&quot; \\ --filter-expression &quot;MQRankSum &lt; -0.2 || MQRankSum &gt; 0.2&quot; \\ --filter-name &quot;filter_MQRankSum&quot; \\ --filter-expression &quot;ReadPosRankSum &lt; -2.0 || ReadPosRankSum &gt; 2.0 &quot; \\ --filter-name &quot;filter_ReadPosRankSum&quot; gatk --java-options &quot;-Xmx75G&quot; \\ SelectVariants \\ -R=\\$BASE_DIR/ressources/HP_genome_unmasked_01.fa \\ -V=intermediate.vcf.gz \\ -O=intermediate.filterd.vcf.gz \\ --exclude-filtered vcftools \\ --gzvcf intermediate.filterd.vcf.gz \\ --max-missing-count 17 \\ --max-alleles 2 \\ --stdout \\ --recode | \\ bgzip &gt; filterd_bi-allelic.vcf.gz tabix -p vcf filterd_bi-allelic.vcf.gz rm intermediate.* &quot;&quot;&quot; } At this point, the genotying is done. 2.2.3 Phasing To get from genotypes to haplotypes, we apply read-aware phasing using Shapeit. This takes the original sequencing reads into account, so in the first step the reads are screened for Phase Informative Reads (i.e. reads containing more than a single SNP). This needs to be done for each LG independently, so we first need to split the genotypes before running extractPIRs. // git 1.15 // extract phase informative reads from // alignments and SNPs process extractPirs { label &#39;L_78g10h_extract_pirs&#39; input: val( lg ) from LG_ids2 set val( sample ), val( sample_lane ), file( input ), file( index ) from pir_bams.collect() set file( vcf ), file( tbi ) from filtered_snps output: set val( lg ), file( &quot;filterd_bi-allelic.LG${lg}.vcf.gz&quot; ), file( &quot;filterd_bi-allelic.LG${lg}.vcf.gz.tbi&quot; ), file( &quot;PIRsList-LG${lg}.txt&quot; ) into pirs_lg script: &quot;&quot;&quot; LG=&quot;LG${lg}&quot; awk -v OFS=&#39;\\t&#39; -v dir=\\$PWD -v lg=\\$LG &#39;{print \\$1,dir&quot;/&quot;\\$2,lg}&#39; \\$BASE_DIR/metadata/bamlist_proto.txt &gt; bamlist.txt vcftools \\ --gzvcf ${vcf} \\ --chr \\$LG \\ --stdout \\ --recode | \\ bgzip &gt; filterd_bi-allelic.LG${lg}.vcf.gz tabix -p vcf filterd_bi-allelic.LG${lg}.vcf.gz extractPIRs \\ --bam bamlist.txt \\ --vcf filterd_bi-allelic.LG${lg}.vcf.gz \\ --out PIRsList-LG${lg}.txt \\ --base-quality 20 \\ --read-quality 15 &quot;&quot;&quot; } Using those PIRs, we can then proceed with the actual phasing. The resulting haplotypes are converted back into .vcf format. // git 1.16 // run the actual phasing process run_shapeit { label &#39;L_75g24h8t_run_shapeit&#39; input: set val( lg ), file( vcf ), file( tbi ), file( pirs ) from pirs_lg output: file( &quot;phased-LG${lg}.vcf.gz&quot; ) into phased_lgs script: &quot;&quot;&quot; LG=&quot;LG${lg}&quot; shapeit \\ -assemble \\ --input-vcf ${vcf} \\ --input-pir ${pirs} \\ --thread 8 \\ -O phased-LG${lg} shapeit \\ -convert \\ --input-hap phased-LG${lg} \\ --output-vcf phased-LG${lg}.vcf bgzip phased-LG${lg}.vcf &quot;&quot;&quot; } After the phasing, we merge the LGs back together to get a single data set. We export a comple data set as well as one that was filtered for a minor allele count of at least two. // git 1.17 // merge the phased LGs back together. // the resulting vcf file represents // the &#39;SNPs only&#39; data set process merge_phased { label &#39;L_28g5h_merge_phased_vcf&#39; publishDir &quot;../../1_genotyping/4_phased/&quot;, mode: &#39;move&#39; input: file( vcf ) from phased_lgs.collect() output: set file( &quot;phased.vcf.gz&quot; ), file( &quot;phased.vcf.gz.tbi&quot; ) into phased_vcf set file( &quot;phased_mac2.vcf.gz&quot; ), file( &quot;phased_mac2.vcf.gz.tbi&quot; ) into phased_mac2_vcf script: &quot;&quot;&quot; vcf-concat \\ phased-LG* | \\ grep -v ^\\$ | \\ tee phased.vcf | \\ vcftools --vcf - --mac 2 --recode --stdout | \\ bgzip &gt; phased_mac2.vcf.gz bgzip phased.vcf tabix -p vcf phased.vcf.gz tabix -p vcf phased_mac2.vcf.gz &quot;&quot;&quot; } Finally, we are done with the entire genotyping procedure for the SNPs olny data set. 2.2.4 Indel masks The genotyping.nf workflow contains an appendix that makes use of the genotyping likelihoods created in step git 1.10 to create an indel mask that is later used in the inference of the hamlet demographic history (git 8.x). (This part is excluded from the initial visualization of this script) We restart by reopening the joint-sample genotype likelyhoods file and calling the indels from it. /* ========================================= */ /* appendix: generate indel masks for msmc: */ // git 1.18 // reopen the gvcf file to also genotype indels process joint_genotype_indel { label &#39;L_O88g90h_genotype_indel&#39; publishDir &quot;../../1_genotyping/2_raw_vcfs/&quot;, mode: &#39;copy&#39; input: set file( vcf ), file( tbi ) from gvcf_indel output: set file( &quot;raw_var_indel.vcf.gz&quot; ), file( &quot;raw_var_indel.vcf.gz.tbi&quot; ) into ( raw_indel, raw_indel_to_metrics ) script: &quot;&quot;&quot; gatk --java-options &quot;-Xmx85g&quot; \\ GenotypeGVCFs \\ -R=\\$REF_GENOME \\ -V=${vcf} \\ -O=intermediate.vcf.gz gatk --java-options &quot;-Xmx85G&quot; \\ SelectVariants \\ -R=\\$REF_GENOME \\ -V=intermediate.vcf.gz \\ --select-type-to-include=INDEL \\ -O=raw_var_indel.vcf.gz rm intermediate.* &quot;&quot;&quot; } We export the the indel genotype metrics to determine cutoff values for the hard filtering step. // git 1.19 // export indel metrics for filtering process indel_metrics { label &#39;L_28g5h_genotype_metrics&#39; publishDir &quot;../../1_genotyping/2_raw_vcfs/&quot;, mode: &#39;copy&#39; input: set file( vcf ), file( tbi ) from raw_indel_to_metrics output: file( &quot;${vcf}.table.txt&quot; ) into raw_indel_metrics script: &quot;&quot;&quot; gatk --java-options &quot;-Xmx25G&quot; \\ VariantsToTable \\ --variant=${vcf} \\ --output=${vcf}.table.txt \\ -F=CHROM -F=POS -F=MQ \\ -F=QD -F=FS -F=MQRankSum -F=ReadPosRankSum \\ --show-filtered &quot;&quot;&quot; } Based on the exported metrics the genotypes are being filtered. // git 1.20 // hard filter indels and create mask process filterIndels { label &#39;L_78g10h_filter_indels&#39; publishDir &quot;../../1_genotyping/3_gatk_filtered/&quot;, mode: &#39;copy&#39; input: set file( vcf ), file( tbi ) from raw_indel output: set file( &quot;filterd.indel.vcf.gz&quot; ), file( &quot;filterd.indel.vcf.gz.tbi&quot; ) into filtered_indel file( &quot;indel_mask.bed.gz&quot; ) into indel_mask_ch /* FILTER THRESHOLDS NEED TO BE UPDATED */ script: &quot;&quot;&quot; gatk --java-options &quot;-Xmx75G&quot; \\ VariantFiltration \\ -R=\\$REF_GENOME \\ -V ${vcf} \\ -O=intermediate.vcf.gz \\ --filter-expression &quot;QD &lt; 2.5&quot; \\ --filter-name &quot;filter_QD&quot; \\ --filter-expression &quot;FS &gt; 25.0&quot; \\ --filter-name &quot;filter_FS&quot; \\ --filter-expression &quot;MQ &lt; 52.0 || MQ &gt; 65.0&quot; \\ --filter-name &quot;filter_MQ&quot; \\ --filter-expression &quot;SOR &gt; 3.0&quot; \\ --filter-name &quot;filter_SOR&quot; \\ --filter-expression &quot;InbreedingCoeff &lt; -0.25&quot; \\ --filter-name &quot;filter_InbreedingCoeff&quot; \\ --filter-expression &quot;MQRankSum &lt; -0.2 || MQRankSum &gt; 0.2&quot; \\ --filter-name &quot;filter_MQRankSum&quot; \\ --filter-expression &quot;ReadPosRankSum &lt; -2.0 || ReadPosRankSum &gt; 2.0 &quot; \\ --filter-name &quot;filter_ReadPosRankSum&quot; gatk --java-options &quot;-Xmx75G&quot; \\ SelectVariants \\ -R=\\$REF_GENOME \\ -V=intermediate.vcf.gz \\ -O=filterd.indel.vcf.gz \\ --exclude-filtered zcat filterd.indel.vcf.gz | \\ awk &#39;! /\\\\#/&#39; | \\ awk &#39;{if(length(\\$4) &gt; length(\\$5)) print \\$1&quot;\\\\t&quot;(\\$2-6)&quot;\\\\t&quot;(\\$2+length(\\$4)+4); else print \\$1&quot;\\\\t&quot;(\\$2-6)&quot;\\\\t&quot;(\\$2+length(\\$5)+4)}&#39; | \\ gzip -c &gt; indel_mask.bed.gz rm intermediate.* &quot;&quot;&quot; } Since we need one indel mask per linkage group, we create a channel of LGs. // git 1.21 /* create channel of linkage groups */ Channel .from( (&#39;01&#39;..&#39;09&#39;) + (&#39;10&#39;..&#39;19&#39;) + (&#39;20&#39;..&#39;24&#39;) ) .map{ &quot;LG&quot; + it } .into{ lg_ch } The linkage group channel is combined with the filtered indels. // git 1.22 // attach linkage groups to indel masks lg_ch.combine( filtered_indel ).set{ filtered_indel_lg } Finally, one mask per linkage group is created from the indel positions. // git 1.23 // split indel mask by linkage group process split_indel_mask { label &#39;L_loc_split_indel_mask&#39; publishDir &quot;../../ressources/indel_masks/&quot;, mode: &#39;copy&#39; input: set val( lg ), file( bed ) from filtered_indel_lg output: set val( lg ), file( &quot;indel_mask.${lg}.bed.gz &quot; ) into lg_indel_mask script: &quot;&quot;&quot; gzip -cd ${bed} | \\ grep ${lg} | \\ gzip -c &gt; indel_mask.${lg}.bed.gz &quot;&quot;&quot; } All indel masks are exported to the resources folder within the root directory. "],
["git-2-genotyping-ii-all-callable-sites.html", "3 (git 2) Genotyping II (all callable sites) 3.1 Summary 3.2 Details of genotyping_all_basepairs.nf", " 3 (git 2) Genotyping II (all callable sites) This pipeline can be executed as follows: cd $BASE_DIR/nf/02_genotyping_all_basepairs source ../sh/nextflow_alias.sh nf_run_allbp 3.1 Summary The genotyping procedure is controlled by the nextflow script genotyping_all_basepairs.nf (located under $BASE_DIR/nf/02_genotyping_all_basepairs/). Based on an intermediate step from genotyping.nf, this script produces a data set that includes all callable sites - that is SNPs as well a invariant sites that are covered by sequence. Below is an overview of the steps involved in the genotyping process. (The green dot indicates the data input, red arrows depict output that is exported for further use.) 3.2 Details of genotyping_all_basepairs.nf 3.2.1 Data preparation The nextflow script starts with a small header and then imports the joint genotyping likelihoods for all samples produced by genotyping_all_basepairs.nf. #!/usr/bin/env nextflow // git 2.1 // open genotype likelyhoods Channel .fromFilePairs(&quot;../../1_genotyping/1_gvcfs/cohort.g.vcf.{gz,gz.tbi}&quot;) .set{ vcf_cohort } The genotyping of the different linkage groups is going to happen in parallel, so we need to initialize a channel for the 24 LGs. // git 2.2 // initialize LG channel Channel .from( (&#39;01&#39;..&#39;09&#39;) + (&#39;10&#39;..&#39;19&#39;) + (&#39;20&#39;..&#39;24&#39;) ) .set{ ch_LG_ids } The genotyping likelihoods are combined, effectively linking the data set to the 24 parallel LGs. // git 2.3 // combine genotypes and LGs ch_LG_ids.combine( vcf_cohort ).set{ vcf_lg_combo } The samples are jointly genotyped, independently for each LG and including invariant sites. // git 2.4 // actual genotyping step (including invariant sites) process joint_genotype_snps { label &quot;L_O88g90h_LGs_genotype&quot; input: set val( lg ), vcfId, file( vcf ) from vcf_lg_combo output: set val( &#39;all&#39; ), val( lg ), file( &quot;all_site*.vcf.gz&quot; ), file( &quot;all_site*.vcf.gz.tbi&quot; ) into all_bp_by_location script: &quot;&quot;&quot; gatk --java-options &quot;-Xmx85g&quot; \\ GenotypeGVCFs \\ -R=\\$BASE_DIR/ressources/HP_genome_unmasked_01.fa \\ -L=LG${lg} \\ -V=${vcf[0]} \\ -O=intermediate.vcf.gz \\ --include-non-variant-sites=true gatk --java-options &quot;-Xmx85G&quot; \\ SelectVariants \\ -R=\\$BASE_DIR/ressources/HP_genome_unmasked_01.fa \\ -V=intermediate.vcf.gz \\ --select-type-to-exclude=INDEL \\ -O=all_sites.LG${lg}.vcf.gz rm intermediate.* &quot;&quot;&quot; } The genotypes of the different LGs are merged. // git 2.5 // merge all LGs process merge_genotypes { label &#39;L_78g5h_merge_genotypes&#39; echo true input: set val( dummy ), val( lg ), file( vcf ), file( tbi ) from all_bp_by_location.groupTuple() output: file( &quot;all_sites.vcf.gz&quot; ) into all_bp_merged script: &quot;&quot;&quot; INPUT=\\$(ls -1 *vcf.gz | sed &#39;s/^/ -I /g&#39; | cat \\$( echo )) gatk --java-options &quot;-Xmx85g&quot; \\ GatherVcfs \\ \\$INPUT \\ -O=all_sites.vcf.gz &quot;&quot;&quot; } The genotypes are hard filtered based on various genotyping scores. // git 2.6 // quality based filtering process filterSNP_first { label &#39;L_105g30h_filter_gt1&#39; input: file( vcf ) from all_bp_merged output: set file( &quot;intermediate.filterd.vcf.gz&quot; ), file( &quot;intermediate.filterd.vcf.gz.tbi&quot; ) into filtered_snps_first script: &quot;&quot;&quot; module load openssl1.0.2 tabix -p vcf ${vcf} gatk --java-options &quot;-Xmx75G&quot; \\ VariantFiltration \\ -R=\\$BASE_DIR/ressources/HP_genome_unmasked_01.fa \\ -V ${vcf} \\ -O=intermediate.vcf.gz \\ --filter-expression &quot;QD &lt; 2.5&quot; \\ --filter-name &quot;filter_QD&quot; \\ --filter-expression &quot;FS &gt; 25.0&quot; \\ --filter-name &quot;filter_FS&quot; \\ --filter-expression &quot;MQ &lt; 52.0 || MQ &gt; 65.0&quot; \\ --filter-name &quot;filter_MQ&quot; \\ --filter-expression &quot;MQRankSum &lt; -0.2 || MQRankSum &gt; 0.2&quot; \\ --filter-name &quot;filter_MQRankSum&quot; \\ --filter-expression &quot;ReadPosRankSum &lt; -2.0 || ReadPosRankSum &gt; 2.0 &quot; \\ --filter-name &quot;filter_ReadPosRankSum&quot; \\ --QUIET true &amp;&gt; var_filt.log gatk --java-options &quot;-Xmx75G&quot; \\ SelectVariants \\ -R=\\$BASE_DIR/ressources/HP_genome_unmasked_01.fa \\ -V=intermediate.vcf.gz \\ -O=intermediate.filterd.vcf.gz \\ --exclude-filtered \\ --QUIET true \\ --verbosity ERROR &amp;&gt; var_select.log &quot;&quot;&quot; } A second filtering is based on the missingness of samples. // git 2.7 // missingness based filtering // the resulting vcf file represents // the &#39;all BP&#39; data set process filterSNP_second { label &#39;L_105g30h_filter_gt2&#39; publishDir &quot;../../1_genotyping/3_gatk_filtered/&quot;, mode: &#39;copy&#39; input: set file( vcf ), file( tbi ) from filtered_snps_first output: file( &quot;filterd.allBP.vcf.gz&quot; ) into filtered_snps script: &quot;&quot;&quot; module load openssl1.0.2 vcftools \\ --gzvcf ${vcf} \\ --max-missing-count 17 \\ --stdout \\ --recode | \\ bgzip &gt; filterd.allBP.vcf.gz &quot;&quot;&quot; } Finally, we are done with the second version of genotyping. "],
["git-3-analysis-i-fst-gxp.html", "4 (git 3) Analysis I (FST &amp; GxP) 4.1 Summary 4.2 Details of analysis_fst_gxp.nf", " 4 (git 3) Analysis I (FST &amp; GxP) This pipeline can be executed as follows: cd $BASE_DIR/nf/03_analysis_fst_gxp source ../sh/nextflow_alias.sh nf_run_basic 4.1 Summary The genetic differentiation, as well as the genotype x phenotype association, are computed within the nextflow script analysis_fst_gxp.nf (located under $BASE_DIR/nf/03_analysis_fst_gxp/). It takes the SNPs only data set and computes FST and the GxP association. Below is an overview of the steps involved in the analysis. (The green dot indicates the genotype input, red arrows depict output that is exported for further use.) 4.2 Details of analysis_fst_gxp.nf 4.2.1 Setup The nextflow script starts by opening the genotype data and feeding it into three different streams. #!/usr/bin/env nextflow // git 3.1 // open genotype data Channel .fromFilePairs(&quot;../../1_genotyping/4_phased/phased_mac2.vcf.{gz,gz.tbi}&quot;) .into{ vcf_locations; vcf_filter; vcf_gxp; vcf_adapt } Since we are going to work on the three sampling locations independently, we create channel for the locations. // git 3.2 // initialize location channel Channel .from( &quot;bel&quot;, &quot;hon&quot;, &quot;pan&quot;) .set{ locations_ch } Then we attach the genotypes to the locations. // git 3.3 // attach genotypes to location locations_ch .combine( vcf_locations ) .set{ vcf_location_combo } Next, we define the species sets sampled at the individual locations. // git 3.4 // define location specific sepcies set Channel.from( [[1, &quot;ind&quot;], [2, &quot;may&quot;], [3, &quot;nig&quot;], [4, &quot;pue&quot;], [5, &quot;uni&quot;]] ).into{ bel_spec1_ch; bel_spec2_ch } Channel.from( [[1, &quot;abe&quot;], [2, &quot;gum&quot;], [3, &quot;nig&quot;], [4, &quot;pue&quot;], [5, &quot;ran&quot;], [6, &quot;uni&quot;]] ).into{ hon_spec1_ch; hon_spec2_ch } Channel.from( [[1, &quot;nig&quot;], [2, &quot;pue&quot;], [3, &quot;uni&quot;]] ).into{ pan_spec1_ch; pan_spec2_ch } For each location, the data is subset to include only local hamlets. // git 3.5 // subset data to local hamlets process subset_vcf_by_location { label &quot;L_20g2h_subset_vcf&quot; input: set val( loc ), vcfId, file( vcf ) from vcf_location_combo output: set val( loc ), file( &quot;${loc}.vcf.gz&quot; ), file( &quot;${loc}.pop&quot; ) into ( vcf_loc_pair1, vcf_loc_pair2, vcf_loc_pair3 ) script: &quot;&quot;&quot; vcfsamplenames ${vcf[0]} | \\ grep ${loc} | \\ grep -v tor | \\ grep -v tab &gt; ${loc}.pop vcftools --gzvcf ${vcf[0]} \\ --keep ${loc}.pop \\ --mac 3 \\ --recode \\ --stdout | gzip &gt; ${loc}.vcf.gz &quot;&quot;&quot; } As we also want to compute global statistics, we create another subset including all sampled hamlets but excluding the outgroups. // git 3.6 // subset the global data set to hamlets only process subset_vcf_hamlets_only { label &quot;L_20g15h_filter_hamlets_only&quot; publishDir &quot;../../1_genotyping/4_phased/&quot;, mode: &#39;copy&#39; , pattern: &quot;*.vcf.gz&quot; //module &quot;R3.5.2&quot; input: set vcfId, file( vcf ) from vcf_filter output: file( &quot;hamlets_only.vcf.gz*&quot; ) into vcf_hamlets_only set file( &quot;hamlets_only.vcf.gz*&quot; ), file( &quot;hamlets_only.pop.txt&quot; ) into vcf_multi_fst script: &quot;&quot;&quot; vcfsamplenames ${vcf[0]} | \\ grep -v &quot;abe\\\\|gum\\\\|ind\\\\|may\\\\|nig\\\\|pue\\\\|ran\\\\|uni&quot; &gt; outgroup.pop vcfsamplenames ${vcf[0]} | \\ grep &quot;abe\\\\|gum\\\\|ind\\\\|may\\\\|nig\\\\|pue\\\\|ran\\\\|uni&quot; | \\ awk &#39;{print \\$1&quot;\\\\t&quot;\\$1}&#39; | \\ sed &#39;s/\\\\t.*\\\\(...\\\\)\\\\(...\\\\)\\$/\\\\t\\\\1\\\\t\\\\2/g&#39; &gt; hamlets_only.pop.txt vcftools \\ --gzvcf ${vcf[0]} \\ --remove outgroup.pop \\ --recode \\ --stdout | gzip &gt; hamlets_only.vcf.gz &quot;&quot;&quot; } 4.2.1 FST Using this subset, we compute the global differentiation among all hamlet populations. // ----------- Fst section ----------- // git 3.7 // compute global fst process fst_multi { label &#39;L_20g15h_fst_multi&#39; publishDir &quot;../../2_analysis/fst/50k/&quot;, mode: &#39;copy&#39; , pattern: &quot;*.50k.tsv.gz&quot; publishDir &quot;../../2_analysis/fst/10k/&quot;, mode: &#39;copy&#39; , pattern: &quot;*.10k.tsv.gz&quot; publishDir &quot;../../2_analysis/fst/logs/&quot;, mode: &#39;copy&#39; , pattern: &quot;*.log&quot; publishDir &quot;../../2_analysis/summaries&quot;, mode: &#39;copy&#39; , pattern: &quot;fst_outliers_998.tsv&quot; //conda &quot;$HOME/miniconda2/envs/py3&quot; //module &quot;R3.5.2&quot; input: set file( vcf ), file( pop ) from vcf_multi_fst output: file( &quot;multi_fst*&quot; ) into multi_fst_output file( &quot;fst_outliers_998.tsv&quot; ) into fst_outlier_output script: &quot;&quot;&quot; awk &#39;{print \\$1&quot;\\\\t&quot;\\$2\\$3}&#39; ${pop} &gt; pop.txt for k in abehon gumhon indbel maybel nigbel nighon nigpan puebel puehon puepan ranhon unibel unihon unipan; do grep \\$k pop.txt | cut -f 1 &gt; pop.\\$k.txt done POP=&quot;--weir-fst-pop pop.abehon.txt \\ --weir-fst-pop pop.gumhon.txt \\ --weir-fst-pop pop.indbel.txt \\ --weir-fst-pop pop.maybel.txt \\ --weir-fst-pop pop.nigbel.txt \\ --weir-fst-pop pop.nighon.txt \\ --weir-fst-pop pop.nigpan.txt \\ --weir-fst-pop pop.puebel.txt \\ --weir-fst-pop pop.puehon.txt \\ --weir-fst-pop pop.puepan.txt \\ --weir-fst-pop pop.ranhon.txt \\ --weir-fst-pop pop.unibel.txt \\ --weir-fst-pop pop.unihon.txt \\ --weir-fst-pop pop.unipan.txt&quot; # fst by SNP # ---------- vcftools --gzvcf ${vcf} \\ \\$POP \\ --stdout 2&gt; multi_fst_snp.log | \\ gzip &gt; multi_fst.tsv.gz # fst 50kb window # --------------- vcftools --gzvcf ${vcf} \\ \\$POP \\ --fst-window-step 5000 \\ --fst-window-size 50000 \\ --stdout 2&gt; multi_fst.50k.log | \\ gzip &gt; multi_fst.50k.tsv.gz # fst 10kb window # --------------- vcftools --gzvcf ${vcf} \\ \\$POP \\ --fst-window-step 1000 \\ --fst-window-size 10000 \\ --stdout 2&gt; multi_fst.10k.log | \\ gzip &gt; multi_fst_snp.tsv.gz Rscript --vanilla \\$BASE_DIR/R/table_fst_outliers.R multi_fst.50k.tsv.gz &quot;&quot;&quot; } Then, we set up the pair wise species comparisons… // git 3.8 // prepare pairwise fsts // ------------------------------ /* (create all possible species pairs depending on location and combine with genotype subset (for the respective location))*/ // ------------------------------ /* channel content after joinig: set [0:val(loc), 1:file(vcf), 2:file(pop), 3:val(spec1), 4:val(spec2)]*/ // ------------------------------ bel_pairs_ch = Channel.from( &quot;bel&quot; ) .join( vcf_loc_pair1 ) .combine(bel_spec1_ch) .combine(bel_spec2_ch) .filter{ it[3] &lt; it[5] } .map{ it[0,1,2,4,6]} hon_pairs_ch = Channel.from( &quot;hon&quot; ) .join( vcf_loc_pair2 ) .combine(hon_spec1_ch) .combine(hon_spec2_ch) .filter{ it[3] &lt; it[5] } .map{ it[0,1,2,4,6]} pan_pairs_ch = Channel.from( &quot;pan&quot; ) .join( vcf_loc_pair3 ) .combine(pan_spec1_ch) .combine(pan_spec2_ch) .filter{ it[3] &lt; it[5] } .map{ it[0,1,2,4,6]} bel_pairs_ch.concat( hon_pairs_ch, pan_pairs_ch ).set { all_fst_pairs_ch } … and run them. // git 3.9 // compute pairwise fsts process fst_run { label &#39;L_32g4h_fst_run&#39; publishDir &quot;../../2_analysis/fst/50k/&quot;, mode: &#39;copy&#39; , pattern: &quot;*.50k.windowed.weir.fst.gz&quot; publishDir &quot;../../2_analysis/fst/10k/&quot;, mode: &#39;copy&#39; , pattern: &quot;*.10k.windowed.weir.fst.gz&quot; publishDir &quot;../../2_analysis/fst/logs/&quot;, mode: &#39;copy&#39; , pattern: &quot;${loc}-${spec1}-${spec2}.log&quot; input: set val( loc ), file( vcf ), file( pop ), val( spec1 ), val( spec2 ) from all_fst_pairs_ch output: set val( loc ), file( &quot;*.50k.windowed.weir.fst.gz&quot; ), file( &quot;${loc}-${spec1}-${spec2}.log&quot; ) into fst_50k file( &quot;*.10k.windowed.weir.fst.gz&quot; ) into fst_10k_output file( &quot;${loc}-${spec1}-${spec2}.log&quot; ) into fst_logs script: &quot;&quot;&quot; grep ${spec1} ${pop} &gt; pop1.txt grep ${spec2} ${pop} &gt; pop2.txt vcftools --gzvcf ${vcf} \\ --weir-fst-pop pop1.txt \\ --weir-fst-pop pop2.txt \\ --fst-window-step 5000 \\ --fst-window-size 50000 \\ --out ${loc}-${spec1}-${spec2}.50k 2&gt; ${loc}-${spec1}-${spec2}.log vcftools --gzvcf ${vcf} \\ --weir-fst-pop pop1.txt \\ --weir-fst-pop pop2.txt \\ --fst-window-size 10000 \\ --fst-window-step 1000 \\ --out ${loc}-${spec1}-${spec2}.10k gzip *.windowed.weir.fst &quot;&quot;&quot; } The genome wide summaries are compiled from the log files of VCFtools. // git 3.10 /* collect the VCFtools logs to crate a table with the genome wide fst values */ process fst_globals { label &#39;L_loc_fst_globals&#39; publishDir &quot;../../2_analysis/summaries&quot;, mode: &#39;copy&#39; , pattern: &quot;fst_globals.txt&quot; //module &quot;R3.5.2&quot; input: file( log ) from fst_logs.collect() output: file( &quot;fst_globals.txt&quot; ) into fst_glob script: &quot;&quot;&quot; cat *.log | \\ grep -E &#39;Weir and Cockerham|--out&#39; | \\ grep -A 3 50k | \\ sed &#39;/^--/d; s/^.*--out //g; s/.50k//g; /^Output/d; s/Weir and Cockerham //g; s/ Fst estimate: /\\t/g&#39; | \\ paste - - - | \\ cut -f 1,3,5 | \\ sed &#39;s/^\\\\(...\\\\)-/\\\\1\\\\t/g&#39; &gt; fst_globals.txt &quot;&quot;&quot; } 4.2.1 GxP The software use for the GxP association uses genotypes in plink file format, so the first step was to convert the input. // ----------- G x P section ----------- // git 3.11 // reformat genotypes (1) process plink12 { label &#39;L_20g2h_plink12&#39; input: set vcfId, file( vcf ) from vcf_gxp output: set file( &quot;GxP_plink.map&quot; ), file( &quot;GxP_plink.ped&quot; ) into plink_GxP script: &quot;&quot;&quot; vcfsamplenames ${vcf[0]} | \\ grep -v &quot;tor\\\\|tab\\\\|flo&quot; | \\ awk &#39;{print \\$1&quot;\\\\t&quot;\\$1}&#39; | \\ sed &#39;s/\\\\t.*\\\\(...\\\\)\\\\(...\\\\)\\$/\\\\t\\\\1\\\\t\\\\2/g&#39; &gt; pop.txt vcftools \\ --gzvcf ${vcf[0]} \\ --plink \\ --out GxP_plink plink \\ --file GxP_plink \\ --recode12 \\ --out hapmap &quot;&quot;&quot; } // git 3.12 // reformat genotypes (2) process GxP_run { label &#39;L_20g2h_GxP_binary&#39; input: set file( map ), file( ped ) from plink_GxP output: set file( &quot;*.bed&quot; ), file( &quot;*.bim&quot; ),file( &quot;*.fam&quot; ) into plink_binary script: &quot;&quot;&quot; # convert genotypes into binary format (bed/bim/fam) plink \\ --noweb \\ --file GxP_plink \\ --make-bed \\ --out GxP_plink_binary &quot;&quot;&quot; } Additionally to the genotypes, the phenotypes of the samples are needed. // git 3.13 // import phenotypes Channel .fromPath(&quot;../../metadata/phenotypes.sc&quot;) .set{ phenotypes_raw } For historic reasons, we perform a PCA on the phenotypes and report the extended phenotype data including the scoring on PC1 &amp; PC2. // git 3.14 // run PCA on phenotypes process phenotye_pca { label &quot;L_loc_phenotype_pca&quot; publishDir &quot;../../2_analysis/phenotype&quot;, mode: &#39;copy&#39; , pattern: &quot;*.txt&quot; //module &quot;R3.5.2&quot; input: file( sc ) from phenotypes_raw output: file( &quot;phenotypes.txt&quot; ) into phenotype_file file( &quot;phenotype_pca*.pdf&quot; ) into phenotype_pca script: &quot;&quot;&quot; Rscript --vanilla \\$BASE_DIR/R/phenotypes_pca.R ${sc} &quot;&quot;&quot; } We set up all the traits for which we want to perform a GxP association. // git 3.15 // setup GxP traits Channel .from(&quot;Bars&quot;, &quot;Snout&quot;, &quot;Peduncle&quot;) //, &quot;Lines&quot;, &quot;Blue&quot;, &quot;Yellow&quot;, &quot;Orange&quot;, &quot;Tail_transparent&quot;,&quot;PC1&quot;, &quot;PC2&quot;, &quot;PC_d1&quot;, &quot;abe&quot;, &quot;gum&quot;, &quot;ind&quot;, &quot;may&quot;, &quot;nig&quot;, &quot;pue&quot;, &quot;ran&quot;, &quot;uni&quot;, &quot;blue2&quot; ) .set{ traits_ch } Then, we combine the reformatted genotypes with the phenotyes and the traits of interest. // git 3.16 // bundle GxP input traits_ch.combine( plink_binary ).combine( phenotype_file ).set{ trait_plink_combo } Having collected all the input, we can now run the GxP. // git 3.17 // actually run the GxP process gemma_run { label &#39;L_32g4h_GxP_run&#39; publishDir &quot;../../2_analysis/GxP/bySNP/&quot;, mode: &#39;copy&#39; //module &quot;R3.5.2&quot; input: set val( pheno ), file( bed ), file( bim ), file( fam ), file( pheno_file ) from trait_plink_combo output: file(&quot;*.GxP.txt.gz&quot;) into gemma_results script: &quot;&quot;&quot; source \\$BASE_DIR/sh/body.sh BASE_NAME=\\$(echo ${fam} | sed &#39;s/.fam//g&#39;) mv ${fam} \\$BASE_NAME-old.fam cp \\${BASE_NAME}-old.fam ${fam} # 1) replace the phenotype values Rscript --vanilla \\$BASE_DIR/R/assign_phenotypes.R ${fam} ${pheno_file} ${pheno} # 2) create relatedness matrix of samples using gemma gemma -bfile \\$BASE_NAME -gk 1 -o ${pheno} # 3) fit linear model using gemma (-lm) gemma -bfile \\$BASE_NAME -lm 4 -miss 0.1 -notsnp -o ${pheno}.lm # 4) fit linear mixed model using gemma (-lmm) gemma -bfile \\$BASE_NAME -k output/${pheno}.cXX.txt -lmm 4 -o ${pheno}.lmm # 5) reformat output sed &#39;s/\\\\trs\\\\t/\\\\tCHROM\\\\tPOS\\\\t/g; s/\\\\([0-2][0-9]\\\\):/\\\\1\\\\t/g&#39; output/${pheno}.lm.assoc.txt | \\ cut -f 2,3,9-14 | body sort -k1,1 -k2,2n | gzip &gt; ${pheno}.lm.GxP.txt.gz sed &#39;s/\\\\trs\\\\t/\\\\tCHROM\\\\tPOS\\\\t/g; s/\\\\([0-2][0-9]\\\\):/\\\\1\\\\t/g&#39; output/${pheno}.lmm.assoc.txt | \\ cut -f 2,3,8-10,13-15 | body sort -k1,1 -k2,2n | gzip &gt; ${pheno}.lmm.GxP.txt.gz &quot;&quot;&quot; } To smooth the GxP results, we initialize two resolutions (50 kb windows with 5 kb increments and 10 kb windows with 1 kb increments). // git 3.18 // setup smoothing levels Channel .from([[50000, 5000], [10000, 1000]]) .set{ gxp_smoothing_levels } The we apply all smoothing levels to the raw GxP output… // git 3.19 // apply all smoothing levels gemma_results.combine( gxp_smoothing_levels ).set{ gxp_smoothing_input } .. and run the smoothing. // git 3.20 // actually run the smoothing process gemma_smooth { label &#39;L_20g2h_GxP_smooth&#39; publishDir &quot;../../2_analysis/GxP/${win}&quot;, mode: &#39;copy&#39; input: set file( lm ), file( lmm ), val( win ), val( step ) from gxp_smoothing_input output: set val( win ), file( &quot;*.lm.*k.txt.gz&quot; ) into gxp_lm_smoothing_output set val( win ), file( &quot;*.lmm.*k.txt.gz&quot; ) into gxp_lmm_smoothing_output script: &quot;&quot;&quot; \\$BASE_DIR/sh/gxp_slider ${lm} ${win} ${step} \\$BASE_DIR/sh/gxp_slider ${lmm} ${win} ${step} &quot;&quot;&quot; } At this step we are done with differentiation and GxP. 4.2.1 FST within species (adaptation scenario) To judge the independence of the individual populations of the hamlets species sampled at several locations, we also compute the FST among those populations. We start by defining the species-set for which we sampled multiple populations. // Fst within species --------------------------------------------------------- // git 3.21 // define species set Channel .from( &quot;nig&quot;, &quot;pue&quot;, &quot;uni&quot;) .set{ species_ch } Then, we define the sampling locations. // git 3.22 // define location set Channel.from( [[1, &quot;bel&quot;], [2, &quot;hon&quot;], [3, &quot;pan&quot;]]).into{ locations_ch_1;locations_ch_2 } Next, we create all population-pairs and bind it to the genotype file. // git 3.23 // create location pairs locations_ch_1 .combine(locations_ch_2) .filter{ it[0] &lt; it[2] } .map{ it[1,3]} .combine( species_ch ) .combine( vcf_adapt ) .set{ vcf_location_combo_adapt } Last, we compute the pair wise FST among all populations. // git 3.24 // compute pairwise fsts process fst_run_adapt { label &#39;L_20g4h_fst_run_adapt&#39; publishDir &quot;../../2_analysis/fst/adapt/&quot;, mode: &#39;copy&#39; , pattern: &quot;*.log&quot; input: set val( loc1 ), val( loc2 ), val( spec ), val(vcf_indx), file( vcf ) from vcf_location_combo_adapt output: file( &quot;adapt_${spec}${loc1}-${spec}${loc2}.log&quot; ) into fst_adapt_logs script: &quot;&quot;&quot; vcfsamplenames ${vcf[0]} | grep ${spec}${loc1} &gt; pop1.txt vcfsamplenames ${vcf[0]} | grep ${spec}${loc2} &gt; pop2.txt vcftools --gzvcf ${vcf[0]} \\ --weir-fst-pop pop1.txt \\ --weir-fst-pop pop2.txt \\ --out adapt_${spec}${loc1}-${spec}${loc2} 2&gt; adapt_${spec}${loc1}-${spec}${loc2}.log &quot;&quot;&quot; } "],
["git-4-analysis-ii-dxy-pi.html", "5 (git 4) Analysis II (dXY &amp; pi) 5.1 Summary 5.2 Details of analysis_dxy.nf", " 5 (git 4) Analysis II (dXY &amp; pi) This pipeline can be executed as follows: cd $BASE_DIR/nf/04_analysis_dxy source ../sh/nextflow_alias.sh nf_run_dxy 5.1 Summary The genetic divergence, as well diversity, are computed within the nextflow script analysis_dxy.nf (located under $BASE_DIR/nf/04_analysis_dxy/). It takes the all BP data set and computes dXY and \\(\\pi\\). Below is an overview of the steps involved in the analysis. (The green dot indicates the genotype input, red arrows depict output that is exported for further use.) 5.2 Details of analysis_dxy.nf 5.2.1 Data preparation The nextflow script starts by opening the genotype data and feeding it into three different streams (one for dXY and one for \\(\\pi\\)). #!/usr/bin/env nextflow // This pipeline includes the analysis run on the // all callable sites data sheet (dxy). // git 4.1 // load genotypes Channel .fromFilePairs(&quot;../../1_genotyping/3_gatk_filtered/filterd.allBP.vcf.{gz,gz.tbi}&quot;) .into{ vcf_ch; vcf_pi_ch } The computation of dXY is split by linkage group, so we need to initialize a channel for the LGs. // git 4.2 // initialize LGs Channel .from( (&#39;01&#39;..&#39;09&#39;) + (&#39;10&#39;..&#39;19&#39;) + (&#39;20&#39;..&#39;24&#39;) ) .set{ lg_ch } Now, we can subset the data set and convert it to a custom genotype format. // git 4.3 // split by LG and reformat the genotypes process split_allBP { label &#39;L_32g15h_split_allBP&#39; tag &quot;LG${lg}&quot; input: set val( lg ), vcfId, file( vcf ) from lg_ch.combine( vcf_ch ) output: set val( lg ), file( &#39;filterd.allBP.vcf.gz&#39; ), file( &quot;allBP.LG${lg}.geno.gz&quot; ) into geno_ch script: &quot;&quot;&quot; module load openssl1.0.2 vcftools --gzvcf ${vcf[0]} \\ --chr LG${lg} \\ --recode \\ --stdout | bgzip &gt; allBP.LG${lg}.vcf.gz python \\$SFTWR/genomics_general/VCF_processing/parseVCF.py \\ -i allBP.LG${lg}.vcf.gz | gzip &gt; allBP.LG${lg}.geno.gz &quot;&quot;&quot; } Since the species composition differs between locations, we need to initialize three separate sets of species. These are going to be used to create the divergence species pairs. // git 4.4 // define location specific sepcies set Channel.from( [[1, &quot;ind&quot;], [2, &quot;may&quot;], [3, &quot;nig&quot;], [4, &quot;pue&quot;], [5, &quot;uni&quot;]] ).into{ bel_spec1_ch; bel_spec2_ch } Channel.from( [[1, &quot;abe&quot;], [2, &quot;gum&quot;], [3, &quot;nig&quot;], [4, &quot;pue&quot;], [5, &quot;ran&quot;], [6, &quot;uni&quot;]] ).into{ hon_spec1_ch; hon_spec2_ch } Channel.from( [[1, &quot;nig&quot;], [2, &quot;pue&quot;], [3, &quot;uni&quot;]] ).into{ pan_spec1_ch; pan_spec2_ch } For the diversity, we also initialize the full set of populations of the study. // git 4.5 // init all sampled populations (for pi) Channel .from(&#39;indbel&#39;, &#39;maybel&#39;, &#39;nigbel&#39;, &#39;puebel&#39;, &#39;unibel&#39;, &#39;abehon&#39;, &#39;gumhon&#39;, &#39;nighon&#39;, &#39;puehon&#39;, &#39;ranhon&#39;, &#39;unihon&#39;, &#39;nigpan&#39;, &#39;puepan&#39;, &#39;unipan&#39;) .set{spec_dxy} We want to run a sliding window at different resolutions, so we set up a channel for these. // git 4.6 // init slining window resolutions Channel .from( 1, 5 ) .into{ kb_ch; kb_ch2; kb_ch3 } 5.2.1 dXY To prepare all species comparisons used to estimate divergence, we combine each species witch all other species within a location. // git 4.7 // prepare pair wise dxy // ------------------------------ // create all possible species pairs depending on location // and combine with genotype subset (for the respective location) // ------------------------------ // channel content after joining: // set [0:val(loc), 1:file(vcf), 2:file(pop), 3:val(spec1), 4:val(spec2)] // ------------------------------ bel_pairs_ch = Channel.from( &quot;bel&quot; ) .combine( bel_spec1_ch ) .combine( bel_spec2_ch ) .filter{ it[1] &lt; it[3] } .map{ it[0,2,4]} hon_pairs_ch = Channel.from( &quot;hon&quot; ) .combine( hon_spec1_ch ) .combine(hon_spec2_ch) .filter{ it[1] &lt; it[3] } .map{ it[0,2,4]} pan_pairs_ch = Channel.from( &quot;pan&quot; ) .combine( pan_spec1_ch ) .combine(pan_spec2_ch) .filter{ it[1] &lt; it[3] } .map{ it[0,2,4]} Now we attach the genotypes and the resolution level to the species pairs. // git 4.8 // combine species pair with genotypes (and window size) bel_pairs_ch .concat( hon_pairs_ch, pan_pairs_ch ) .combine( geno_ch ) .combine( kb_ch ) .into { all_dxy_pairs_ch; random_dxy_pairs_ch } At this point, we can calculate dXY. // git 4.9 // compute the dxy values process dxy_lg { label &#39;L_G32g15h_dxy_lg&#39; tag &quot;${spec1}${loc}-${spec2}${loc}_LG${lg}&quot; input: set val( loc ), val( spec1 ), val( spec2 ), val( lg ), file( vcf ), file( geno ), val( kb ) from all_dxy_pairs_ch output: set val( &quot;${spec1}${loc}-${spec2}${loc}-${kb}&quot; ), file( &quot;dxy.${spec1}${loc}-${spec2}${loc}.LG${lg}.${kb}0kb-${kb}kb.txt.gz&quot; ), val( lg ), val( &quot;${spec1}${loc}&quot; ), val( &quot;${spec2}${loc}&quot; ), val( kb ) into dxy_lg_ch script: &quot;&quot;&quot; module load openssl1.0.2 module load intel17.0.4 intelmpi17.0.4 zcat ${geno} | \\ head -n 1 | \\ cut -f 3- | \\ sed &#39;s/\\\\t/\\\\n/g&#39; | \\ awk -v OFS=&#39;\\\\t&#39; &#39;{print \\$1, substr( \\$1, length(\\$1) - 5, 6)}&#39; &gt; pop.txt mpirun \\$NQSII_MPIOPTS -np 1 \\ python \\$SFTWR/genomics_general/popgenWindows.py \\ -w ${kb}0000 -s ${kb}000 \\ --popsFile pop.txt \\ -p ${spec1}${loc} -p ${spec2}${loc} \\ -g ${geno} \\ -o dxy.${spec1}${loc}-${spec2}${loc}.LG${lg}.${kb}0kb-${kb}kb.txt.gz \\ -f phased \\ --writeFailedWindows \\ -T 1 &quot;&quot;&quot; } Since the calculation was split across LGs, we now need to collect all LGs of a particular species pair… // git 4.10 // collect all LGs for each species pair dxy_lg_ch .groupTuple() .set{ tubbled_dxy } … and merge the results. // git 4.11 // concatenate all LGs for each species pair process receive_tuple { label &#39;L_20g2h_receive_tuple&#39; publishDir &quot;../../2_analysis/dxy/${kb[0]}0k/&quot;, mode: &#39;copy&#39; tag &quot;${pop1[0]}-${pop2[0]}&quot; input: set val( comp ), file( dxy ), val( lg ), val( pop1 ), val( pop2 ), val( kb ) from tubbled_dxy output: file( &quot;dxy.${pop1[0]}-${pop2[0]}.${kb[0]}0kb-${kb[0]}kb.tsv.gz&quot; ) into dxy_output_ch script: &quot;&quot;&quot; zcat dxy.${pop1[0]}-${pop2[0]}.LG01.${kb[0]}0kb-${kb[0]}kb.txt.gz | \\ head -n 1 &gt; dxy.${pop1[0]}-${pop2[0]}.${kb[0]}0kb-${kb[0]}kb.tsv; for j in {01..24};do echo &quot;-&gt; LG\\$j&quot; zcat dxy.${pop1[0]}-${pop2[0]}.LG\\$j.${kb[0]}0kb-${kb[0]}kb.txt.gz | \\ awk &#39;NR&gt;1{print}&#39; &gt;&gt; dxy.${pop1[0]}-${pop2[0]}.${kb[0]}0kb-${kb[0]}kb.tsv; done gzip dxy.${pop1[0]}-${pop2[0]}.${kb[0]}0kb-${kb[0]}kb.tsv &quot;&quot;&quot; } For control, we also create a “random” dXY run, where we take the most diverged species pair and randomize the population assignment of the samples. Therefore, we first pick the most diverged species pair. // git 4.12 // collect a species pair to randomize Channel .from( [[&#39;bel&#39;, &#39;ind&#39;, &#39;may&#39;]] ) .set{ random_run_ch } Then we set the sliding window resolution. // git 4.13 // setup channel content for random channel Channel .from( 1 ) .combine( random_run_ch ) .combine( kb_ch2 ) .filter{ it[4] == 5 } .set{ random_sets_ch } Now, we randomize the population assignment of the samples and calculate differentiation. // git 4.14 // permute the population assignment (the randomization) process randomize_samples { label &#39;L_20g15h_randomize_samples&#39; publishDir &quot;../../2_analysis/fst/${kb}0k/random&quot;, mode: &#39;copy&#39; , pattern: &quot;*_windowed.weir.fst.gz&quot; module &quot;R3.5.2&quot; input: set val( random_set ), val( loc ), val(spec1), val(spec2), val( kb ) from random_sets_ch output: set random_set, file( &quot;random_pop.txt&quot; ) into random_pops_ch file( &quot;*_windowed.weir.fst.gz&quot;) into random_fst_out script: &quot;&quot;&quot; cut -f 2,3 \\$BASE_DIR/metadata/sample_info.txt | \\ grep &quot;${loc}&quot; | \\ grep &quot;${spec1}\\\\|${spec2}&quot; &gt; pop_prep.tsv Rscript --vanilla \\$BASE_DIR/R/randomize_pops.R grep A random_pop.txt | cut -f 1 &gt; pop1.txt grep B random_pop.txt | cut -f 1 &gt; pop2.txt vcftools \\ --gzvcf \\$BASE_DIR/1_genotyping/3_gatk_filtered/filterd_bi-allelic.allBP.vcf.gz \\ --weir-fst-pop pop1.txt \\ --weir-fst-pop pop2.txt \\ --fst-window-step ${kb}0000 \\ --fst-window-size ${kb}0000 \\ --stdout | gzip &gt; ${loc}-aaa-bbb.${kb}0k.random_${spec1}_${spec2}_windowed.weir.fst.gz &quot;&quot;&quot; } We set up another channel to prepare the random dXY… // git 4.15 // pick random pair of interest random_dxy_pairs_ch .filter{ it[0] == &#39;bel&#39; &amp;&amp; it[1] == &#39;ind&#39; &amp;&amp; it[2] == &#39;may&#39; &amp;&amp; it[6] == 5 } .combine( random_pops_ch ) .set{ random_assigned_ch } .. and compute the divergence. // git 4.16 // compute the dxy values process dxy_lg_random { label &#39;L_G32g15h_dxy_lg_random&#39; tag &quot;aaa${loc}-bbb${loc}_LG${lg}&quot; module &quot;R3.5.2&quot; input: set val( loc ), val( spec1 ), val( spec2 ), val( lg ), file( vcf ), file( geno ), val( kb ), val( random_set ), file( pop_file ) from random_assigned_ch output: set val( &quot;aaa${loc}-bbb${loc}-${kb}0kb&quot; ), file( &quot;dxy.aaa${loc}-bbb${loc}.LG${lg}.${kb}0kb-${kb}kb.txt.gz&quot; ), val( lg ), val( &quot;aaa${loc}&quot; ), val( &quot;bbb${loc}&quot; ), val( kb ) into dxy_random_lg_ch script: &quot;&quot;&quot; module load openssl1.0.2 module load intel17.0.4 intelmpi17.0.4 mpirun \\$NQSII_MPIOPTS -np 1 \\ python \\$SFTWR/genomics_general/popgenWindows.py \\ -w ${kb}0000 -s ${kb}000 \\ --popsFile ${pop_file} \\ -p A -p B \\ -g ${geno} \\ -o dxy.aaa${loc}-bbb${loc}.LG${lg}.${kb}0kb-${kb}kb.txt.gz \\ -f phased \\ --writeFailedWindows \\ -T 1 &quot;&quot;&quot; } Again, we collect the output of the individual LGs…. // git 4.17 // collect all LGs of random run dxy_random_lg_ch .groupTuple() .set{ tubbled_random_dxy } … and we merge them // git 4.18 // concatinate all LGs of random run process receive_random_tuple { label &#39;L_20g2h_receive_random_tuple&#39; publishDir &quot;../../2_analysis/dxy/random/&quot;, mode: &#39;copy&#39; input: set val( comp ), file( dxy ), val( lg ), val( pop1 ), val( pop2 ), val( kb ) from tubbled_random_dxy output: file( &quot;dxy.${pop1[0]}-${pop2[0]}.${kb[0]}0kb-${kb[0]}kb.tsv.gz&quot; ) into dxy_random_output_ch script: &quot;&quot;&quot; zcat dxy.${pop1[0]}-${pop2[0]}.LG01.${kb[0]}0kb-${kb[0]}kb.txt.gz | \\ head -n 1 &gt; dxy.${pop1[0]}-${pop2[0]}.${kb[0]}0kb-${kb[0]}kb.tsv; for j in {01..24};do echo &quot;-&gt; LG\\$j&quot; zcat dxy.${pop1[0]}-${pop2[0]}.LG\\$j.${kb[0]}0kb-${kb[0]}kb.txt.gz | \\ awk &#39;NR&gt;1{print}&#39; &gt;&gt; dxy.${pop1[0]}-${pop2[0]}.${kb[0]}0kb-${kb[0]}kb.tsv; done gzip dxy.${pop1[0]}-${pop2[0]}.${kb[0]}0kb-${kb[0]}kb.tsv &quot;&quot;&quot; } 5.2.1 \\(\\pi\\) Estimating the diversity is quite straight forward: We take the prepared, population identifier, the data set and the window resolutions and run VCFtools on each combination. // --------------------------------------------------------------- // The pi part need to be run AFTER the global fst outlier // windows were selected (REMEMBER TO CHECK FST OUTLIER DIRECTORY) // --------------------------------------------------------------- // git 4.19 // calculate pi per species process pi_per_spec { label &#39;L_32g15h_pi&#39; tag &quot;${spec}&quot; publishDir &quot;../../2_analysis/pi/${kb}0k&quot;, mode: &#39;copy&#39; input: set val( spec ), vcfId, file( vcf ), val( kb ) from spec_dxy.combine( vcf_pi_ch ).combine( kb_ch3 ) output: file( &quot;*.${kb}0k.windowed.pi.gz&quot; ) into pi_50k script: &quot;&quot;&quot; module load openssl1.0.2 vcfsamplenames ${vcf[0]} | \\ grep ${spec} &gt; pop.txt vcftools --gzvcf ${vcf[0]} \\ --keep pop.txt \\ --window-pi ${kb}0000 \\ --window-pi-step ${kb}000 \\ --out ${spec}.${kb}0k 2&gt; ${spec}.pi.log gzip ${spec}.${kb}0k.windowed.pi tail -n +2 \\$BASE_DIR/2_analysis/summaries/fst_outliers_998.tsv | \\ cut -f 2,3,4 &gt; outlier.bed vcftools --gzvcf ${vcf[0]} \\ --keep pop.txt \\ --exclude-bed outlier.bed \\ --window-pi ${kb}0000 \\ --window-pi-step ${kb}000\\ --out ${spec}_no_outlier.${kb}0k 2&gt; ${spec}_${kb}0k_no_outllier.pi.log gzip ${spec}_no_outlier.${kb}0k.windowed.pi &quot;&quot;&quot; } At this step we are done with divergence and diversity. "],
["git-5-analysis-iii-phylogeny-topology-weighting.html", "6 (git 5) Analysis III (phylogeny &amp; topology weighting) 6.1 Summary 6.2 Details of analysis_fasttree_twisst.nf", " 6 (git 5) Analysis III (phylogeny &amp; topology weighting) This pipeline can be executed as follows: cd $BASE_DIR/nf/05_analysis_fasttree_twisst source ../sh/nextflow_alias.sh nf_run_phylo 6.1 Summary The whole genome phylogeny and the topology weighting are prepared within the nextflow script analysis_fasttree_twisst.nf (located under $BASE_DIR/nf/05_analysis_fasttree_twisst/), which runs on the SNPs only data set. Below is an overview of the steps involved in the process. (The green dot indicates the genotype input, red arrows depict output that is exported for further use.) 6.2 Details of analysis_fasttree_twisst.nf 6.2.1 Setup The nextflow script starts by opening the genotype data and feeding it into two different streams (one for the phylogeny and one for topology weighting). #!/usr/bin/env nextflow // git 5.1 // open genotype data Channel .fromFilePairs(&quot;../../1_genotyping/4_phased/phased_mac2.vcf.{gz,gz.tbi}&quot;) .into{ vcf_fasttree_whg; vcf_locations } 6.2.1 Whole genome phylogeny During data exploration, various subsets of the data set were investigated, including the different sampling locations, whole genome vs. non-outlier regions and all samples vs. hamlets only. (Now, most of the options have been muted by commenting these options out.) Here, we set up a channel managing the subset by location. // git 5.2 // setting the sampling location // (the script is set up to run on diffferent subsets of samples) Channel .from( &quot;all&quot; ) //, &quot;bel&quot;, &quot;hon&quot;, &quot;pan&quot; ) .set{ locations4_ch } This channel toggles the inclusion of FST outlier regions. // git 5.3 // setting loci restrictions // (keep vs remove outlier regions) Channel .from( &quot;whg&quot; ) //, &quot;no_musks&quot; ) .set{ whg_modes } This channel toggles the inclusion of the non-hamlet outgroup. // git 5.4 // setting the sampling mode // (the script is set up to run on diffferent subsets of samples) Channel .from( &quot;no_outgroups&quot; ) //, &quot;all&quot; ) .into{ sample_modes } We combine the different selector channels to create all possible combinations of the settings. // git 5.5 // compile the config settings and add data file locations4_ch .combine( vcf_fasttree_whg ) .combine( whg_modes ) .combine( sample_modes ) .set{ vcf_fasttree_whg_location_combo } To prepare the input for the phylogeny, the fine tuned selection is applied to subset the genotypes. // git 5.6 // apply sample filter, subset and convert genotypes process subset_vcf_by_location_whg { label &quot;L_28g5h_subset_vcf_whg&quot; input: set val( loc ), vcfId, file( vcf ), val( mode ), val( sample_mode ) from vcf_fasttree_whg_location_combo output: set val( mode ), val( loc ), val( sample_mode ), file( &quot;${loc}.${mode}.${sample_mode}.whg.geno.gz&quot; ) into snp_geno_tree_whg script: &quot;&quot;&quot; DROP_CHRS=&quot; &quot; # check if samples need to be dropped based on location if [ &quot;${loc}&quot; == &quot;all&quot; ];then vcfsamplenames ${vcf[0]} &gt; prep.pop else vcfsamplenames ${vcf[0]} | \\ grep ${loc} &gt; prep.pop fi # check if outgroups need to be dropped if [ &quot;${sample_mode}&quot; == &quot;all&quot; ];then mv prep.pop ${loc}.pop else cat prep.pop | \\ grep -v tor | \\ grep -v tab &gt; ${loc}.pop fi # check if diverged LGs need to be dropped if [ &quot;${mode}&quot; == &quot;no_musks&quot; ];then DROP_CHRS=&quot;--not-chr LG04 --not-chr LG07 --not-chr LG08 --not-chr LG09 --not-chr LG12 --not-chr LG17 --not-chr LG23&quot; fi vcftools --gzvcf ${vcf[0]} \\ --keep ${loc}.pop \\ \\$DROP_CHRS \\ --mac 3 \\ --recode \\ --stdout | gzip &gt; ${loc}.${mode}.${sample_mode}.vcf.gz python \\$SFTWR/genomics_general/VCF_processing/parseVCF.py \\ -i ${loc}.${mode}.${sample_mode}.vcf.gz | gzip &gt; ${loc}.${mode}.${sample_mode}.whg.geno.gz &quot;&quot;&quot; } The subset is converted to fasta format, creating two whole genome pseudo haplotypes per sample. // git 5.7 // convert genotypes to fasta process fasttree_whg_prep { label &#39;L_190g4h_fasttree_whg_prep&#39; tag &quot;${mode} - ${loc} - ${sample_mode}&quot; input: set val( mode ), val( loc ), val( sample_mode ), file( geno ) from snp_geno_tree_whg output: set val( mode ), val( loc ), val( sample_mode ), file( &quot;all_samples.${loc}.${mode}.${sample_mode}.whg.SNP.fa&quot; ) into ( fasttree_whg_prep_ch ) script: &quot;&quot;&quot; python \\$SFTWR/genomics_general/genoToSeq.py -g ${geno} \\ -s all_samples.${loc}.${mode}.${sample_mode}.whg.SNP.fa \\ -f fasta \\ --splitPhased &quot;&quot;&quot; } Then, the phylogeny is reconstructed based on the converted geneotypes. // git 5.8 // create phylogeny process fasttree_whg_run { label &#39;L_300g30h_fasttree_run&#39; tag &quot;${mode} - ${loc} - ${sample_mode}&quot; publishDir &quot;../../2_analysis/fasttree/&quot;, mode: &#39;copy&#39; input: set val( mode ), val( loc ), val( sample_mode ), file( fa ) from fasttree_whg_prep_ch output: file( &quot;${sample_mode}.${loc}.${mode}.SNP.tree&quot; ) into ( fasttree_whg_output ) script: &quot;&quot;&quot; fasttree -nt ${fa} &gt; ${sample_mode}.${loc}.${mode}.SNP.tree &quot;&quot;&quot; } 6.2.1 Topology weighting The complexity of topology weighting increases non-linearely with the number of included populations as the number of possible unrooted topologies skyrockets. The maximum number of possible populations allowed within twisst is eight, so we are running the topology weighting for Belize and Honduras independently (for the three species at Panama only one unrooted topology is possible, so we don’t run twisst here). We start by setting up a channel for the sampling location. // git 5.9 // initialize the locations for topology weighting Channel .from( &quot;bel&quot;, &quot;hon&quot; ) .set{ locations_ch } Then, we attach the genotypes to the location. // git 5.10 locations_ch .combine( vcf_locations ) .set{ vcf_location_combo } The analysis is split by linkage group, so we need to initialize the LGs. // git 5.11 // initialize LGs Channel .from( (&#39;01&#39;..&#39;09&#39;) + (&#39;10&#39;..&#39;19&#39;) + (&#39;20&#39;..&#39;24&#39;) ) .map{ &quot;LG&quot; + it } .set{ lg_twisst } The data is subset to include only the samples of the respective location. // git 5.12 // subset the genotypes by location process subset_vcf_by_location { label &quot;L_20g2h_subset_vcf&quot; input: set val( loc ), vcfId, file( vcf ) from vcf_location_combo output: set val( loc ), file( &quot;${loc}.vcf.gz&quot; ), file( &quot;${loc}.pop&quot; ) into ( vcf_loc_twisst ) script: &quot;&quot;&quot; vcfsamplenames ${vcf[0]} | \\ grep ${loc} | \\ grep -v tor | \\ grep -v tab &gt; ${loc}.pop vcftools --gzvcf ${vcf[0]} \\ --keep ${loc}.pop \\ --mac 3 \\ --recode \\ --stdout | gzip &gt; ${loc}.vcf.gz &quot;&quot;&quot; } While running twisst, we ran into an issues regarding incompatibilities of the used scheduling system used on our computing cluster and the threading within the python scripts used in the preparation of twisst. Unfortunately, this prevented us running the preparation in place (as part of the nextflow script). Instead, we rand the preparation separately on a local computer and clumsily plugged the results into the nextflow process. Below we include the originally intended workflow which we muted so that the script is runnable using the plug-in approach. Still, the the original workflow describes the steps executed locally and conveys the intermediate steps more clearly. Also, on a different computer cluster, the original script should work alright. The next step is to attach the LGs to the genotype subsets. // --------------------------------------------------------------- // Unfortunately the twisst preparation did not work on the cluster // (&#39;in place&#39;), so I had to setup the files locally and then plug // them into this workflow. // Below is the originally intended clean workflow (commented out), // while the plugin version picks up at git 5.19. // --------------------------------------------------------------- /* MUTE: // git 5.13 // add the lg channel to the genotype subset vcf_loc_twisst .combine( lg_twisst ) .set{ vcf_loc_lg_twisst } */ Based on the LGs the genotypes are split. /* MUTE: python thread conflict - run locally and feed into ressources/plugin // git 5.14 // subset genotypes by LG process vcf2geno_loc { label &#39;L_20g15h_vcf2geno&#39; input: set val( loc ), file( vcf ), file( pop ), val( lg ) from vcf_loc_lg_twisst output: set val( loc ), val( lg ), file( &quot;${loc}.${lg}.geno.gz&quot; ), file( pop ) into snp_geno_twisst script: &quot;&quot;&quot; vcftools \\ --gzvcf ${vcf} \\ --chr ${lg} \\ --recode \\ --stdout | gzip &gt; intermediate.vcf.gz python \\$SFTWR/genomics_general/VCF_processing/parseVCF.py \\ -i intermediate.vcf.gz | gzip &gt; ${loc}.${lg}.geno.gz &quot;&quot;&quot; } */ /* MUTE: python thread conflict - run locally and feed into ressources/plugin We initialize the window size size (as SNPs) used for the topology weighting… // git 5.15 // initialize SNP window size Channel.from( 50, 200 ).set{ twisst_window_types } */ …and attach them to the genotypes. /* MUTE: python thread conflict - run locally and feed into ressources/plugin // git 5.16 // add the SNP window size to the genotype subset snp_geno_twisst.combine( twisst_window_types ).set{ twisst_input_ch } */ To conduct topology weighting, we need some underlying phylogenies, so we run PhyML along the sliding window. /* MUTE: python thread conflict - run locally and feed into ressources/plugin // git 5.17 // create the phylogenies along the sliding window process twisst_prep { label &#39;L_G120g40h_prep_twisst&#39; publishDir &quot;../../2_analysis/twisst/positions/${loc}/&quot;, mode: &#39;copy&#39; input: set val( loc ), val( lg ), file( geno ), file( pop ), val( twisst_w ) from twisst_input_ch.filter { it[0] != &#39;pan&#39; } output: set val( loc ), val( lg ), file( geno ), file( pop ), val( twisst_w ), file( &quot;*.trees.gz&quot; ), file( &quot;*.data.tsv&quot; ) into twisst_prep_ch script: &quot;&quot;&quot; module load intel17.0.4 intelmpi17.0.4 mpirun \\$NQSII_MPIOPTS -np 1 \\ python \\$SFTWR/genomics_general/phylo/phyml_sliding_windows.py \\ -g ${geno} \\ --windType sites \\ -w ${twisst_w} \\ --prefix ${loc}.${lg}.w${twisst_w}.phyml_bionj \\ --model HKY85 \\ --optimise n \\ --threads 1 &quot;&quot;&quot; } */ The last step then is to run twisst prepared phylogenies. /* MUTE: python thread conflict - run locally and feed into ressources/plugin // git 5.18 // run the topology weighting on the phylogenies process twisst_run { label &#39;L_G120g40h_run_twisst&#39; publishDir &quot;../../2_analysis/twisst/weights/&quot;, mode: &#39;copy&#39; input: set val( loc ), val( lg ), file( geno ), file( pop ), val( twisst_w ), file( tree ), file( data ) from twisst_prep_ch output: set val( loc ), val( lg ), val( twisst_w ), file( &quot;*.weights.tsv.gz&quot; ), file( &quot;*.data.tsv&quot; ) into ( twisst_output ) script: &quot;&quot;&quot; module load intel17.0.4 intelmpi17.0.4 awk &#39;{print \\$1&quot;\\\\t&quot;\\$1}&#39; ${pop} | \\ sed &#39;s/\\\\(...\\\\)\\\\(...\\\\)\\$/\\\\t\\\\1\\\\t\\\\2/g&#39; | \\ cut -f 1,3 | \\ awk &#39;{print \\$1&quot;_A\\\\t&quot;\\$2&quot;\\\\n&quot;\\$1&quot;_B\\\\t&quot;\\$2}&#39; &gt; ${loc}.${lg}.twisst_pop.txt TWISST_POPS=\\$( cut -f 2 ${loc}.${lg}.twisst_pop.txt | sort | uniq | paste -s -d&#39;,&#39; | sed &#39;s/,/ -g /g; s/^/-g /&#39; ) mpirun \\$NQSII_MPIOPTS -np 1 \\ python \\$SFTWR/twisst/twisst.py \\ --method complete \\ -t ${tree} \\ -T 1 \\ \\$TWISST_POPS \\ --groupsFile ${loc}.${lg}.twisst_pop.txt | \\ gzip &gt; ${loc}.${lg}.w${twisst_w}.phyml_bionj.weights.tsv.gz &quot;&quot;&quot; } */ Here, the plug in approach picks up. At this point we have run PhyML locally and deposited the results under $BASE_DIR/ressources/plugin/trees. To restart, we need to emulate the settings needed for the twisst process. // git 5.19 // emulate setting Channel .from(50, 200) .combine( vcf_loc_twisst ) .combine( lg_twisst ) .set{ twisst_modes } Then, we feed the phylogenies from an external directory into twisst. // git 5.20 // run the topology weighting on the phylogenies process twisst_plugin { label &#39;L_G120g40h_twisst_plugin&#39; publishDir &quot;../../2_analysis/twisst/weights/&quot;, mode: &#39;copy&#39; tag &quot;${loc}-${lg}-${mode}&quot; input: set val( mode ), val( loc ), file( vcf ), file( pop ), val( lg ) from twisst_modes output: set val( loc ), val( lg ), file( &quot;*.weights.tsv.gz&quot; ) into ( twisst_output ) script: &quot;&quot;&quot; module load intel17.0.4 intelmpi17.0.4 awk &#39;{print \\$1&quot;\\\\t&quot;\\$1}&#39; ${pop} | \\ sed &#39;s/\\\\(...\\\\)\\\\(...\\\\)\\$/\\\\t\\\\1\\\\t\\\\2/g&#39; | \\ cut -f 1,3 | \\ awk &#39;{print \\$1&quot;_A\\\\t&quot;\\$2&quot;\\\\n&quot;\\$1&quot;_B\\\\t&quot;\\$2}&#39; &gt; ${loc}.${lg}.twisst_pop.txt TWISST_POPS=\\$( cut -f 2 ${loc}.${lg}.twisst_pop.txt | sort | uniq | paste -s -d&#39;,&#39; | sed &#39;s/,/ -g /g; s/^/-g /&#39; ) mpirun \\$NQSII_MPIOPTS -np 1 \\ python \\$SFTWR/twisst/twisst.py \\ --method complete \\ -t \\$BASE_DIR/ressources/plugin/trees/${loc}/${loc}.${lg}.w${mode}.phyml_bionj.trees.gz \\ \\$TWISST_POPS \\ --groupsFile ${loc}.${lg}.twisst_pop.txt | \\ gzip &gt; ${loc}.${lg}.w${mode}.phyml_bionj.weights.tsv.gz &quot;&quot;&quot; } At this step we are done with phylogeny and the topology weighting. "],
["git-6-analysis-iv-rho.html", "7 (git 6) Analysis IV (rho) 7.1 Summary 7.2 Details of analysis_recombination.nf", " 7 (git 6) Analysis IV (rho) This pipeline can be executed as follows: cd $BASE_DIR/nf/06_analysis_recombination source ../sh/nextflow_alias.sh nf_run_recombination 7.1 Summary The population recombination rate is estimated within the nextflow script analysis_recombination.nf (located under $BASE_DIR/nf/06_analysis_recombination/), which runs on the SNPs only data set. Below is an overview of the steps involved in the analysis. (The green dot indicates the genotype input, red arrows depict output that is exported for further use.) 7.2 Details of analysis_recombination.nf 7.2.1 Data preparation The nextflow script starts by opening the genotype data. #!/usr/bin/env nextflow // This pipeline includes the recombination anlysis // git 6.1 // load genotypes Channel .fromFilePairs(&quot;../../1_genotyping/4_phased/phased_mac2.vcf.{gz,gz.tbi}&quot;) .set{ vcf_ch } The estimation of the population recombination rate using FastEPRR happens in three steps. The first step is run independently for all linkage groups, so we set up a channel for the LGs. // git 6.2 // initialize LGs Channel .from( 1..24 ) .map{ it.toString().padLeft(2, &quot;0&quot;) } .set{ lg_ch } To prepare the input, the genotypes are split by LG. // git 6.3 // split genotypes by LG process split_allBP { label &#39;L_20g2h_split_by_lg&#39; tag &quot;LG${lg}&quot; input: set val( lg ), vcfId, file( vcf ) from lg_ch.combine( vcf_ch ) output: set val( lg ), file( &quot;phased_mac2.LG${lg}.vcf.gz&quot; ) into vcf_by_lg_ch script: &quot;&quot;&quot; module load openssl1.0.2 vcftools --gzvcf ${vcf[0]} \\ --chr LG${lg} \\ --recode \\ --stdout | bgzip &gt; phased_mac2.LG${lg}.vcf.gz &quot;&quot;&quot; } The prepared data is then fed to the first step of FastEPRR. // git 6.4 // run fasteprr step 1 process fasteprr_s1 { label &#39;L_20g2h_fasteprr_s1&#39; tag &quot;LG${lg}&quot; module &quot;R3.5.2&quot; input: set val( lg ), file( vcf ) from vcf_by_lg_ch output: file( &quot;step1_LG${lg}&quot; ) into step_1_out_ch script: &quot;&quot;&quot; mkdir step1_LG${lg} Rscript --vanilla \\$BASE_DIR/R/fasteprr_step1.R ./${vcf} step1_LG${lg} LG${lg} 50 &quot;&quot;&quot; } Since nextflow manages the results of its processes in a complex file structure, we need to collect all results of step 1 and bundle them before proceeding. // git 6.5 // collect step 1 output process fasteprr_s1_summary { label &#39;L_loc_fasteprr_s1_summmary&#39; input: file( step1 ) from step_1_out_ch.collect() output: file( &quot;step1&quot; ) into ( step1_ch1, step1_ch2 ) script: &quot;&quot;&quot; mkdir step1 cp step1_LG*/* step1/ &quot;&quot;&quot; } The second step of FastEPRR is parallelized over an arbitrary number of sub-processes. Here, we initialize 250 parallel processes and combine the parallelization index with the results from step 1. // git 6.6 // initialize fasteperr subprocesses and attach them to step 1 output Channel .from( 1..250 ) .map{ it.toString().padLeft(3, &quot;0&quot;) } .combine( step1_ch1 ) .set{ step_2_run_ch } Taking this prepared bundle, we now can start the second step of FastEPRR. // git 6.7 // run fasteprr step 2 process fasteprr_s2 { label &#39;L_long_loc_fasteprr_s2&#39; tag &quot;run_${idx}&quot; module &quot;R3.5.2&quot; input: set val( idx ), file( step1 ) from step_2_run_ch output: set val( idx ), file( &quot;step2_run${idx}&quot; ) into step_2_out_ch script: &quot;&quot;&quot; mkdir -p step2_run${idx} Rscript --vanilla \\$BASE_DIR/R/fasteprr_step2.R ${step1} step2_run${idx} ${idx} &quot;&quot;&quot; } The file management of nextflow can be a bit complicated at times, so here duplicate the output of step 2 to later easily access the parallelization indices and the actual output. // git 6.8 // clone step 2 output step_2_out_ch.into{ step_2_indxs; step_2_files } We collect both clones of the step 2 results and bundle the results in a single directory. // git 6.9 // collect step 2 output process fasteprr_s2_summary { label &#39;L_loc_fasteprr_s2_summmary&#39; input: val( idx ) from step_2_indxs.map{ it[0] }.collect() file( files ) from step_2_files.map{ it[1] }.collect() output: file( &quot;step2&quot; ) into ( step2_ch ) script: &quot;&quot;&quot; mkdir step2 for k in \\$( echo ${idx} | sed &#39;s/\\\\[//g; s/\\\\]//g; s/,//g&#39;); do cp -r step2_run\\$k/* step2/ done &quot;&quot;&quot; } Then we feed the bundled results into the third step of FastEPRR. // git 6.10 // run fasteprr step 3 process fasteprr_s3 { label &#39;L_32g4h_fasteprr_s3&#39; module &quot;R3.5.2&quot; input: set file( step1 ), file( step2 ) from step1_ch2.combine( step2_ch ) output: file( &quot;step3&quot; ) into step_3_out_ch script: &quot;&quot;&quot; mkdir step3 Rscript --vanilla \\$BASE_DIR/R/fasteprr_step3.R ${step1} ${step2} step3 &quot;&quot;&quot; } To ease the usage of the FastEPRR results downstream, we reformat them and compile a tidy table. // git 6.11 // reformat overall fasteprr output process fasteprr_s3_summary { label &#39;L_loc_fasteprr_s3_summmary&#39; publishDir &quot;../../2_analysis/fasteprr&quot;, mode: &#39;copy&#39; input: file( step3 ) from step_3_out_ch output: file( &quot;step4/fasteprr.all.rho.txt.gz&quot; ) into ( step3_ch ) script: &quot;&quot;&quot; mkdir step4 # ------ rewriting the fasteprr output into tidy format -------- for k in {01..24};do j=&quot;LG&quot;\\$k; echo \\$j; \\$BASE_DIR/sh/fasteprr_trans.sh step3/chr_\\$j \\$j step4/fasteprr.\\$j done # --------- combining all LGs into a single data set ----------- cd step4 head -n 1 fasteprr.LG01.rho.txt &gt; fasteprr.all.rho.txt for k in {01..24}; do echo &quot;LG&quot;\\$k awk &#39;NR&gt;1{print \\$0}&#39; fasteprr.LG\\$k.rho.txt &gt;&gt; fasteprr.all.rho.txt done gzip fasteprr.all.rho.txt cd .. &quot;&quot;&quot; } Finally, we are done with recombination rate. "],
["git-7-analysis-v-poptrees.html", "8 (git 7) Analysis V (poptrees) 8.1 Summary 8.2 Details of analysis_fst_poptree.nf", " 8 (git 7) Analysis V (poptrees) This pipeline can be executed as follows: cd $BASE_DIR/nf/07_analysis_fst_poptree source ../sh/nextflow_alias.sh nf_run_poptree 8.1 Summary The population recombination rate is estimated within the nextflow script analysis_fst_poptree.nf (located under $BASE_DIR/nf/07_analysis_fst_poptree/), which runs on the SNPs only data set. Below is an overview of the steps involved in the analysis. (The green dot indicates the genotype input, red arrows depict output that is exported for further use.) 8.2 Details of analysis_fst_poptree.nf 8.2.1 Data preparation The nextflow script starts by opening the genotype data. #!/usr/bin/env nextflow // git 7.1 // open genotype data Channel .fromFilePairs(&quot;../../1_genotyping/4_phased/phased_mac2.vcf.{gz,gz.tbi}&quot;) .set{ vcf_fst } Next, all possible population pairs are loaded. // git 7.2 // load all possible population pairs Channel .fromPath(&quot;../../ressources/plugin/poptrees/all_crosses.tsv&quot;) .splitCsv(header:true, sep:&quot;\\t&quot;) .map{ row -&gt; [ pop1:row.pop1, pop2:row.pop2 ] } .set{ crosses_ch } Then the FST outlier regions are loaded and combined with the population pairs and the genotypes. // git 7.3 // open the focal Fst outlier regions Channel .fromPath(&quot;../../ressources/plugin/poptrees/outlier.bed&quot;) .splitCsv(header:true, sep:&quot;\\t&quot;) .map{ row -&gt; [ chrom:row.chrom, start:row.start, end:row.end, gid:row.gid ] } .combine( vcf_fst ) .combine( crosses_ch ) .set{ crosses_vcf } Now we have all the information needed to compute the average FST within each outlier region for all population pairs. // git 7.4 // compute the average Fst for all possible pair within the outlier region process outlier_fst { label &quot;L_loc_collect_fst&quot; publishDir &quot;../../2_analysis/fst/poptree/single&quot;, mode: &#39;copy&#39; input: set val( grouping ), val( vcfidx ), file( vcf ), val( cross_pop ) from crosses_vcf output: set val( grouping.gid ), val( cross_pop ), file( &quot;*.fst.tsv&quot; ) into outlier_fst_gid_ch script: &quot;&quot;&quot; echo -e &quot;CHROM\\\\tSTART\\\\tEND&quot; &gt; outl.bed echo -e &quot;${grouping.chrom}\\\\t${grouping.start}\\\\t${grouping.end}&quot; &gt;&gt; outl.bed vcfsamplenames ${vcf[0]} | \\ grep &quot;${cross_pop.pop1}&quot; &gt; pop1.pop vcfsamplenames ${vcf[0]} | \\ grep &quot;${cross_pop.pop2}&quot; &gt; pop2.pop vcftools --gzvcf ${vcf[0]} \\ --bed outl.bed \\ --keep pop1.pop \\ --keep pop2.pop \\ --weir-fst-pop pop1.pop \\ --weir-fst-pop pop2.pop \\ --stdout 2&gt; ${cross_pop.pop1}-${cross_pop.pop2}.50k.log | \\ gzip &gt; ${cross_pop.pop1}-${cross_pop.pop2}.fst.tsv.gz mFST=\\$(grep &quot;Weir and Cockerham mean Fst estimate:&quot; ${cross_pop.pop1}-${cross_pop.pop2}.50k.log | sed &#39;s/Weir and Cockerham mean Fst estimate: //&#39;) wFST=\\$(grep &quot;Weir and Cockerham weighted Fst estimate:&quot; ${cross_pop.pop1}-${cross_pop.pop2}.50k.log | sed &#39;s/Weir and Cockerham weighted Fst estimate: //&#39;) echo -e &quot;${cross_pop.pop1}-${cross_pop.pop2}\\\\t\\$mFST\\\\t\\$wFST&quot; &gt; ${cross_pop.pop1}-${cross_pop.pop2}.${grouping.gid}.fst.tsv &quot;&quot;&quot; } The FST results for all population pairs are bundled for each outlier region compiled into a single table each. // git 7.5 // collect all population pairs within each region and compile Fst table process outlier_fst_collect { label &quot;L_20g2h_outlier_fst&quot; publishDir &quot;../../2_analysis/fst/poptree/summary&quot;, mode: &#39;copy&#39; input: set val( gid ), val( cross_pop ), file( fst ) from outlier_fst_gid_ch.groupTuple() output: file( &quot;${gid}.fst.all.tsv&quot; ) into outlier_fst_collect_ch script: &quot;&quot;&quot; echo -e &quot;run\\\\tmean_fst\\\\tweighted_fst&quot; &gt; ${gid}.fst.all.tsv cat *.fst.tsv &gt;&gt; ${gid}.fst.all.tsv &quot;&quot;&quot; } At this point the preparation for the population pairs is completed. The actual neighbor joining happens later within the plotting script of Figure 4 (R/fig/plot_F4.R). // git 7.6 // neighbour joining happens within the R script R/fig/plot_F4.R "],
["git-8-analysis-vi-demographic-history.html", "9 (git 8) Analysis VI (Demographic History) 9.1 Summary 9.2 Details of analysis_msmc.nf", " 9 (git 8) Analysis VI (Demographic History) This pipeline can be executed as follows: cd $BASE_DIR/nf/08_analysis_msmc source ../sh/nextflow_alias.sh nf_run_msmc 9.1 Summary The demographic history rate is inferred within the nextflow script analysis_msmc.nf (located under $BASE_DIR/nf/08_analysis_msmc/), which runs on the SNPs only data set. Below is an overview of the steps involved in the inference. (The green dot indicates the genotype input, red arrows depict output that is exported for further use.) 9.2 Details of analysis_msmc.nf 9.2.1 Data preparation The first part of the scripts includes a large block of preparation work. In this initial block, the data masks are being generated based on the samples coverage statistics combined with the locations of idels and the reference genomes mappability. The whole script is opened by a creating a channel for the linkage groups since the coverage statistics are being created on a linkage group basis. #!/usr/bin/env nextflow // git 8.1 // create channel of linkage groups Channel .from( (&#39;01&#39;..&#39;09&#39;) + (&#39;10&#39;..&#39;19&#39;) + (&#39;20&#39;..&#39;24&#39;) ) .map{ &quot;LG&quot; + it } .into{ lg_ch1; lg_ch2; lg_ch3 } Then the phased genotype data is opened (for later use in msmc). // git 8.2 // open phased genotype data Channel .fromFilePairs(&quot;../../1_genotyping/4_phased/phased_mac2.vcf.{gz,gz.tbi}&quot;) .set{ vcf_msmc } To extract the sequencing depth for each individual, the unphased genotypes are opened as well (as this information is lost during phasing). // git 8.3 // open unphased genotype data to extract depth information Channel .fromFilePairs(&quot;../../1_genotyping/3_gatk_filtered/filterd_bi-allelic.vcf.{gz,gz.tbi}&quot;) .set{ vcf_depth } The outgroups are removed from the data set and the depth is reported for each individual. // git 8.4 // gather depth per individual process gather_depth { label &#39;L_20g2h_split_by_sample&#39; publishDir &quot;../../metadata&quot;, mode: &#39;copy&#39; input: set vcfID, file( vcf ) from vcf_depth output: file( &quot;depth_by_sample.txt&quot; ) into depth_ch script: &quot;&quot;&quot; vcfsamplenames ${vcf[0]} | \\ grep -v &quot;tor\\\\|tab\\\\|flo&quot; &gt; pop.txt vcftools \\ --gzvcf ${vcf[0]} \\ --keep pop.txt \\ --depth \\ --stdout &gt; depth_by_sample.txt &quot;&quot;&quot; } The depth information is fed into a channel so that the information is accessible for nextflow. // git 8.5 // create channel out of sequencing depth table depth_ch .splitCsv(header:true, sep:&quot;\\t&quot;) .map{ row -&gt; [ id:row.INDV, sites:row.N_SITES, depth:row.MEAN_DEPTH] } .map{ [it.id, it.sites, it.depth] } .set { depth_by_sample_ch } Next, a channel is created from all the original .bam files from the mapped sequences (git 1.6). // git 8.6 // create channel from bam files and add sample id Channel .fromPath( &#39;../../1_genotyping/0_dedup_bams/*.bam&#39; ) .map{ file -&gt; def key = file.name.toString().tokenize(&#39;.&#39;).get(0) return tuple(key, file)} .set{ sample_bams } The previously created depth information is attached to the bam channel… // git 8.7 // combine sample bams and sequencing depth sample_bams .join( depth_by_sample_ch ) .set{ sample_bam_and_depth } … and the genotype data and individual linkage groups are added. // git 8.8 // multiply the sample channel by the linkage groups sample_bam_and_depth .combine( vcf_msmc ) .combine( lg_ch1 ) .set{ samples_msmc } Now, the data is split by individual and all the additional information is passed on to the masking (git 8.10) as well as to the production of the the individuals segregating sites (git 8.11). // git 8.9 // split vcf by individual process split_vcf_by_individual { label &#39;L_20g15m_split_by_vcf&#39; input: set val( id ), file( bam ), val( sites ), val( depth ), val( vcf_id ), file( vcf ), val( lg ) from samples_msmc output: set val( id ), val( lg ), file( bam ), val( depth ), file( &quot;phased_mac2.${id}.${lg}.vcf.gz&quot; ) into ( sample_vcf, sample_vcf2 ) script: &quot;&quot;&quot; gatk --java-options &quot;-Xmx10G&quot; \\ SelectVariants \\ -R \\$REF_GENOME \\ -V ${vcf[0]} \\ -sn ${id} \\ -L ${lg}\\ -O phased_mac2.${id}.${lg}.vcf.gz &quot;&quot;&quot; } The individual coverage statistics are then being queried using the individuals average depth to create the coverage mask for each individual. // git 8.10 // create coverage mask from original mapped sequences process bam_caller { label &#39;L_36g47h_bam_caller&#39; publishDir &quot;../../ressources/coverage_masks&quot;, mode: &#39;copy&#39; , pattern: &quot;*.coverage_mask.bed.gz&quot; conda &quot;$HOME/miniconda2/envs/py3&quot; input: set val( id ), val( lg ), file( bam ), val( depth ), file( vcf ) from sample_vcf output: set val( id ), val( lg ), file( &quot;*.bam_caller.vcf.gz&quot; ), file( &quot;*.coverage_mask.bed.gz&quot; ) into coverage_by_sample_lg script: &quot;&quot;&quot; module load openssl1.0.2 samtools index ${bam} samtools mpileup -q 25 -Q 20 -C 50 -u -r ${lg} -f \\$REF_GENOME ${bam} | \\ bcftools call -c -V indels | \\ \\$BASE_DIR/py/bamHamletCaller.py ${depth} ${id}.${lg}.coverage_mask.bed.gz | \\ gzip -c &gt; ${id}.${lg}.bam_caller.vcf.gz &quot;&quot;&quot; } For each individual, the segregating sites are created. // git 8.11 // create segsites file process generate_segsites { label &quot;L_20g15m_msmc_generate_segsites&quot; publishDir &quot;../../2_analysis/msmc/segsites&quot;, mode: &#39;copy&#39; , pattern: &quot;*.segsites.vcf.gz&quot; input: set val( id ), val( lg ), file( bam ), val( depth ), file( vcf ) from sample_vcf2 output: set val( id ), val( lg ), file( &quot;*.segsites.vcf.gz&quot; ), file( &quot;*.covered_sites.bed.txt.gz&quot; ) into segsites_by_sample_lg script: &quot;&quot;&quot; zcat ${vcf} | \\ vcfAllSiteParser.py ${id} ${id}.${lg}.covered_sites.bed.txt.gz | \\ gzip -c &gt; ${id}.${lg}.segsites.vcf.gz &quot;&quot;&quot; } 9.2.1 Grouping of individuals At this point, the data masks are prepared and the samples can be assigned to their respective groups for their demographic history and and cross-coalescence rate inference. // git 8.12 // assign samples randomly across MSMC and cross coalescence runs process msmc_sample_grouping { label &quot;L_loc_msmc_grouping&quot; publishDir &quot;../../2_analysis/msmc/setup&quot;, mode: &#39;copy&#39; module &quot;R3.5.2&quot; output: file( &quot;msmc_grouping.txt&quot; ) into msmc_grouping file( &quot;msmc_cc_grouping.txt&quot; ) into cc_grouping script: &quot;&quot;&quot; Rscript --vanilla \\$BASE_DIR/R/sample_assignment_msmc.R \\ \\$BASE_DIR/R/distribute_samples_msmc_and_cc.R \\ \\$BASE_DIR/R/cross_cc.R \\ \\$BASE_DIR/metadata/sample_info.txt \\ msmc &quot;&quot;&quot; } The results of the random assignment are being fed into a channel. // git 8.13 // read grouping into a channel msmc_grouping .splitCsv(header:true, sep:&quot;\\t&quot;) .map{ row -&gt; [ run:row.msmc_run, spec:row.spec, geo:row.geo, group_nr:row.group_nr, group_size:row.group_size, samples:row.samples ] } .set { msmc_runs } Since this script uses an unorthodox way of making the results of the data preparation available to all following processes by exporting them back to the root folder, the following dummy process is installed to wait for the data preparation to finish before proceeding with the workflow. // git 8.14 // wait for bam_caller and generate_segsites to finish: /*this &#39;.collect&#39; is only meant to wait until the channel is done, files are being redirected via publishDir*/ coverage_by_sample_lg.collect().map{ &quot;coverage done!&quot; }.into{ coverage_done; coverage_cc } segsites_by_sample_lg.collect().map{ &quot;segsites done!&quot; }.into{ segsites_done; segsites_cc } To set up the msmc2 runs, the sample grouping is waiting for the dummy process to finish. // git 8.15 // attach masks to MSMC group assignment lg_ch2 .combine( msmc_runs ) .combine( coverage_done ) .combine( segsites_done ) .map{[it[0], it[1].run, it[1]]} .set{ msmc_grouping_after_segsites } Then, the specific msmc2 input files are compiled from the combined masks of the involved samples (for each linkage group individually). // git 8.16 // generating MSMC input files (4 or 3 inds per species) process generate_multihetsep { label &quot;L_120g40h_msmc_generate_multihetsep&quot; publishDir &quot;../../2_analysis/msmc/input/run_${run}&quot;, mode: &#39;copy&#39; , pattern: &quot;*.multihetsep.txt&quot; conda &quot;$HOME/miniconda2/envs/py3&quot; input: /* content msmc_gr: val( msmc_run ), val( spec ), val( geo ), val( group_nr ), val( group_size ), val( samples ) */ /*[LG20, [msmc_run:45, spec:uni, geo:pan, group_nr:4, group_size:3, samples:ind1, ind2, ind3], coverage done!, segsites done!]*/ set val( lg ), val( run ), msmc_gr from msmc_grouping_after_segsites output: set val( run ), val( lg ), val( msmc_gr.spec ), val( msmc_gr.geo ), val( msmc_gr.group_size ), file( &quot;msmc_run.*.multihetsep.txt&quot; ) into msmc_input_lg script: &quot;&quot;&quot; COVDIR=&quot;\\$BASE_DIR/ressources/coverage_masks/&quot; SMP=\\$(echo ${msmc_gr.samples} | \\ sed &quot;s|, |\\\\n--mask=\\${COVDIR}|g; s|^|--mask=\\${COVDIR}|g&quot; | \\ sed &quot;s/\\$/.${lg}.coverage_mask.bed.gz/g&quot; | \\ echo \\$( cat ) ) SEGDIR=&quot;\\$BASE_DIR/2_analysis/msmc/segsites/&quot; SEG=\\$(echo ${msmc_gr.samples} | \\ sed &quot;s|, |\\\\n\\${SEGDIR}|g; s|^|\\${SEGDIR}|g&quot; | \\ sed &quot;s/\\$/.${lg}.segsites.vcf.gz/g&quot; | \\ echo \\$( cat ) ) generate_multihetsep.py \\ \\$SMP \\ --mask=\\$BASE_DIR/ressources/mappability_masks/${lg}.mapmask.bed.txt.gz \\ --negative_mask=\\$BASE_DIR/ressources/indel_masks/indel_mask.${lg}.bed.gz \\ \\$SEG &gt; msmc_run.${msmc_gr.run}.${msmc_gr.spec}.${msmc_gr.geo}.${lg}.multihetsep.txt &quot;&quot;&quot; } The input files of all linkage group are collected for each sample grouping. // git 8.17 // collect all linkage groups for each run msmc_input_lg .groupTuple() .set {msmc_input} And finally msmc2 is executed to infer the demographic history of the involved samples. // git 8.18 // run msmc process msmc_run { label &quot;L_190g100h_msmc_run&quot; publishDir &quot;../../2_analysis/msmc/output/&quot;, mode: &#39;copy&#39; , pattern: &quot;*.final.txt&quot; publishDir &quot;../../2_analysis/msmc/loops/&quot;, mode: &#39;copy&#39; , pattern: &quot;*.loop.txt&quot; input: set msmc_run, lg , spec, geo, group_size, file( hetsep ) from msmc_input output: file(&quot;*.msmc2.*.txt&quot;) into msmc_output script: &quot;&quot;&quot; NHAP=\\$(echo \\$(seq 0 \\$((${group_size[0]}*2-1))) | sed &#39;s/ /,/g&#39; ) INFILES=\\$( echo ${hetsep} ) msmc2 \\ -m 0.00254966 -t 8 \\ -p 1*2+25*1+1*2+1*3 \\ -o run${msmc_run}.${spec[0]}.${geo[0]}.msmc2 \\ -I \\${NHAP} \\ \\${INFILES} &quot;&quot;&quot; } The process cross-coalescence rate is similar (with git 8.19 being the equivalent of git 8.13). So, again the grouping information in fed into a channel. // git 8.19 // generate MSMC cross coalescence input files (2 inds x 2 species) cc_grouping .splitCsv(header:true, sep:&quot;\\t&quot;) .map{ row -&gt; [ run:row.run_nr, geo:row.geo, spec_1:row.spec_1, spec_2:row.spec_2, contrast_nr:row.contrast_nr, samples_1:row.samples_1, samples_2:row.samples_2 ] } .set { cc_runs } The groups wait for the data preparation to finish. // git 8.20 // attach masks to cross coalescence group assignment lg_ch3 .combine( cc_runs ) .combine( coverage_cc ) .combine( segsites_cc ) .map{[it[0], it[1].run, it[1]]} .set{ cc_grouping_after_segsites } The msmc2 input files are being compiled based on the involved samples (for each linkage group). // git 8.21 // create multihetsep files (combination off all 4 individuals) process generate_multihetsep_cc { label &quot;L_105g30h_cc_generate_multihetsep&quot; publishDir &quot;../../2_analysis/cross_coalescence/input/run_${run}&quot;, mode: &#39;copy&#39; , pattern: &quot;*.multihetsep.txt&quot; conda &quot;$HOME/miniconda2/envs/py3&quot; input: /* content cc_gr: val( run_nr ), val( geo ), val( spec_1 ), val( spec_2 ), val( contrast_nr ), val( samples_1 ), val( samples_2 ) */ set val( lg ), val( run ), cc_gr from cc_grouping_after_segsites output: set val( cc_gr.run ), val( lg ), val( cc_gr.spec_1 ), val( cc_gr.spec_2 ), val( cc_gr.geo ), val( cc_gr.contrast_nr ), val( cc_gr.samples_1 ), val( cc_gr.samples_2 ), file( &quot;cc_run.*.multihetsep.txt&quot; ) into cc_input_lg script: &quot;&quot;&quot; COVDIR=&quot;\\$BASE_DIR/ressources/coverage_masks/&quot; SMP1=\\$(echo ${cc_gr.samples_1} | \\ sed &quot;s|, |\\\\n--mask=\\${COVDIR}|g; s|^|--mask=\\${COVDIR}|g&quot; | \\ sed &quot;s/\\$/.${lg}.coverage_mask.bed.gz/g&quot; | \\ echo \\$( cat ) ) SMP2=\\$(echo ${cc_gr.samples_2} | \\ sed &quot;s|, |\\\\n--mask=\\${COVDIR}|g; s|^|--mask=\\${COVDIR}|g&quot; | \\ sed &quot;s/\\$/.${lg}.coverage_mask.bed.gz/g&quot; | \\ echo \\$( cat ) ) SEGDIR=&quot;\\$BASE_DIR/2_analysis/msmc/segsites/&quot; SEG1=\\$(echo ${cc_gr.samples_1} | \\ sed &quot;s|, |\\\\n\\${SEGDIR}|g; s|^|\\${SEGDIR}|g&quot; | \\ sed &quot;s/\\$/.${lg}.segsites.vcf.gz/g&quot; | \\ echo \\$( cat ) ) SEG2=\\$(echo ${cc_gr.samples_2} | \\ sed &quot;s|, |\\\\n\\${SEGDIR}|g; s|^|\\${SEGDIR}|g&quot; | \\ sed &quot;s/\\$/.${lg}.segsites.vcf.gz/g&quot; | \\ echo \\$( cat ) ) generate_multihetsep.py \\ \\${SMP1} \\ \\${SMP2} \\ --mask=\\$BASE_DIR/ressources/mappability_masks/${lg}.mapmask.bed.txt.gz \\ --negative_mask=\\$BASE_DIR/ressources/indel_masks/indel_mask.${lg}.bed.gz \\ \\${SEG1} \\ \\${SEG2} \\ &gt; cc_run.${run}.${cc_gr.spec_1}-${cc_gr.spec_2}.${cc_gr.contrast_nr}.${cc_gr.geo}.${lg}.multihetsep.txt &quot;&quot;&quot; } The input files of all linkage group are collected for each sample grouping. // git 8.22 // collect all linkage groups for each run cc_input_lg .groupTuple() .set {cc_input} And msmc2 is executed to infer the cross-coalescence rate of the involved samples. // git 8.23 // run cross coalescence process cc_run { label &quot;L_190g10ht24_cc_run&quot; publishDir &quot;../../2_analysis/cross_coalescence/output/&quot;, mode: &#39;copy&#39; tag &quot;${cc_run}-${geo[0]}:${spec1[0]}/${spec2[0]}&quot; conda &quot;$HOME/miniconda2/envs/py3&quot; input: set cc_run, lg , spec1, spec2, geo, contr_nr, samples_1, samples_2, file( hetsep ) from cc_input output: file(&quot;cc_run.*.final.txt.gz&quot;) into cc_output script: &quot;&quot;&quot; INFILES=\\$( echo ${hetsep} ) POP1=\\$( echo &quot;${samples_1}&quot; | sed &#39;s/\\\\[//g; s/, /,/g; s/\\\\]//g&#39; ) POP2=\\$( echo &quot;${samples_2}&quot; | sed &#39;s/\\\\[//g; s/, /,/g; s/\\\\]//g&#39; ) msmc2 \\ -m 0.00255863 -t 24 \\ -p 1*2+25*1+1*2+1*3 \\ -o cc_run.${cc_run}.${spec1[0]}.msmc \\ -I 0,1,2,3 \\ \\${INFILES} msmc2 \\ -m 0.00255863 -t 24 \\ -p 1*2+25*1+1*2+1*3 \\ -o cc_run.${cc_run}.${spec2[0]}.msmc \\ -I 4,5,6,7 \\ \\${INFILES} msmc2 \\ -m 0.00255863 -t 24 \\ -p 1*2+25*1+1*2+1*3 \\ -o cc_run.${cc_run}.cross.msmc \\ -I 0,1,2,3,4,5,6,7 \\ -P 0,0,0,0,1,1,1,1 \\ \\${INFILES} combineCrossCoal.py \\ cc_run.${cc_run}.cross.msmc.final.txt \\ cc_run.${cc_run}.${spec1[0]}.msmc.final.txt \\ cc_run.${cc_run}.${spec2[0]}.msmc.final.txt | \\ gzip &gt; cc_run.${cc_run}.final.txt.gz &quot;&quot;&quot; } Finally, we are done with the inference of the demographic history. "],
["git-9-analysis-vii-hybridization.html", "10 (git 9) Analysis VII (hybridization) 10.1 Summary 10.2 Details of analysis_hybridization.nf", " 10 (git 9) Analysis VII (hybridization) This pipeline can be executed as follows: cd $BASE_DIR/nf/09_analysis_hybridization source ../sh/nextflow_alias.sh nf_run_hybrid 10.1 Summary The population recombination rate is estimated within the nextflow script analysis_hybridization.nf (located under $BASE_DIR/nf/09_analysis_hybridization/), which runs on the SNPs only data set. Below is an overview of the steps involved in the analysis. (The green dot indicates the genotype input, red arrows depict output that is exported for further use.) 10.2 Details of analysis_hybridization.nf 10.2.1 Data preparation The nextflow script starts by opening the genotype data. #!/usr/bin/env nextflow // git 9.1 // open genotype data Channel .fromFilePairs(&quot;../../1_genotyping/4_phased/phased_mac2.vcf.{gz,gz.tbi}&quot;) .into{ vcf_loc1; vcf_loc2; vcf_loc3 } // git 9.2 // initialize location channel Channel .from( &quot;bel&quot;, &quot;hon&quot;, &quot;pan&quot;) .set{ locations_ch } // git 9.3 // define location specific sepcies set Channel.from( [[1, &quot;ind&quot;], [2, &quot;may&quot;], [3, &quot;nig&quot;], [4, &quot;pue&quot;], [5, &quot;uni&quot;]] ).into{ bel_spec1_ch; bel_spec2_ch } Channel.from( [[1, &quot;abe&quot;], [2, &quot;gum&quot;], [3, &quot;nig&quot;], [4, &quot;pue&quot;], [5, &quot;ran&quot;], [6, &quot;uni&quot;]] ).into{ hon_spec1_ch; hon_spec2_ch } Channel.from( [[1, &quot;nig&quot;], [2, &quot;pue&quot;], [3, &quot;uni&quot;]] ).into{ pan_spec1_ch; pan_spec2_ch } // git 9.4 // prepare pairwise new_hybrids // ------------------------------ /* (create all possible species pairs depending on location and combine with genotype subset (for the respective location))*/ bel_pairs_ch = Channel.from( &quot;bel&quot; ) .combine( vcf_loc1 ) .combine(bel_spec1_ch) .combine(bel_spec2_ch) .filter{ it[3] &lt; it[5] } .map{ it[0,1,2,4,6]} hon_pairs_ch = Channel.from( &quot;hon&quot; ) .combine( vcf_loc2 ) .combine(hon_spec1_ch) .combine(hon_spec2_ch) .filter{ it[3] &lt; it[5] } .map{ it[0,1,2,4,6]} pan_pairs_ch = Channel.from( &quot;pan&quot; ) .combine( vcf_loc3 ) .combine(pan_spec1_ch) .combine(pan_spec2_ch) .filter{ it[3] &lt; it[5] } .map{ it[0,1,2,4,6]} bel_pairs_ch.concat( hon_pairs_ch, pan_pairs_ch ).set { all_fst_pairs_ch } // git 9.5 // comute pairwise fsts for SNP filtering process fst_run { label &#39;L_20g45m_fst_run&#39; tag &quot;${spec1}${loc}-${spec2}${loc}&quot; input: set val( loc ), val( vcfidx ), file( vcf ), val( spec1 ), val( spec2 ) from all_fst_pairs_ch output: set val( loc ), val( spec1 ), val( spec2 ), file( &quot;${vcf[0]}&quot; ), file( &quot;*.fst.tsv.gz&quot; ), file( &quot;${spec1}${loc}.pop&quot;), file( &quot;${spec2}${loc}.pop&quot;) into fst_SNPS script: &quot;&quot;&quot; vcfsamplenames ${vcf[0]} | grep ${spec1}${loc} &gt; ${spec1}${loc}.pop vcfsamplenames ${vcf[0]} | grep ${spec2}${loc} &gt; ${spec2}${loc}.pop vcftools --gzvcf ${vcf[0]} \\ --weir-fst-pop ${spec1}${loc}.pop \\ --weir-fst-pop ${spec2}${loc}.pop \\ --stdout | gzip &gt; ${spec1}${loc}-${spec2}${loc}.fst.tsv.gz &quot;&quot;&quot; } // git 9.6 // select the 800 most differentiated SNPs for each population pair process filter_fst { label &#39;L_8g15m_filter_fst&#39; tag &quot;${spec1}${loc}-${spec2}${loc}&quot; input: set val( loc ), val( spec1 ), val( spec2 ), file( vcf ), file( fst ), file( pop1 ), file( pop2 ) from fst_SNPS output: set val( loc ), val( spec1 ), val( spec2 ), file( vcf ), file( pop1 ), file( pop2 ), file( &quot;*SNPs.snps&quot; ) into filter_SNPs script: &quot;&quot;&quot; Rscript --vanilla \\$BASE_DIR/R/filter_snps.R ${fst} 800 ${spec1}${loc}-${spec2}${loc} &quot;&quot;&quot; } // git 9.7 // filter the SNP set by min distance (5kb), than randomly pick 80 SNPs // then reformat newhybrid input process prep_nh_input { label &#39;L_8g15m_prep_nh&#39; tag &quot;${spec1}${loc}-${spec2}${loc}&quot; input: set val( loc ), val( spec1 ), val( spec2 ), file( vcf ), file( pop1 ), file( pop2 ), file( snps ) from filter_SNPs output: set val( loc ), val( spec1 ), val( spec2 ), file( &quot;*_individuals.txt&quot; ), file( &quot;*.80SNPs.txt&quot;) into newhybrids_input script: &quot;&quot;&quot; vcftools \\ --gzvcf ${vcf} \\ --keep ${pop1} \\ --keep ${pop2} \\ --thin 5000 \\ --out newHyb.${spec1}${loc}-${spec2}${loc} \\ --positions ${snps} \\ --recode grep &#39;#&#39; newHyb.${spec1}${loc}-${spec2}${loc}.recode.vcf &gt; newHyb.${spec1}${loc}-${spec2}${loc}.80SNPs.vcf grep -v &#39;#&#39; newHyb.${spec1}${loc}-${spec2}${loc}.recode.vcf | \\ shuf -n 80 | \\ sort -k 1 -k2 &gt;&gt; newHyb.${spec1}${loc}-${spec2}${loc}.80SNPs.vcf grep &#39;#CHROM&#39; newHyb.${spec1}${loc}-${spec2}${loc}.80SNPs.vcf | \\ cut -f 10- | \\ sed &#39;s/\\\\t/\\\\n/g&#39; &gt; newHyb.${spec1}${loc}-${spec2}${loc}.80SNPs_individuals.txt /usr/bin/java -Xmx1024m -Xms512M \\ -jar \\$SFTWR/PGDSpider/PGDSpider2-cli.jar \\ -inputfile newHyb.${spec1}${loc}-${spec2}${loc}.80SNPs.vcf \\ -inputformat VCF \\ -outputfile newHyb.${spec1}${loc}-${spec2}${loc}.80SNPs.txt \\ -outputformat NEWHYBRIDS \\ -spid \\$BASE_DIR/ressources/vcf2nh.spid &quot;&quot;&quot; } // git 9.8 // Run new hybrids // (copy of nh_input is needed because nh can&#39;t read links) process run_nh { label &#39;L_20g15h4x_run_nh&#39; tag &quot;${spec1}${loc}-${spec2}${loc}&quot; publishDir &quot;../../2_analysis/newhyb/&quot;, mode: &#39;copy&#39; input: set val( loc ), val( spec1 ), val( spec2 ), file( inds ), file( snps ) from newhybrids_input output: set file( &quot;nh_input/NH.Results/newHyb.*/*_individuals.txt&quot; ), file( &quot;nh_input/NH.Results/newHyb.*/*_PofZ.txt&quot; ) into newhybrids_output script: &quot;&quot;&quot; mkdir -p nh_input cp ${snps} nh_input/${snps} cp ${inds} nh_input/${inds} Rscript --vanilla \\$BASE_DIR/R/run_newhybrids.R &quot;&quot;&quot; } Finally, we are done with XX. "],
["git-10-analysis-viii-admixture.html", "11 (git 10) Analysis VIII (admixture) 11.1 Summary 11.2 Details of analysis_admixture.nf", " 11 (git 10) Analysis VIII (admixture) This pipeline can be executed as follows: cd $BASE_DIR/nf/10_analysis_admixture source ../sh/nextflow_alias.sh nf_run_admixture 11.1 Summary The population recombination rate is estimated within the nextflow script analysis_admixture.nf (located under $BASE_DIR/nf/10_analysis_admixture/), which runs on the SNPs only data set. Below is an overview of the steps involved in the analysis. (The green dot indicates the genotype input, red arrows depict output that is exported for further use.) 11.2 Details of analysis_admixture.nf 11.2.1 Data preparation The nextflow script starts by opening the genotype data. #!/usr/bin/env nextflow // git 10.1 // open genotype data Channel .fromFilePairs(&quot;../../1_genotyping/4_phased/phased_mac2.vcf.{gz,gz.tbi}&quot;) .set{ vcf_ch } Next, we select the range k values that we we want to explore within the admixture analysis. // git 10.2 // Set different k values for the admixture analysis Channel .from( 2..15 ) .set{ k_ch } Then, we open the FST outlier regions within which we want to run the admixture analysis. // git 10.3 // load Fst outlier regions Channel .fromPath(&quot;../../ressources/plugin/poptrees/outlier.bed&quot;) .splitCsv(header:true, sep:&quot;\\t&quot;) .map{ row -&gt; [ chrom:row.chrom, start:row.start, end:row.end, gid:row.gid ] } .combine( vcf_ch ) .set{ vcf_admx } Then, we subset the genotypes to each outlier regions of interest respectively and reformat them to the plink genotype format. // git 10.4 // subset genotypes to the outlier region and reformat process plink12 { label &#39;L_20g2h_plink12&#39; tag &quot;${grouping.gid}&quot; input: set val( grouping ), val( vcfidx ), file( vcf ) from vcf_admx output: set val( grouping ), file( &quot;hapmap.*.ped&quot; ), file( &quot;hapmap.*.map&quot; ), file( &quot;hapmap.*.nosex&quot; ), file( &quot;pop.txt&quot; ) into admx_plink script: &quot;&quot;&quot; echo -e &quot;CHROM\\\\tSTART\\\\tEND&quot; &gt; outl.bed echo -e &quot;${grouping.chrom}\\\\t${grouping.start}\\\\t${grouping.end}&quot; &gt;&gt; outl.bed vcfsamplenames ${vcf[0]} | \\ grep -v &quot;tor\\\\|tab\\\\|flo&quot; | \\ awk &#39;{print \\$1&quot;\\\\t&quot;\\$1}&#39; | \\ sed &#39;s/\\\\t.*\\\\(...\\\\)\\\\(...\\\\)\\$/\\\\t\\\\1\\\\t\\\\2/g&#39; &gt; pop.txt vcftools \\ --gzvcf ${vcf[0]} \\ --keep pop.txt \\ --bed outl.bed \\ --plink \\ --out admx_plink plink \\ --file admx_plink \\ --recode12 \\ --out hapmap.${grouping.gid} &quot;&quot;&quot; } Then, we combine the reformatted genotypes with the k values… // git 10.5 // combine genoutype subsets with k values admx_prep = k_ch.combine( admx_plink ) … and run admixture. // git 10.6 // run admixture process admixture_all { label &#39;L_20g4h_admixture_all&#39; publishDir &quot;../../2_analysis/admixture/&quot;, mode: &#39;copy&#39; tag &quot;${grouping.gid}.${k}&quot; input: set val( k ), val( grouping ), file( ped ), file( map ), file( nosex ), file( pop ) from admx_prep output: set val( &quot;dummy&quot; ), file( &quot;*.out&quot; ), file( &quot;*.Q&quot; ), file( &quot;*.txt&quot; ) into admx_log script: &quot;&quot;&quot; mv ${pop} pop.${grouping.gid}.${k}.txt admixture --cv ${ped} ${k} | tee log.${grouping.gid}.${k}.out &quot;&quot;&quot; } "],
["git-11-visualization.html", "12 (git 11) Visualization", " 12 (git 11) Visualization After all nextflow pipelines are successfully run to completion, each Figure (and Suppl. Figure) of the manuscript can be re-created with its respective R script located under R/fig. These are executable R scripts that can be launched from the base directory; Rscript --vanilla R/fig/plot_Fxyz.R input1 input2 ... For convenience, there also exists a bash script that can be used to re-create all Figures in one go (git 11): cd $BASE_DIR bash sh/create_figures.sh After running create_figures.sh, Figures 1 - 5 and Suppl. Figures 1 - 9 should be created withing the folder figures/. In the remaining documentation, the individual Visualization scripts are going to discussed in detail. "],
["figure-1.html", "13 Figure 1 13.1 Summary 13.2 Details of plot_F1.R", " 13 Figure 1 13.1 Summary This is the accessory documentation of Figure 1. The Figure can be recreated by running the R script plot_F1.R: cd $BASE_DIR Rscript --vanilla R/fig/plot_F1.R 2_analysis/dxy/50k/ 2_analysis/fst/50k/ 13.2 Details of plot_F1.R In the following, the individual steps of the R script are documented. It is an executable R script that depends on the accessory R package GenomicOriginsScripts. 13.2.1 Config The scripts start with a header that contains copy &amp; paste templates to execute or debug the script: #!/usr/bin/env Rscript # run from terminal: # Rscript --vanilla R/fig/plot_F1.R \\ # 2_analysis/dxy/50k/ 2_analysis/fst/50k/ 2_analysis/summaries/fst_globals.txt # =============================================================== # This script produces Figure 1 of the study &quot;Ancestral variation, hybridization and modularity # fuel a marine radiation&quot; by Hench, McMillan and Puebla # --------------------------------------------------------------- # =============================================================== # args &lt;- c(&#39;2_analysis/dxy/50k/&#39;, &#39;2_analysis/fst/50k/&#39;, &#39;2_analysis/summaries/fst_globals.txt&#39;) # script_name &lt;- &quot;R/fig/plot_F1.R&quot; The next section processes the input from the command line. It stores the arguments in the vector args. The R package GenomicOriginsScripts is loaded and the script name and the current working directory are stored inside variables (script_name, plot_comment). This information will later be written into the meta data of the figure to help us tracing back the scripts that created the figures in the future. Then we drop all the imported information besides the arguments following the script name and print the information to the terminal. args &lt;- commandArgs(trailingOnly=FALSE) # setup ----------------------- library(GenomicOriginsScripts) library(hypoimg) cat(&#39;\\n&#39;) script_name &lt;- args[5] %&gt;% str_remove(., &#39;--file=&#39;) plot_comment &lt;- script_name %&gt;% str_c(&#39;mother-script = &#39;, getwd(), &#39;/&#39;, .) args &lt;- process_input(script_name, args) #&gt; ── Script: R/fig/plot_F1.R ────────────────────────────────────────────── #&gt; Parameters read: #&gt; ★ 1: 2_analysis/dxy/50k/ #&gt; ★ 2: 2_analysis/fst/50k/ #&gt; ★ 3: 2_analysis/summaries/fst_globals.txt #&gt; ─────────────────────────────────────────── /current/working/directory ── The directories for the different data types are received and stored in respective variables. Also, we set a few parameters for the plot layout: # config ----------------------- dxy_dir &lt;- as.character(args[1]) fst_dir &lt;- as.character(args[2]) fst_globals &lt;- as.character(args[3]) wdh &lt;- .3 # The width of the boxplots scaler &lt;- 20 # the ratio of the Fst and the dxy axis clr_sec &lt;- &#39;gray&#39; # the color of the secondary axis (dxy) 13.2.2 Data import We begin with the data import by first collecting the paths to all files containing either FST or dXY data (dir()), then iterating the import function over all files (map(summarize_fst)) and finally combining the outputs into a single tibble (bind_rows()). This is done for both FST and dXY. # start script ------------------- # import Fst fst_files &lt;- dir(fst_dir, pattern = &#39;.50k.windowed.weir.fst.gz&#39;) fst_data &lt;- str_c(fst_dir,fst_files) %&gt;% purrr::map(summarize_fst) %&gt;% bind_rows() # lookup dxy files dxy_files &lt;- dir(dxy_dir) # import dxy dxy_data &lt;- str_c(dxy_dir,dxy_files) %&gt;% purrr::map(summarize_dxy) %&gt;% bind_rows() We use the genome wide average FST to rank the individual pair wise comparisons. # determine fst ranking fst_order &lt;- fst_data %&gt;% select(run, `mean_weighted-fst`) %&gt;% mutate(run = fct_reorder(run, `mean_weighted-fst`)) Then, we merge the FST and dXY data sets and do quite a bit of data wrangling to create a rescaled dXY value and to prepare the placement of the boxplots. # merge fst and dxy cc_data # (large parts of this code are now unnecessary after the separation of dxy and # fst plots into separate panels b &amp; c) data &lt;- left_join(fst_data, dxy_data) %&gt;% select(c(8,1:7,9:15)) %&gt;% # reformat table to enable parallel plotting (with secondary axis) gather(key = &#39;stat&#39;, value = &#39;val&#39;, 2:15) %&gt;% # sumstat contains the values needed to plot the boxplots (quartiles, etc) separate(stat, into = c(&#39;sumstat&#39;, &#39;popstat&#39;), sep = &#39;_&#39;) %&gt;% # duplicate dxy values scaled to fst range mutate(val_scaled = ifelse(popstat == &#39;dxy&#39;, val * scaler , val)) %&gt;% unite(temp, val, val_scaled) %&gt;% # separate th eoriginal values from the scales ons (scaled = secondary axis) spread(.,key = &#39;sumstat&#39;,value = &#39;temp&#39;) %&gt;% separate(mean, into = c(&#39;mean&#39;,&#39;mean_scaled&#39;),sep = &#39;_&#39;, convert = TRUE) %&gt;% separate(median, into = c(&#39;median&#39;,&#39;median_scaled&#39;), sep = &#39;_&#39;, convert = TRUE) %&gt;% separate(sd, into = c(&#39;sd&#39;,&#39;sd_scaled&#39;),sep = &#39;_&#39;, convert = TRUE) %&gt;% separate(lower, into = c(&#39;lower&#39;,&#39;lower_scaled&#39;), sep = &#39;_&#39;, convert = TRUE) %&gt;% separate(upper, into = c(&#39;upper&#39;,&#39;upper_scaled&#39;), sep = &#39;_&#39;, convert = TRUE) %&gt;% separate(lowpoint, into = c(&#39;lowpoint&#39;,&#39;lowpoint_scaled&#39;), sep = &#39;_&#39;, convert = TRUE) %&gt;% separate(highpoint, into = c(&#39;highpoint&#39;,&#39;highpoint_scaled&#39;), sep = &#39;_&#39;, convert = TRUE) %&gt;% # include &quot;dodge&quot;-positions for side-by-side plotting (secondary axis) mutate(loc = str_sub(run,4,6), run = factor(run, levels = levels(fst_order$run)), x = as.numeric(run) , x_dodge = ifelse(popstat == &#39;dxy&#39;, x + .25, x - .25), x_start_dodge = x_dodge - wdh/2, x_end_dodge = x_dodge + wdh/2, popstat_loc = str_c(popstat,&#39;[&#39;,loc,&#39;]&#39;)) At this point, the data is ready for the boxplots. But first we are going to prepare the networks of pairwise comparisons. For this we create a tibble of the runs with their respective rank. Then, we prepare a config table with one row per location, storing the parameters needed for the layout function for the networks. We need to define the location, the number of species at the location, the short three letter ID of those species and a weight parameter that is shifting the comparison label on the link within the networks. Finally, we create one network plot per location. # sort run by average genome wide Fst run_ord &lt;- tibble(run = levels(data$run), run_ord = 1:length(levels(data$run))) # onderlying structure for the network plots networx &lt;- tibble( loc = c(&#39;bel&#39;,&#39;hon&#39;, &#39;pan&#39;), n = c(5,6,3), label = list(str_c(c(&#39;ind&#39;,&#39;may&#39;,&#39;nig&#39;,&#39;pue&#39;,&#39;uni&#39;),&#39;bel&#39;), str_c(c(&#39;abe&#39;,&#39;gum&#39;,&#39;nig&#39;,&#39;pue&#39;,&#39;ran&#39;,&#39;uni&#39;),&#39;hon&#39;), str_c(c(&#39;nig&#39;,&#39;pue&#39;,&#39;uni&#39;),&#39;pan&#39;)), weight = c(1,1.45,1)) %&gt;% purrr::pmap(network_layout) %&gt;% bind_rows() # plot the individual networks by location plot_list &lt;- networx %&gt;% purrr::pmap(plot_network, node_lab_shift = .2) 13.2.3 Plotting To create the first panel of Figure 1, we combine the three networks and label the locations. # assemble panel a p1 &lt;- cowplot::plot_grid( grid::textGrob(&#39;Belize&#39;), grid::textGrob(&#39;Honduras&#39;), grid::textGrob(&#39;Panama&#39;), plot_list[[1]], plot_list[[2]], plot_list[[3]], ncol = 3, rel_heights = c(.1,1)) Now, we can create the second panel of Figure 1, by plotting our prepared data tibble. We are going to plot each boxplot element as a single layer. (This, might seem a little cumbersome given geom_boxplot(), but this approach was chosen for specific fine tuning of the positioning, dropping of outliers and reducing runtime during the plotting phase - otherwise the entire genome wide data set would have been carried though whole script. By now, this indirect approach is also somewhat obsolete since the FST and dXY boxplots are now separated into several panels) The FST boxplots are created for panel b… # assemble panel b p2 &lt;- data %&gt;% filter(popstat == &quot;weighted-fst&quot;) %&gt;% ggplot(aes(color = loc)) + geom_segment(aes(x = x, xend = x, y = lowpoint, yend = highpoint))+ geom_rect(aes(xmin = x - wdh, xmax = x + wdh, ymin = lower, ymax = upper), fill = &#39;white&#39;)+ geom_segment(aes(x = x - wdh, xend = x + wdh, y = median, yend = median), lwd = .9)+ geom_point(aes(x = x, y = mean), shape = 21, size = .7, fill = &#39;white&#39;)+ scale_x_continuous(breaks = 1:28) + scale_y_continuous(#breaks = c(0,.05,.1,.15), name = expression(italic(F[ST])))+ scale_color_manual(values = c(make_faint_clr(&#39;bel&#39;), make_faint_clr(&#39;hon&#39;), make_faint_clr(&#39;pan&#39;))[c(2,4,6)])+ coord_cartesian(xlim = c(0,29), expand = c(0,0))+ theme_minimal()+ theme(axis.title.x = element_blank(), legend.position = &#39;none&#39;, strip.placement = &#39;outside&#39;, strip.text = element_text(size = 12), panel.grid.major.x = element_blank(), panel.grid.minor.y = element_blank(), axis.text.y.right = element_text(color = clr_sec), axis.title.y.right = element_text(color = clr_sec)) … and the dXY boxplotsfor panel c: # assemble panel c p3 &lt;- data %&gt;% filter(popstat == &quot;dxy&quot;) %&gt;% ggplot(aes(color = loc)) + geom_segment(aes(x = x, xend = x, y = lowpoint, yend = highpoint))+ geom_rect(aes(xmin = x - wdh, xmax = x + wdh, ymin = lower, ymax = upper), fill = &#39;white&#39;)+ geom_segment(aes(x = x - wdh, xend = x + wdh, y = median, yend = median),lwd = .9)+ geom_point(aes(x = x, y = mean), shape = 21, size = .7, fill = &#39;white&#39;)+ scale_x_continuous(breaks = 1:28) + scale_y_continuous( expression(italic(d[XY])), breaks = c(0,.0025,.005,.0075,.01), limits = c(0,.01))+ scale_color_manual(values = c(make_faint_clr(&#39;bel&#39;), make_faint_clr(&#39;hon&#39;), make_faint_clr(&#39;pan&#39;))[c(2,4,6)])+ coord_cartesian(xlim = c(0,29), expand = c(0,0))+ theme_minimal()+ theme(axis.title.x = element_blank(), legend.position = &#39;none&#39;, strip.placement = &#39;outside&#39;, strip.text = element_text(size = 12), panel.grid.major.x = element_blank(), panel.grid.minor.y = element_blank(), axis.text.y.right = element_text(color = clr_sec), axis.title.y.right = element_text(color = clr_sec)) The panels b and c are merged to crate the lower half of the figure and then the entire figure is being brought together. # merge panel b &amp; c p23 &lt;- cowplot::plot_grid(p2,p3, ncol = 2, labels = letters[2:3] %&gt;% project_case()) # merge all panels p_done &lt;- cowplot::plot_grid(p1, p23, ncol = 1, rel_heights = c(.9,1), labels = c(letters[1], NULL) %&gt;% project_case()) Finally, we can export Figure 1. hypo_save(p_done, filename = &#39;figures/F1.pdf&#39;, width = 11, height = 6.5, comment = plot_comment) The function hypo_save() is simply a wrapper around ggsave(), that will write the name of the currently running script into the meta data of the plot (after the plot has been exported). The benefit of this is that you can read this information later to remember how a specific plot was created using hypo_show_metadata(). This is done using exiftool and has currently only been tested on my linux system. If this does not work for you, simple replace hypo_save() with ggsave() and drop the comment parameter. hypo_show_metadata(&#39;figures/F1.pdf&#39;) #&gt; [1] &quot;mother-script = /current/working/directory/R/fig/plot_F1.R&quot; 13.2.4 Table export This script is also used to compile the sub-tables Suppl. Table 3 a - c. The table is compiled from the weighted average FST values which are imported from the nextflow results and the average dXY values which are computed within this R script. First, a proto-version of Suppl. Table 3 is created. # compile fst and dxy table (table 1) for the manuscript table_all &lt;- dxy_data %&gt;% select(run, mean_dxy) %&gt;% left_join( vroom::vroom(fst_globals, delim = &#39;\\t&#39;, col_names = c(&#39;loc&#39;,&#39;run&#39;,&#39;mean&#39;,&#39;weighted_fst&#39;)) %&gt;% mutate(run = str_c(loc,&#39;-&#39;,run) %&gt;% reformat_run_name()) %&gt;% select(run, weighted_fst)) %&gt;% pivot_longer(names_to = &#39;stat&#39;,2:3) %&gt;% separate(run, into = c(&#39;pop1&#39;, &#39;pop2&#39;), sep = &#39;-&#39;) %&gt;% mutate(prep1 = ifelse(stat == &quot;weighted_fst&quot;, pop2,pop1), prep2 = ifelse(stat == &quot;weighted_fst&quot;, pop1,pop2), pop1 = factor(prep1, levels = pop_levels), pop2 = factor(prep2, levels = pop_levels), value = sprintf(&#39;%7.5f&#39;, value) ) %&gt;% select(pop1,pop2,value) %&gt;% arrange(pop2,pop1) %&gt;% mutate(pop2 = as.character(pop2) %&gt;% str_replace(pattern = &#39;([a-z]{3})([a-z]{3})&#39;, replacement = &#39;\\\\1|\\\\2&#39;), pop1 = as.character(pop1) %&gt;% str_replace(pattern = &#39;([a-z]{3})([a-z]{3})&#39;, replacement = &#39;\\\\1|\\\\2&#39;)) %&gt;% pivot_wider(values_from = value, names_from = pop2) %&gt;% rename( Population = &#39;pop1&#39;) %&gt;% mutate(srt1 = str_sub(Population,-3, -1), srt2 = str_sub(Population,1, 3)) %&gt;% arrange(srt1,srt2) %&gt;% select(-srt1,-srt2) Then, missing data is formated to be displayed as dashes. # replace &quot;NA&quot; by dashes &quot;-&quot; table_all[is.na(table_all)] &lt;- &#39;-&#39; Finally, the table is subset for each location and exported as a .tex file to be easily integrated within the latex document of the manuscript. # export sub-tables 1 a - c table_all[1:5,c(1,2:6)] %&gt;% export_2_latex(name = &#39;tables/suppl_tab3a.tex&#39;) table_all[6:11,c(1,7:12)] %&gt;% export_2_latex(name = &#39;tables/suppl_tab3b.tex&#39;) table_all[12:14, c(1,13:15)] %&gt;% export_2_latex(name = &#39;tables/suppl_tab3c.tex&#39;) "],
["figure-2.html", "14 Figure 2 14.1 Summary 14.2 Details of plot_F2.R", " 14 Figure 2 14.1 Summary This is the accessory documentation of Figure 2. The Figure can be recreated by running the R script plot_F2.R: cd $BASE_DIR Rscript --vanilla R/fig/plot_F2.R \\ 2_analysis/msmc/output/ 2_analysis/cross_coalescence/output/ \\ 2_analysis/msmc/setup/msmc_grouping.txt 2_analysis/msmc/setup/msmc_cc_grouping.txt \\ 2_analysis/summaries/fst_globals.txt 14.2 Details of plot_F2.R In the following, the individual steps of the R script are documented. It is an executable R script that depends on the accessory R packages GenomicOriginsScripts, as well as on the R packages hypoimg and patchwork. 14.2.1 Config The scripts start with a header that contains copy &amp; paste templates to execute or debug the script: #!/usr/bin/env Rscript # run from terminal: # Rscript --vanilla R/fig/plot_F2.R \\ # 2_analysis/msmc/output/ 2_analysis/cross_coalescence/output/ \\ # 2_analysis/msmc/setup/msmc_grouping.txt 2_analysis/msmc/setup/msmc_cc_grouping.txt \\ # 2_analysis/summaries/fst_globals.txt # =============================================================== # This script produces Figure 2 of the study &quot;Ancestral variation, hybridization and modularity # fuel a marine radiation&quot; by Hench, McMillan and Puebla # --------------------------------------------------------------- # =============================================================== # args &lt;- c(&#39;2_analysis/msmc/output/&#39;, &#39;2_analysis/cross_coalescence/output/&#39;, # &#39;2_analysis/msmc/setup/msmc_grouping.txt&#39;, &#39;2_analysis/msmc/setup/msmc_cc_grouping.txt&#39;, # &#39;2_analysis/summaries/fst_globals.txt&#39;) # script_name &lt;- &quot;R/fig/plot_F2.R&quot; # ---------------------------------------- The next section processes the input from the command line (supposedly…). It stores the arguments in the vector args. The R packages GenomicOriginsScripts and patchwork are loaded and the script name and the current working directory are stored inside variables (script_name, plot_comment). This information will later be written into the meta data of the figure to help us tracing back the scripts that created the figures in the future. Then we drop all the imported information besides the arguments following the script name and print the information to the terminal. args &lt;- commandArgs(trailingOnly = FALSE) # setup ----------------------- library(GenomicOriginsScripts) library(hypoimg) library(patchwork) cat(&#39;\\n&#39;) script_name &lt;- args[5] %&gt;% str_remove(.,&#39;--file=&#39;) plot_comment &lt;- script_name %&gt;% str_c(&#39;mother-script = &#39;,getwd(),&#39;/&#39;,.) args &lt;- process_input(script_name, args) #&gt; ── Script: scripts/plot_F2.R ──────────────────────────────────────────── #&gt; Parameters read: #&gt; ★ 1: 2_analysis/msmc/output/ #&gt; ★ 2: 2_analysis/cross_coalescence/output/ #&gt; ★ 3: 2_analysis/msmc/setup/msmc_grouping.txt #&gt; ★ 4: 2_analysis/msmc/setup/msmc_cc_grouping.txt #&gt; ★ 5: 2_analysis/summaries/fst_globals.txt #&gt; ─────────────────────────────────────────── /current/working/directory ── The directories for the demographic inference and the cross-coalescence data are received and stored in respective variables. Also, the files containing the groupings for demographic inference and cross-coalescence as well as the reference file for the genome wide \\(F_{ST}\\) values are received. # config ----------------------- msmc_path &lt;- as.character(args[1]) cc_path &lt;- as.character(args[2]) msmc_group_file &lt;- as.character(args[3]) cc_group_file &lt;- as.character(args[4]) fst_globals_file &lt;- as.character(args[5]) The msmc sample groupings are imported and the \\(F_{ST}\\) values loaded. # actual script ========================================================= msmc_groups &lt;- read_tsv(msmc_group_file) cc_groups &lt;- read_tsv(cc_group_file) fst_globals &lt;- vroom::vroom(fst_globals_file,delim = &#39;\\t&#39;, col_names = c(&#39;loc&#39;,&#39;run_prep&#39;,&#39;mean_fst&#39;,&#39;weighted_fst&#39;)) %&gt;% separate(run_prep,into = c(&#39;pop1&#39;,&#39;pop2&#39;),sep = &#39;-&#39;) %&gt;% mutate(run = str_c(pop1,loc,&#39;-&#39;,pop2,loc), run = fct_reorder(run,weighted_fst)) Next, the filenames of all msmc results are collected. # locate cross-coalescence results msmc_files &lt;- dir(msmc_path, pattern = &#39;.final.txt.gz&#39;) cc_files &lt;- dir(cc_path, pattern = &#39;.final.txt.gz&#39;) Separately, all the demographic inference and cross-coalescence data are read in an compiled into two data sets. # import effective population size data msmc_data &lt;- msmc_files %&gt;% map_dfr(.f = get_msmc, msmc_path = msmc_path) # import cross-coalescence data cc_data &lt;- cc_files %&gt;% map_dfr(get_cc, cc_groups = cc_groups, cc_path = cc_path) %&gt;% mutate( run = factor(run, levels = levels(fst_globals$run))) The default color scheme is adjusted (to keep H. unicolor visible) and the tick color for the plots is defined. # color adjustments for line plots (replace white by gray) clr_alt &lt;- clr clr_alt[&#39;uni&#39;] &lt;- rgb(.8,.8,.8) clr_ticks &lt;- &#39;lightgray&#39; The first panel containing the demographic history is created. p_msmc &lt;- msmc_data %&gt;% # remove the two first and last time segments filter(!time_index %in% c(0:2,29:31)) %&gt;% ggplot( aes(x=YBP, y=Ne, group = run_nr, colour = spec)) + # add guides for the logarithmic axes annotation_logticks(sides=&quot;tl&quot;, color = clr_ticks) + # add the msmc data as lines geom_line()+ # set the color scheme scale_color_manual(&#39;Species&#39;, values = clr_alt, label = sp_labs) + # format the x axis scale_x_log10(expand = c(0,0), breaks = c(10^3, 10^4, 10^5), position = &#39;top&#39;, labels = c(&quot;1-3 kya&quot;, &quot;10-30 kya&quot;, &quot;100-300 kya&quot;), name = &quot;Years Before Present&quot;) + # format the y axis scale_y_log10(labels = scales::trans_format(&quot;log10&quot;, scales::math_format(10^.x)), breaks = c(10^3,10^4,10^5,10^6)) + # format the color legend guides(colour = guide_legend(title.position = &quot;top&quot;, override.aes = list(alpha = 1, size=1), nrow = 2, byrow=TRUE)) + # set the axis titles labs(x=&quot;Generations Before Present&quot;, y = expression(Effective~Population~Size~(italic(N[e])))) + # set plot range coord_cartesian(xlim = c(250, 5*10^5)) + # tune plot appreance theme_minimal()+ theme(axis.ticks = element_line(colour = clr_ticks), legend.position = c(1,.03), legend.justification = c(1,0), legend.text.align = 0, panel.grid.major.x = element_blank(), panel.grid.minor.y = element_blank(), title = element_text(face = &#39;bold&#39;), axis.title = element_text(face = &#39;plain&#39;), axis.text.x = element_blank(), axis.title.x = element_blank(), legend.title = element_text(face = &#39;plain&#39;)) The second panel containing the cross-coalescence is created. p_cc &lt;- cc_data %&gt;% # remove the two first and last time segments filter( !time_index %in% c(0:2,29:31)) %&gt;% arrange(run_nr) %&gt;% # attach fst data left_join(fst_globals %&gt;% select(run, weighted_fst)) %&gt;% ggplot(aes(x = YBP, y = Cross_coal, group = run_nr, color = weighted_fst)) + # add guides for the logarithmic axis annotation_logticks(sides=&quot;b&quot;, color = clr_ticks) + # add the msmc data as lines geom_line(alpha = 0.2)+ # set the color scheme scale_color_gradientn(name = expression(Global~weighted~italic(F[ST])), colours = hypogen::hypo_clr_LGs[1:24])+ # format the x axis scale_x_log10(expand = c(0,0), labels = scales::trans_format(&quot;log10&quot;, scales::math_format(10^.x))) + # set the axis titles guides(color = guide_colorbar(barheight = unit(7, &#39;pt&#39;), barwidth = unit(150, &#39;pt&#39;), title.position = &#39;top&#39;))+ # set the axis titles labs(x = &quot;Generations Before Present&quot;, y = &#39;Cross-coalescence Rate&#39;) + # set plot range coord_cartesian(xlim = c(250, 5*10^5)) + # tune plot appreance theme_minimal()+ theme(axis.ticks = element_line(colour = clr_ticks), legend.position = c(1,.03), legend.direction = &#39;horizontal&#39;, legend.justification = c(1,0), panel.grid.major.x = element_blank(), panel.grid.minor.y = element_blank(), title = element_text(face = &#39;bold&#39;), axis.title = element_text(face = &#39;plain&#39;), legend.title = element_text(face = &#39;plain&#39;)) Then, the figure is composed from both panels. # combine panels a and b p_done &lt;- p_msmc / p_cc + plot_annotation(tag_levels = c(&#39;a&#39;)) Finally, we can export Figure 2. # export figure 2 hypo_save(plot = p_done, filename = &#39;figures/F2.pdf&#39;, width = 10, height = 7, comment = plot_comment, device = cairo_pdf) "],
["figure-3.html", "15 Figure 3 15.1 Summary 15.2 Details of plot_F3.R", " 15 Figure 3 15.1 Summary This is the accessory documentation of Figure 3. It should be possible to recreate the figure by running the R script plot_F3.R: cd $BASE_DIR Rscript --vanilla R/fig/plot_F3.R \\ 2_analysis/fst/50k/ 2_analysis/summaries/fst_globals.txt 15.2 Details of plot_F3.R In the following, the individual steps of the R script are documented. It is an executable R script that depends on the accessory R packages GenomicOriginsScripts and on the R packages hypoimg, vroom and ggforce. 15.2.1 Config The scripts start with a header that contains copy &amp; paste templates to execute or debug the script: The next section processes the input from the command line. It stores the arguments in the vector args. The R packages are loaded and the script name and the current working directory are stored inside variables (script_name, plot_comment). This information will later be written into the meta data of the figure to help us tracing back the scripts that created the figures in the future. Then we drop all the imported information besides the arguments following the script name and print the information to the terminal. args &lt;- commandArgs(trailingOnly = FALSE) # setup ----------------------- library(GenomicOriginsScripts) library(ggforce) library(hypoimg) library(vroom) cat(&#39;\\n&#39;) script_name &lt;- args[5] %&gt;% str_remove(.,&#39;--file=&#39;) plot_comment &lt;- script_name %&gt;% str_c(&#39;mother-script = &#39;,getwd(),&#39;/&#39;,.) args &lt;- process_input(script_name, args) #&gt; ── Script: scripts/plot_F3.R ──────────────────────────────────────────── #&gt; Parameters read: #&gt; ★ 1: 2_analysis/fst/50k/ #&gt; ★ 2: 2_analysis/summaries/fst_globals.txt #&gt; ─────────────────────────────────────────── /current/working/directory ── The directory containing the sliding window FST data and the and the file with the genome wide average FST for all the species comparisons are received from the command line input. # config ----------------------- data_dir &lt;- as.character(args[1]) globals_file &lt;- as.character(args[2]) # script ----------------------- Then, the data folder is scanned for windowed FST data with an window size of 50 kb. # locate data files files &lt;- dir(path = data_dir, pattern = &#39;.50k.windowed.weir.fst.gz&#39;) Next, the genome wide average FST data is loaded. # load genome wide average fst data globals &lt;- vroom::vroom(globals_file, delim = &#39;\\t&#39;, col_names = c(&#39;loc&#39;,&#39;run&#39;,&#39;mean&#39;,&#39;weighted&#39;)) %&gt;% mutate(run = str_c(loc,&#39;-&#39;,run) %&gt;% reformat_run_name() ) The package GenomicOriginsScripts contains the function get_fst_fixed to import FST data and compute the number, average length and cumulative length of regions exceeding a given FST threshold. Here, we prepare a table of a series of thresholds and all pair wise species comparisons as a configuration table for the following import with get_fst_fixed. # prepare data import settings within a data table (tibble) import_table &lt;- list(file = str_c(data_dir,files), fst_threshold = c(.5,.4,.3,.2,.1,.05,.02,.01)) %&gt;% cross_df() %&gt;% mutate( run = file %&gt;% str_remove(&#39;^.*/&#39;) %&gt;% str_sub(.,1,11) %&gt;% reformat_run_name()) Using the configuration table, the FST data are loaded, and the threshold-specific stats are computed. # load data and compute statistics based on fixed fst treshold data &lt;- purrr::pmap_dfr(import_table,get_fst_fixed) %&gt;% left_join(globals) %&gt;% mutate(run = fct_reorder(run, weighted)) To simplify the figure, a subset of the original thresholds are selected and some columns are renamed for clean figure labels. # pre-format labels data2 &lt;- data %&gt;% select(threshold_value,weighted,n,avg_length,overal_length) %&gt;% mutate(avg_length = avg_length/1000, overal_length = overal_length/(10^6)) %&gt;% rename(`Number~of~Regions` = &#39;n&#39;, `Average~Region~Length~(kb)` = &#39;avg_length&#39;, `Cummulative~Region~Length~(Mb)` = &#39;overal_length&#39;) %&gt;% pivot_longer(names_to = &#39;variable&#39;,values_to = &#39;Value&#39;,3:5) %&gt;% mutate(threshold_value = str_c(&#39;italic(F[ST])~threshold:~&#39;, threshold_value), variable = factor(variable, levels = c(&#39;Number~of~Regions&#39;, &#39;Average~Region~Length~(kb)&#39;, &#39;Cummulative~Region~Length~(Mb)&#39;))) Then we set the font size and create the figure. # set font size fnt_sz &lt;- 10 # compile plot p &lt;- data2 %&gt;% # select thresholds of interest filter(!(threshold_value %in% (c(0.02,.1,0.3, .4) %&gt;% str_c(&quot;italic(F[ST])~threshold:~&quot;,.)))) %&gt;% ggplot(aes(x = weighted, y = Value, fill = weighted))+ # add red line for genome extent in lowest row geom_hline(data = tibble(variable = factor(c(&#39;Cummulative~Region~Length~(Mb)&#39;, &#39;Average~Region~Length~(kb)&#39;, &#39;Number~of~Regions&#39;), levels = c(&#39;Number~of~Regions&#39;, &#39;Average~Region~Length~(kb)&#39;, &#39;Cummulative~Region~Length~(Mb)&#39;)), y = c(559649677/(10^6),NA,NA)), aes(yintercept = y), color=rgb(1,0,0,.25))+ # add data points geom_point(size = 1.75, color = plot_clr, shape = 21)+ # define plot stucture facet_grid(variable~threshold_value, scale=&#39;free&#39;, switch = &#39;y&#39;, labeller = label_parsed)+ # configure scales scale_fill_gradientn(name = expression(weighted~italic(F[ST])), colours = hypogen::hypo_clr_LGs[1:24] %&gt;% clr_lighten(factor = .3))+ scale_x_continuous(name = expression(Whole-genome~differentiation~(weighted~italic(F[ST]))), breaks = c(0,.05,.1), limits = c(-.00025,.10025), labels = c(&quot;0&quot;, &quot;0.05&quot;, &quot;0.1&quot;))+ # configure legend guides(fill = guide_colorbar(barwidth = unit(150, &quot;pt&quot;), label.position = &quot;top&quot;, barheight = unit(5,&quot;pt&quot;)))+ # tweak plot apperance theme(axis.title.y = element_blank(), axis.text.x = element_text(vjust = .5, angle = 0), axis.title.x = element_text(vjust = -2), legend.position = &quot;bottom&quot;, strip.text = element_text(size = fnt_sz), legend.direction = &quot;horizontal&quot;, strip.placement = &#39;outside&#39;, axis.title = element_text(size = fnt_sz), legend.title = element_text(size = fnt_sz), strip.background.y = element_blank()) Finally, we can export Figure 3. # export figure 3 scl &lt;- .675 hypo_save(filename = &#39;figures/F3.pdf&#39;, plot = p, width = 16*scl, height = 12*scl, device = cairo_pdf, comment = plot_comment) "],
["figure-4.html", "16 Figure 4 16.1 Summary 16.2 Details of plot_F4.R", " 16 Figure 4 16.1 Summary This is the accessory documentation of Figure 4. The Figure can be recreated by running the R script plot_F4.R: cd $BASE_DIR Rscript --vanilla R/fig/plot_F4.R 2_analysis/dxy/50k/ \\ 2_analysis/fst/50k/multi_fst.50k.tsv.gz 2_analysis/GxP/50000/ \\ 2_analysis/summaries/fst_outliers_998.tsv \\ https://raw.githubusercontent.com/simonhmartin/twisst/master/plot_twisst.R \\ 2_analysis/twisst/weights/ ressources/plugin/trees/ \\ 2_analysis/fasteprr/step4/fasteprr.all.rho.txt.gz \\ 2_analysis/summaries/fst_globals.txt 16.2 Details of plot_F4.R In the following, the individual steps of the R script are documented. It is an executable R script that depends on the accessory R package GenomicOriginsScripts. 16.2.1 Config The scripts start with a header that contains copy &amp; paste templates to execute or debug the script: #!/usr/bin/env Rscript # run from terminal: # Rscript --vanilla R/fig/plot_F4.R 2_analysis/dxy/50k/ \\ # 2_analysis/fst/50k/multi_fst.50k.tsv.gz 2_analysis/GxP/50000/ \\ # 2_analysis/summaries/fst_outliers_998.tsv \\ # https://raw.githubusercontent.com/simonhmartin/twisst/master/plot_twisst.R \\ # 2_analysis/twisst/weights/ ressources/plugin/trees/ \\ # 2_analysis/fasteprr/step4/fasteprr.all.rho.txt.gz \\ # 2_analysis/summaries/fst_globals.txt # =============================================================== # This script produces Figure 4 of the study &quot;Ancestral variation, hybridization and modularity # fuel a marine radiation&quot; by Hench, McMillan and Puebla # --------------------------------------------------------------- # =============================================================== # args &lt;- c(&#39;2_analysis/dxy/50k/&#39;,&#39;2_analysis/fst/50k/multi_fst.50k.tsv.gz&#39;, # &#39;2_analysis/GxP/50000/&#39;, &#39;2_analysis/summaries/fst_outliers_998.tsv&#39;, # &#39;https://raw.githubusercontent.com/simonhmartin/twisst/master/plot_twisst.R&#39;, # &#39;2_analysis/twisst/weights/&#39;, &#39;ressources/plugin/trees/&#39;, # &#39;2_analysis/fasteprr/step4/fasteprr.all.rho.txt.gz&#39;, &#39;2_analysis/summaries/fst_globals.txt&#39;) # script_name &lt;- &quot;R/fig/plot_F4.R&quot; The next section processes the input from the command line. It stores the arguments in the vector args. The R package GenomicOriginsScripts is loaded and the script name and the current working directory are stored inside variables (script_name, plot_comment). This information will later be written into the meta data of the figure to help us tracing back the scripts that created the figures in the future. Then we drop all the imported information besides the arguments following the script name and print the information to the terminal. args &lt;- commandArgs(trailingOnly = FALSE) # setup ----------------------- library(GenomicOriginsScripts) library(hypoimg) cat(&#39;\\n&#39;) script_name &lt;- args[5] %&gt;% str_remove(.,&#39;--file=&#39;) plot_comment &lt;- script_name %&gt;% str_c(&#39;mother-script = &#39;,getwd(),&#39;/&#39;,.) args &lt;- process_input(script_name, args) #&gt; ── Script: scripts/plot_F4.R ──────────────────────────────────────────── #&gt; Parameters read: #&gt; ★ 1: 2_analysis/dxy/50k/ #&gt; ★ 2: 2_analysis/fst/50k/multi_fst.50k.tsv.gz #&gt; ★ 3: 2_analysis/GxP/50000/ #&gt; ★ 4: 2_analysis/summaries/fst_outliers_998.tsv #&gt; ★ 5: https://raw.githubusercontent.com/simonhmartin/twisst/master/plot_twisst.R #&gt; ★ 6: 2_analysis/twisst/weights/ #&gt; ★ 7: ressources/plugin/trees/ #&gt; ★ 8: 2_analysis/fasteprr/step4/fasteprr.all.rho.txt.gz #&gt; ★ 9: 2_analysis/summaries/fst_globals.txt #&gt; ─────────────────────────────────────────── /current/working/directory ── The directories for the different data types are received and stored in respective variables. Also, we source an external r script from the original twisst github repository that we need to import the twisst data: # config ----------------------- dxy_dir &lt;- as.character(args[1]) fst_file &lt;- as.character(args[2]) gxp_dir &lt;- as.character(args[3]) outlier_table &lt;- as.character(args[4]) twisst_script &lt;- as.character(args[5]) w_path &lt;- as.character(args[6]) d_path &lt;- as.character(args[7]) recombination_file &lt;- as.character(args[8]) global_fst_file &lt;- as.character(args[9]) source(twisst_script) 16.2.2 Data import Supplementary Figure 4 contains quite a lot of different data sets. The main part of this script is just importing and organizing all of this data: In the following we’ll go step by step through the import of: differentiation data (\\(F_{ST}\\)) divergence data (\\(d_{XY}\\), also containing diversity data - \\(\\pi\\)) genotype \\(\\times\\) phenotype association data (\\(p_{Wald}\\)) recombination data (\\(\\rho\\)) topology weighting data We start with the import of the \\(F_{ST}\\) data, specifically the data set containing the genome wide \\(F_{ST}\\) computed for all populations simultaneously (joint \\(F_{ST}\\)). The data file is read, the columns are renamed and the genomic position is added. Then, only the genomic position and the \\(F_{ST}\\) columns are selected and a window column is added for faceting in ggplot(). # start script ------------------- # import fst data fst_data &lt;- vroom::vroom(fst_file, delim = &#39;\\t&#39;) %&gt;% select(CHROM, BIN_START, BIN_END, N_VARIANTS, WEIGHTED_FST) %&gt;% setNames(., nm = c(&#39;CHROM&#39;, &#39;BIN_START&#39;, &#39;BIN_END&#39;, &#39;n_snps&#39;, &#39;fst&#39;) ) %&gt;% add_gpos() %&gt;% select(GPOS, fst) %&gt;% setNames(., nm = c(&#39;GPOS&#39;,&#39;value&#39;)) %&gt;% mutate(window = str_c(&#39;bold(&#39;,project_case(&#39;a&#39;),&#39;):joint~italic(F[ST])&#39;)) Next, we import the \\(d_{XY}\\) data. Here we are importing all 28 pairwise comparisons, so we first collect all the file paths and the iterate the data import over all files. # locate dxy data files dxy_files &lt;- dir(dxy_dir) # import dxy data dxy_data &lt;- str_c(dxy_dir,dxy_files) %&gt;% purrr::map(get_dxy) %&gt;% bind_rows() %&gt;% select(N_SITES:GPOS, run) %&gt;% mutate(pop1 = str_sub(run,1,6), pop2 = str_sub(run,8,13)) From this data, we compute the divergence difference (\\(\\Delta d_{XY}\\)). # compute delta dxy dxy_summary &lt;- dxy_data %&gt;% group_by(GPOS) %&gt;% summarise(delta_dxy = max(dxy)-min(dxy), sd_dxy = sd(dxy), delt_pi = max(c(max(PI_POP1),max(PI_POP2))) - min(c(min(PI_POP1),min(PI_POP2)))) %&gt;% ungroup() %&gt;% setNames(., nm = c(&#39;GPOS&#39;, str_c(&#39;bold(&#39;,project_case(&#39;e&#39;),&#39;):\\u0394~italic(d[xy])&#39;), str_c(&#39;bold(&#39;,project_case(&#39;e&#39;),&#39;):italic(d[xy])~(sd)&#39;), str_c(&#39;bold(&#39;,project_case(&#39;e&#39;),&#39;):\\u0394~italic(\\u03C0)&#39;))) %&gt;% gather(key = &#39;window&#39;, value = &#39;value&#39;,2:4) %&gt;% filter(window == str_c(&#39;bold(&#39;,project_case(&#39;e&#39;),&#39;):\\u0394~italic(d[xy])&#39;)) Then we import the genotype \\(\\times\\) phenotype association data. For this, we list all the traits we want to include and then iterate the import function over all traits. We combine the data sets and transform the table to long format. # set G x P traits to be imported traits &lt;- c(&quot;Bars.lm.50k.5k.txt.gz&quot;, &quot;Peduncle.lm.50k.5k.txt.gz&quot;, &quot;Snout.lm.50k.5k.txt.gz&quot;) # set trait figure panels trait_panels &lt;- c(Bars = str_c(&#39;bold(&#39;,project_case(&#39;h&#39;),&#39;)&#39;), Peduncle = str_c(&#39;bold(&#39;,project_case(&#39;i&#39;),&#39;)&#39;), Snout = str_c(&#39;bold(&#39;,project_case(&#39;j&#39;),&#39;)&#39;)) # import G x P data gxp_data &lt;- str_c(gxp_dir,traits) %&gt;% purrr::map(get_gxp) %&gt;% join_list() %&gt;% gather(key = &#39;window&#39;, value = &#39;value&#39;,2:4) Then, we import the genome wide \\(F_{ST}\\) summary for all 28 pair wise comparisons to be able to pick a divergence data set of an intermediately differentiated species pair (the species pair of rank 15, close to 14.5 - the median of rank 1 to 28). # import genome wide Fst data summary -------- globals &lt;- vroom::vroom(global_fst_file, delim = &#39;\\t&#39;, col_names = c(&#39;loc&#39;,&#39;run&#39;,&#39;mean&#39;,&#39;weighted&#39;)) %&gt;% mutate(run = str_c(str_sub(run,1,3),loc,&#39;-&#39;,str_sub(run,5,7),loc), run = fct_reorder(run,weighted)) # dxy and pi are only shown for one exemplary population (/pair) # select dxy pair run (15 is one of the two central runs of the 28 pairs) # here, the 15th lowest fst value is identified as &quot;selector&quot; selectors_dxy &lt;- globals %&gt;% arrange(weighted) %&gt;% .$weighted %&gt;% .[15] # the dxy population pair corresponding to the selector is identified select_dxy_runs &lt;- globals %&gt;% filter(weighted %in% selectors_dxy) %&gt;% .$run %&gt;% as.character() # then thne dxy data is subset based on the selector dxy_select &lt;- dxy_data %&gt;% filter(run %in% select_dxy_runs) %&gt;% mutate(window = str_c(&#39;bold(&#39;,project_case(&#39;b&#39;),&#39;): italic(d[XY])&#39;)) The \\(d_{XY}\\) data set includes also \\(\\pi\\) of the involved populations. We first extract the diversity data for each population (pop1 &amp; pop2), combine them and compute the statistics needed for ranking the populations based on their diversity. # the pi data is filtered on a similar logic to the dxy data # first a table with the genome wide average pi for each population is compiled # (based on the first populations from the dxy data table # which contains pi for both populations) pi_summary_1 &lt;- dxy_data %&gt;% group_by(pop1,run) %&gt;% summarise(avg_pi = mean(PI_POP1)) %&gt;% ungroup() %&gt;% set_names(., nm = c(&#39;pop&#39;,&#39;run&#39;,&#39;avg_pi&#39;)) # the mean genome wide average pi is compiled for all the second populations # from the dxy data # then, the average of all the comparisons is computed for each population pi_summary &lt;- dxy_data %&gt;% group_by(pop2,run) %&gt;% summarise(avg_pi = mean(PI_POP2)) %&gt;% ungroup() %&gt;% set_names(., nm = c(&#39;pop&#39;,&#39;run&#39;,&#39;avg_pi&#39;)) %&gt;% bind_rows(pi_summary_1) %&gt;% group_by(pop) %&gt;% summarise(n = length(pop), mean_pi = mean(avg_pi), min_pi = min(avg_pi), max_pi = max(avg_pi), sd_pi = sd(avg_pi)) %&gt;% arrange(n) Then, we determine an intermediately diverse candidate of our 14 populations (rank 7, again: \\(7 \\approx median(1:14)\\)) and average over the diversities estimated in all pairwise comparisons this population was involved in. # one of the central populations with respect to average genome # wide pi is identified # for this, the 7th lowest pi value of the 14 populations is # determined as &quot;selector&quot; selectors_pi &lt;- pi_summary %&gt;% .$mean_pi %&gt;% sort() %&gt;% .[7] # the respective population is identified select_pi_pops &lt;- pi_summary %&gt;% filter(mean_pi %in% selectors_pi) %&gt;% .$pop %&gt;% as.character() # then the dxy data is subset by that population and the average pi over # all pair-wise runs is calculated for each window pi_data_select &lt;- dxy_data %&gt;% select(GPOS, PI_POP1, pop1 )%&gt;% set_names(., nm = c(&#39;GPOS&#39;,&#39;pi&#39;,&#39;pop&#39;)) %&gt;% bind_rows(.,dxy_data %&gt;% select(GPOS, PI_POP2, pop2 )%&gt;% set_names(., nm = c(&#39;GPOS&#39;,&#39;pi&#39;,&#39;pop&#39;))) %&gt;% group_by(GPOS,pop) %&gt;% summarise(n = length(pop), mean_pi = mean(pi), min_pi = min(pi), max_pi = max(pi), sd_pi = sd(pi)) %&gt;% filter(pop %in% select_pi_pops) %&gt;% mutate(window = str_c(&#39;bold(&#39;,project_case(&#39;c&#39;),&#39;):~\\u03C0&#39;)) The import of the recombination data is pretty straight forward: Reading one file, adding genomic position and window column for faceting. # import recombination data recombination_data &lt;- vroom::vroom(recombination_file,delim = &#39;\\t&#39;) %&gt;% add_gpos() %&gt;% mutate(window = str_c(&#39;bold(&#39;,project_case(&#39;d&#39;),&#39;):~\\u03C1&#39;)) Then we import the topology weighting data. This is done once per location, the data sets are combined and specific columns are selected: The gnomic position, the topology number (format: three digits with leading zeros, hence “topo3”), relative topology rank ranging from 0 to 1, the faceting column and the actual weight data. We also create a dummy tibble that contains the null expectation of the topology weight for the two locations (1/n, with n = number of possible topologies - n = 15 for Belize and 105 for Honduras). # import topology weighting data twisst_data &lt;- tibble(loc = c(&#39;bel&#39;,&#39;hon&#39;), panel = c(&#39;f&#39;,&#39;g&#39;) %&gt;% project_case() %&gt;% str_c(&#39;bold(&#39;,.,&#39;)&#39;)) %&gt;% purrr::pmap(match_twisst_files) %&gt;% bind_rows() %&gt;% select(GPOS, topo3,topo_rel,window,weight) # the &quot;null-weighting&quot; is computed for both locations twisst_null &lt;- tibble(window = c(str_c(&#39;bold(&#39;,project_case(&#39;f&#39;),&#39;):~italic(w)[bel]&#39;), str_c(&#39;bold(&#39;,project_case(&#39;g&#39;),&#39;):~italic(w)[hon]&#39;)), weight = c(1/15, 1/105)) We create a single data set for \\(d_{XY}\\), \\(F_{ST}\\) and genotype \\(\\times\\) phenotype data. # combine data types -------- data &lt;- bind_rows(dxy_summary, fst_data, gxp_data) Then we load the positions of the the \\(F_{ST}\\) outlier windows, select the focal outliers that will receive individual labels and create a tibble and two parameters for the label placement within the plot. # import fst outliers outliers &lt;- vroom::vroom(outlier_table, delim = &#39;\\t&#39;) # the focal outlier IDs are set outlier_pick &lt;- c(&#39;LG04_1&#39;, &#39;LG12_3&#39;, &#39;LG12_4&#39;) # the table for the outlier labels is created outlier_label &lt;- outliers %&gt;% filter(gid %in% outlier_pick) %&gt;% mutate(label = letters[row_number()] %&gt;% project_inv_case(), x_shift_label = c(-1,-1.2,1)*10^7, gpos_label = gpos + x_shift_label, gpos_label2 = gpos_label - sign(x_shift_label) *.5*10^7, window = str_c(&#39;bold(&#39;,project_case(&#39;a&#39;),&#39;):joint~italic(F[ST])&#39;)) # the y height of the outlier labels and the corresponding tags is set outlier_y &lt;- .45 outlier_yend &lt;- .475 We prepare a set of trait annotations for the genotype \\(\\times\\) phenotype association panels. # the icons for the traits of the GxP are loaded trait_tibble &lt;- tibble(window = c(&quot;bold(h):italic(p)[Bars]&quot;, &quot;bold(i):italic(p)[Peduncle]&quot;, &quot;bold(j):italic(p)[Snout]&quot;), grob = hypo_trait_img$grob_circle[hypo_trait_img$trait %in% c(&#39;Bars&#39;, &#39;Peduncle&#39;, &#39;Snout&#39;)]) 16.2.3 Plotting Finally it is time to put the pieces together with one giant ggplot(): # finally, the figure is being put together p_done &lt;- ggplot()+ # add gray/white LGs background geom_hypo_LG()+ # the red highlights for the outlier regions are added geom_vline(data = outliers, aes(xintercept = gpos), color = outlr_clr)+ # the tags of the outlier labels are added geom_segment(data = outlier_label, aes(x = gpos, xend = gpos_label2, y = outlier_y, yend = outlier_yend), color = alpha(outlr_clr,1),size = .2)+ # the outlier labels are added geom_text(data = outlier_label, aes(x = gpos_label, y = outlier_yend, label = label), color = alpha(outlr_clr,1), fontface = &#39;bold&#39;)+ # the fst, delta dxy and gxp data is plotted geom_point(data = data, aes(x = GPOS, y = value),size = plot_size, color = plot_clr) + # the dxy data is plotted geom_point(data = dxy_select,aes(x = GPOS, y = dxy),size = plot_size, color = plot_clr)+ # the pi data is plotted geom_point(data = pi_data_select, aes(x = GPOS, y = mean_pi),size = plot_size, color = plot_clr) + # the roh data is plotted geom_point(data = recombination_data, aes(x = GPOS, y = RHO),size = plot_size, color = plot_clr) + # the smoothed rho is plotted geom_smooth(data = recombination_data, aes(x = GPOS, y = RHO, group = CHROM), color = &#39;red&#39;, se = FALSE, size = .7) + # the topology weighting data is plotted geom_line(data = twisst_data, aes(x = GPOS, y = weight, color = topo_rel), size = .4) + # the null weighting is added geom_hline(data = twisst_null, aes(yintercept = weight), color = rgb(1, 1, 1, .5), size = .4) + # the trait icons are added geom_hypo_grob(data = trait_tibble, aes(grob = grob, angle = 0, height = .65), inherit.aes = FALSE, x = .95, y = 0.65)+ # setting the scales scale_fill_hypo_LG_bg() + scale_x_hypo_LG()+ scale_color_gradient( low = &quot;#f0a830ff&quot;, high = &quot;#084082ff&quot;, guide = FALSE)+ # organizing the plot across panels facet_grid(window~.,scales = &#39;free&#39;,switch = &#39;y&#39;, labeller = label_parsed)+ # tweak plot appreance theme_hypo()+ theme(legend.position = &#39;bottom&#39;, axis.title = element_blank(),strip.text = element_text(size = 11), strip.background = element_blank(), strip.placement = &#39;outside&#39;) The final figure is then exported using hypo_save(). # export figure 4 scl &lt;- .8 hypo_save(p_done, filename = &#39;figures/F4.png&#39;, width = 297*scl, height = 275*scl, units = &#39;mm&#39;, type = &quot;cairo&quot;, comment = plot_comment) "],
["figure-5.html", "17 Figure 5 17.1 Summary 17.2 Details of plot_F5.R", " 17 Figure 5 17.1 Summary This is the accessory documentation of Figure 5. The Figure can be recreated by running the R script plot_F5.R: cd $BASE_DIR Rscript --vanilla R/fig/plot_F5.R \\ 2_analysis/twisst/weights/ ressources/plugin/trees/ \\ https://raw.githubusercontent.com/simonhmartin/twisst/master/plot_twisst.R \\ 2_analysis/summaries/fst_outliers_998.tsv 2_analysis/dxy/50k/ \\ 2_analysis/fst/50k/ 2_analysis/summaries/fst_globals.txt \\ 2_analysis/GxP/50000/ 200 5 2_analysis/fst/poptree/summary/ 17.2 Details of plot_F5.R In the following, the individual steps of the R script are documented. It is an executable R script that depends on the accessory R package GenomicOriginsScripts. 17.2.1 Config The scripts start with a header that contains copy &amp; paste templates to execute or debug the script: #!/usr/bin/env Rscript # run from terminal: # Rscript --vanilla R/fig/plot_F5.R \\ # 2_analysis/twisst/weights/ ressources/plugin/trees/ \\ # https://raw.githubusercontent.com/simonhmartin/twisst/master/plot_twisst.R \\ # 2_analysis/summaries/fst_outliers_998.tsv 2_analysis/dxy/50k/ \\ # 2_analysis/fst/50k/ 2_analysis/summaries/fst_globals.txt \\ # 2_analysis/GxP/50000/ 200 5 2_analysis/fst/poptree/summary/ # =============================================================== # This script produces Figure 5 of the study &quot;Ancestral variation, hybridization and modularity # fuel a marine radiation&quot; by Hench, McMillan and Puebla # --------------------------------------------------------------- # =============================================================== # args &lt;- c(&#39;2_analysis/twisst/weights/&#39;, &#39;ressources/plugin/trees/&#39;, # &#39;https://raw.githubusercontent.com/simonhmartin/twisst/master/plot_twisst.R&#39;, # &#39;2_analysis/summaries/fst_outliers_998.tsv&#39;, # &#39;2_analysis/dxy/50k/&#39;, &#39;2_analysis/fst/50k/&#39;, # &#39;2_analysis/summaries/fst_globals.txt&#39;, # &#39;2_analysis/GxP/50000/&#39;, 200, 5, # &quot;2_analysis/fst/poptree/summary/&quot;) # script_name &lt;- &quot;R/fig/plot_F5.R&quot; The next section processes the input from the command line. It stores the arguments in the vector args. The R package GenomicOriginsScripts is loaded and the script name and the current working directory are stored inside variables (script_name, plot_comment). This information will later be written into the meta data of the figure to help us tracing back the scripts that created the figures in the future. Then we drop all the imported information besides the arguments following the script name and print the information to the terminal. args &lt;- commandArgs(trailingOnly = FALSE) # setup ----------------------- library(GenomicOriginsScripts) library(hypoimg) library(furrr) library(ggraph) library(tidygraph) library(ggtext) cat(&#39;\\n&#39;) script_name &lt;- args[5] %&gt;% str_remove(.,&#39;--file=&#39;) plot_comment &lt;- script_name %&gt;% str_c(&#39;mother-script = &#39;,getwd(),&#39;/&#39;,.) args &lt;- process_input(script_name, args) #&gt; ── Script: scripts/plot_F5.R ──────────────────────────────────────────── #&gt; Parameters read: #&gt; ★ 1: 2_analysis/twisst/weights/ #&gt; ★ 2: ressources/plugin/trees/ #&gt; ★ 3: https://raw.githubusercontent.com/simonhmartin/twisst/master/plot_twisst.R #&gt; ★ 4: 2_analysis/summaries/fst_outliers_998.tsv #&gt; ★ 5: 2_analysis/dxy/50k/ #&gt; ★ 6: 2_analysis/fst/50k/ #&gt; ★ 7: 2_analysis/summaries/fst_globals.txt #&gt; ★ 8: 2_analysis/GxP/50000/ #&gt; ★ 9: 200 #&gt; ★ 10: 5 #&gt; ★ 11: 2_analysis/fst/poptree/summary/ #&gt; ─────────────────────────────────────────── /current/working/directory ── The directories for the different data types are received and stored in respective variables. Also, we source an external r script from the original twisst github repository that we need to import the twisst data: # config ----------------------- w_path &lt;- as.character(args[1]) d_path &lt;- as.character(args[2]) twisst_functions &lt;- as.character(args[3]) out_table &lt;- as.character(args[4]) dxy_dir &lt;- as.character(args[5]) fst_dir &lt;- as.character(args[6]) fst_globals &lt;- as.character(args[7]) gxp_dir &lt;- as.character(args[8]) twisst_size &lt;- as.numeric(args[9]) resolution &lt;- as.numeric(args[10]) fst_summary_path &lt;- as.character(args[11]) source(twisst_functions, local = TRUE) The we define a buffer width. This is the extent left and right of the \\(F_{ST}\\) outlier windows that is included in the plots. We also load the R packages ape and igraph that will help us working with phylogenetic objects (the twisst topologies). plan(multiprocess) window_buffer &lt;- 2.5*10^5 #------------------- library(ape) library(igraph) 17.2.2 Data import Then, we start with the data import. For the figure we are going to need: \\(d_{XY}\\) data genotype \\(\\times\\) phenotype data \\(F_{ST}\\) data topology weighing data the positions of the genome annotations the positions of the \\(F_{ST}\\) outlier windows We start by importing \\(d_{XY}\\) by first listing all \\(d_{XY}\\) data files and then iterating the \\(d_{XY}\\) import function over the files. # actual script ========================================================= # locate dxy data files dxy_files &lt;- dir(dxy_dir, pattern = str_c(&#39;dxy.*[a-z]{3}.*.&#39;, resolution ,&#39;0kb-&#39;, resolution ,&#39;kb.tsv.gz&#39;)) # import dxy data dxy_data &lt;- tibble(file = str_c(dxy_dir, dxy_files)) %&gt;% purrr::pmap_dfr(get_dxy, kb = str_c(resolution ,&#39;0kb&#39;)) Next we iterate the genotype \\(\\times\\) phenotype import function over the trait names Bars, Snout and Peduncle. # set traits of interest for GxP gxp_traits &lt;- c(&#39;Bars&#39;, &#39;Snout&#39;, &#39;Peduncle&#39;) # import GxP data gxp_data &lt;- str_c(gxp_dir,gxp_traits,&#39;.lm.&#39;, resolution ,&#39;0k.&#39;, resolution ,&#39;k.txt.gz&#39;) %&gt;% future_map_dfr(get_gxp_long, kb = 50) Then, we define two sets of colors - one for the topology highlighting schemes and one for the traits of the genotype \\(\\times\\) phenotype association. # set topology weighting color scheme twisst_clr &lt;- c(Blue = &quot;#0140E5&quot;, Bars = &quot;#E32210&quot;, Butter = &quot;#E4E42E&quot;) # set GxP color scheme gxp_clr &lt;- c(Bars = &quot;#79009f&quot;, Snout = &quot;#E48A00&quot;, Peduncle = &quot;#5B9E2D&quot;) %&gt;% darken(factor = .95) %&gt;% set_names(., nm = gxp_traits) Next, we compute the average genome wide \\(d_{XY}\\) and load the average genome wide \\(F_{ST}\\) values for all 28 pair wise species comparisons. # copute genome wide average dxy dxy_globals &lt;- dxy_data %&gt;% filter(BIN_START %% ( resolution * 10000 ) == 1 ) %&gt;% group_by( run ) %&gt;% summarise(mean_global_dxy = sum(dxy*N_SITES)/sum(N_SITES)) %&gt;% mutate(run = fct_reorder(run,mean_global_dxy)) # import genome wide average fst # and order population pairs by genomewide average fst fst_globals &lt;- vroom::vroom(fst_globals,delim = &#39;\\t&#39;, col_names = c(&#39;loc&#39;,&#39;run_prep&#39;,&#39;mean_fst&#39;,&#39;weighted_fst&#39;)) %&gt;% separate(run_prep,into = c(&#39;pop1&#39;,&#39;pop2&#39;),sep = &#39;-&#39;) %&gt;% mutate(run = str_c(pop1,loc,&#39;-&#39;,pop2,loc), run = fct_reorder(run,weighted_fst)) After this, we import \\(F_{ST}\\) by first listing all \\(F_{ST}\\) data files and then iterating the \\(F_{ST}\\) import function over the files. # locate fst data files fst_files &lt;- dir(fst_dir ,pattern = str_c(&#39;.&#39;, resolution ,&#39;0k.windowed.weir.fst.gz&#39;)) # import fst data fst_data &lt;- str_c(fst_dir, fst_files) %&gt;% future_map_dfr(get_fst, kb = str_c(resolution ,&#39;0k&#39;)) %&gt;% left_join(dxy_globals) %&gt;% left_join(fst_globals) %&gt;% mutate(run = refactor(., fst_globals), BIN_MID = (BIN_START+BIN_END)/2) We then add the genome wide averages of \\(F_{ST}\\) and \\(d_{XY}\\) as new columns to the \\(d_{XY}\\) data. This will be used later for coloring the \\(d_{XY}\\) panel. # order dxy population pairs by genomewide average fst dxy_data &lt;- dxy_data %&gt;% left_join(dxy_globals) %&gt;% left_join(fst_globals) %&gt;% mutate(run = refactor(dxy_data, fst_globals), window = &#39;bold(italic(d[xy]))&#39;) Then, we summarize the \\(d_{XY}\\) data to compute \\(\\Delta d_{XY}\\). # compute delta dxy data_dxy_summary &lt;- dxy_data %&gt;% group_by(GPOS) %&gt;% summarise(scaffold = CHROM[1], start = BIN_START[1], end = BIN_END[1], mid = BIN_MID[1], min_dxy = min(dxy), max_dxy = max(dxy), mean_dxy = mean(dxy), median_dxy = median(dxy), sd_dxy = sd(dxy), delta_dxy = max(dxy)-min(dxy)) To only load the relevant twisst data, we first load the positions of the \\(F_{ST}\\) outlier regions. We also define a set of outliers of interest. # twisst part ------------------ # load fst outlier regions outlier_table &lt;- vroom::vroom(out_table, delim = &#39;\\t&#39;) %&gt;% setNames(., nm = c(&quot;outlier_id&quot;,&quot;lg&quot;, &quot;start&quot;, &quot;end&quot;, &quot;gstart&quot;,&quot;gend&quot;,&quot;gpos&quot;)) # set outlier regions of interest outlier_pick = c(&#39;LG04_1&#39;, &#39;LG12_3&#39;, &#39;LG12_4&#39;) Then we define a set of genes of interest. These are the ones, that later will be labeled in the annotation panel. # select genes to label cool_genes &lt;- c(&#39;arl3&#39;,&#39;kif16b&#39;,&#39;cdx1&#39;,&#39;hmcn2&#39;, &#39;sox10&#39;,&#39;smarca4&#39;, &#39;rorb&#39;, &#39;alox12b&#39;,&#39;egr1&#39;, &#39;ube4b&#39;,&#39;casz1&#39;, &#39;hoxc8a&#39;,&#39;hoxc9&#39;,&#39;hoxc10a&#39;, &#39;hoxc13a&#39;,&#39;rarga&#39;,&#39;rarg&#39;, &#39;snai1&#39;,&#39;fam83d&#39;,&#39;mafb&#39;,&#39;sws2abeta&#39;,&#39;sws2aalpha&#39;,&#39;sws2b&#39;,&#39;lws&#39;,&#39;grm8&#39;) Next, we load the twisst data for both locations and list all species from Belize (This will be needed to calculate their pair wise distances for the topology highlighting). # load twisst data data_tables &lt;- list(bel = prep_data(loc = &#39;bel&#39;), hon = prep_data(loc = &#39;hon&#39;)) # set species sampled in belize pops_bel &lt;- c(&#39;ind&#39;, &#39;may&#39;, &#39;nig&#39;, &#39;pue&#39;, &#39;uni&#39;) 17.2.3 Plotting As a last step before the actual plotting, we are defining a list of outliers to be included within the final plots. # set outlier region label region_label_tibbles &lt;- tibble(outlier_id = outlier_pick, label = c(&#39;A&#39;,&#39;B&#39;,&#39;C&#39;)) We adjust the trait images to show the depicted trait in the same color it is also plotted within figure. # load and recolor trait icons trait_grob &lt;- tibble(svg = hypoimg::hypo_trait_img$grob_circle[hypoimg::hypo_trait_img$trait %in% gxp_traits], layer = c(4,3,7), color = gxp_clr[gxp_traits %&gt;% sort()])%&gt;% pmap(.f = hypo_recolor_svg) %&gt;% set_names(nm = gxp_traits %&gt;% sort()) # recolor second bars-layer trait_grob[[&quot;Bars&quot;]] &lt;- trait_grob[[&quot;Bars&quot;]] %&gt;% hypo_recolor_svg(layer = 7,color = gxp_clr[[&quot;Bars&quot;]]) Then we prepare the population trees by computing neighbour joining trees based on pair wise \\(F_{ST}\\). (This happens within GenomicOriginsScripts::plot_fst_poptree().) # prepare population tree trait_panels ---------------------- # load outler region wide average fst data tree_list &lt;- outlier_pick %&gt;% purrr::map_dfr(get_fst_summary_data) # create neighbour-joining trees and plot poptree_plot_list &lt;- tree_list %&gt;% purrr::pmap(plot_fst_poptree) Then, we iterate the main plotting function over all selected \\(F_{ST}\\) outlier windows and combine the resulting plots into a multi panel plot. # compose base figure p_single &lt;- outlier_table %&gt;% filter(outlier_id %in% outlier_pick) %&gt;% left_join(region_label_tibbles) %&gt;% mutate(outlier_nr = row_number(), text = ifelse(outlier_nr == 1,TRUE,FALSE), trait = c(&#39;Snout&#39;, &#39;Bars&#39;, &#39;Peduncle&#39;)) %&gt;% pmap(plot_curtain, cool_genes = cool_genes) %&gt;% c(., poptree_plot_list) %&gt;% cowplot::plot_grid(plotlist = ., nrow = 2, rel_heights = c(1, .17), labels = letters[1:length(outlier_pick)] %&gt;% project_case()) At this point all that we miss is the figure legend. So, for the \\(F_{ST}\\), \\(d_{XY}\\) and genotype \\(\\times\\) phenotype color schemes we create two dummy plots from where we can export the legends. We combine those two classical color legends into what will become the left column of the legend. # compile legend # dummy plot for fst legend p_dummy_fst &lt;- outlier_table %&gt;% filter(row_number() == 1) %&gt;% purrr::pmap(plot_panel_fst) %&gt;% .[[1]] # dummy plot for gxp legend p_dummy_gxp &lt;- outlier_table %&gt;% filter(row_number() == 1) %&gt;% purrr::pmap(plot_panel_gxp, trait = &#39;Bars&#39;) %&gt;% .[[1]] # fst legend p_leg_fst &lt;- (p_dummy_fst+theme(legend.position = &#39;bottom&#39;)) %&gt;% get_legend() # gxp legend p_leg_gxp &lt;- (p_dummy_gxp+theme(legend.position = &#39;bottom&#39;)) %&gt;% get_legend() # create poptree legend p_leg_poptree &lt;- (poptree_plot_list[[1]] + theme(legend.position = &quot;bottom&quot;)) %&gt;% get_legend() # create sub-legend 1 p_leg1 &lt;- cowplot::plot_grid(p_leg_fst, p_leg_gxp, ncol = 1) Then, we construct the topology highlighting color legend. We first define the three highlighting scenarios, the involved species and their base color and then iterate the legend plotting functions over those configurations. The resulting legend elements are then combined to create the right side of the figure legend and the two main legend elements are combined. # create sub-legend 2 (phylogeny schematics) p_leg2 &lt;- tibble(spec1 = c(&#39;indigo&#39;, &#39;indigo&#39;,&#39;unicolor&#39;), spec2 = c(&#39;maya&#39;, &#39;puella&#39;,NA), color = twisst_clr %&gt;% unname() %&gt;% darken(.,factor = .8), mode = c(rep(&#39;pair&#39;,2),&#39;isolation&#39;)) %&gt;% future_pmap(plot_leg) %&gt;% cowplot::plot_grid(plotlist = ., nrow = 1) # create sub-legend 3 p_leg_3 &lt;- cowplot::plot_grid(p_leg1, p_leg2, nrow = 1, rel_widths = c(.6, 1)) Lastly, the legend elements from the population trees are added to complete the final legend. # complete legend p_leg &lt;- cowplot::plot_grid( p_leg_poptree, p_leg_3, ncol = 1, rel_heights = c(.2, 1)) After adding the legend to the main part, Figure 4 is complete. # finalize figure p_done &lt;- cowplot::plot_grid(p_single, p_leg, ncol = 1, rel_heights = c(1, .17)) Finally, we can export Figure 5. # export figure scl &lt;- .9 hypo_save(plot = p_done, filename = &#39;figures/F5.pdf&#39;, width = 14*scl, height = 13*scl, comment = script_name, device = cairo_pdf) "],
["supplementary-figure-1.html", "18 Supplementary Figure 1 18.1 Summary 18.2 Details of plot_SF.R", " 18 Supplementary Figure 1 18.1 Summary This is the accessory documentation of Supplementary Figure . The Figure can be recreated by running the R script plot_SF1.R: cd $BASE_DIR Rscript --vanilla R/fig/plot_SF1.R 2_analysis/newhyb/nh_input/NH.Results/ 18.2 Details of plot_SF.R In the following, the individual steps of the R script are documented. It is an executable R script that depends on the accessory R package GenomicOriginsScripts. 18.2.1 Config The scripts start with a header that contains copy &amp; paste templates to execute or debug the script: #!/usr/bin/env Rscript # run from terminal: # Rscript --vanilla R/fig/plot_SF1.R 2_analysis/newhyb/nh_input/NH.Results/ # =============================================================== # This script produces Suppl. Figure 1 of the study &quot;Ancestral variation, # hybridization and modularity fuel a marine radiation&quot; # by Hench, McMillan and Puebla # --------------------------------------------------------------- # =============================================================== # args &lt;- c(&quot;2_analysis/newhyb/nh_input/NH.Results/&quot;) # script_name &lt;- &quot;R/fig/plot_SF1.R&quot; The next section processes the input from the command line. It stores the arguments in the vector args. The R package GenomicOriginsScripts is loaded and the script name and the current working directory are stored inside variables (script_name, plot_comment). This information will later be written into the meta data of the figure to help us tracing back the scripts that created the figures in the future. Then we drop all the imported information besides the arguments following the script name and print the information to the terminal. args &lt;- commandArgs(trailingOnly=FALSE) # setup ----------------------- library(GenomicOriginsScripts) library(prismatic) library(paletteer) library(patchwork) library(ggtext) library(hypoimg) cat(&#39;\\n&#39;) script_name &lt;- args[5] %&gt;% str_remove(.,&#39;--file=&#39;) plot_comment &lt;- script_name %&gt;% str_c(&#39;mother-script = &#39;,getwd(),&#39;/&#39;,.) cli::rule( left = str_c(crayon::bold(&#39;Script: &#39;),crayon::red(script_name))) args = args[7:length(args)] cat(&#39; &#39;) cat(str_c(crayon::green(cli::symbol$star),&#39; &#39;, 1:length(args),&#39;: &#39;,crayon::green(args),&#39;\\n&#39;)) cli::rule(right = getwd()) #&gt; ── Script: scripts/plot_SF1.R ──────────────────────────────────────────── #&gt; Parameters read: #&gt; ★ 1: 2_analysis/newhyb/nh_input/NH.Results/ #&gt; ─────────────────────────────────────────── /current/working/directory ── The directories containing the hybridization data is received and stored in a variable. # config ----------------------- base_dir &lt;- as.character(args[1]) All the hybridization-subfolders are located (there is one per pair wise newhybrids run). # locate hybridization data files folders &lt;- dir(base_dir) Then we run the high-level function GenomicOriginsScripts::plot_loc() which reads in all the hybridization data of a given sampling location and creates the respective figure panel. # load data and create plots by location p_loc &lt;- c(&quot;bel&quot;, &quot;hon&quot;, &quot;pan&quot;) %&gt;% map(plot_loc) As an example we can have a look at the result for plot_loc(\"pan\"): All three panels are collected and the final Figure is composed using the pacckage patchwork: # compose figure from the individual panels p &lt;- (p_loc[[1]] + guides(fill = guide_legend(title = &quot;Hybrid Class&quot;)) + theme_hyb(legend.position = c(1,1)) ) + (p_loc[[2]] + theme_hyb() ) + (p_loc[[3]] + theme_hyb() ) + plot_layout(ncol = 1, heights = c(10,15,3) %&gt;% label_spacer())+ plot_annotation(tag_levels = &#39;a&#39;) Finally, we can export Supplementary Figure 1. # export the final figure hypo_save(filename = &quot;figures/SF1.pdf&quot;, plot = p, height = 16, width = 10, device = cairo_pdf, comment = plot_comment) "],
["supplementary-figure-2.html", "19 Supplementary Figure 2 19.1 Summary 19.2 Details of plot_SF2.R", " 19 Supplementary Figure 2 19.1 Summary This is the accessory documentation of Supplementary Figure 2. The Figure can be recreated by running the R script plot_SF2.R: cd $BASE_DIR Rscript --vanilla R/fig/plot_SF2.R 2_analysis/fst/50k/ \\ 2_analysis/summaries/fst_outliers_998.tsv \\ 2_analysis/summaries/fst_globals.txt 19.2 Details of plot_SF2.R In the following, the individual steps of the R script are documented. It is an executable R script that depends on the accessory R package GenomicOriginsScripts. 19.2.1 Config The scripts start with a header that contains copy &amp; paste templates to execute or debug the script: #!/usr/bin/env Rscript # run from terminal: # Rscript --vanilla R/fig/plot_SF2.R 2_analysis/fst/50k/ \\ # 2_analysis/summaries/fst_outliers_998.tsv \\ # 2_analysis/summaries/fst_globals.txt # =============================================================== # This script produces Suppl. Figure 2 of the study &quot;Ancestral variation, # hybridization and modularity fuel a marine radiation&quot; # by Hench, McMillan and Puebla # --------------------------------------------------------------- # =============================================================== # args &lt;- c(&#39;2_analysis/fst/50k/&#39;, # &#39;2_analysis/summaries/fst_outliers_998.tsv&#39;, # &#39;2_analysis/summaries/fst_globals.txt&#39;) # script_name &lt;- &quot;R/fig/plot_SF2.R&quot; The next section processes the input from the command line. It stores the arguments in the vector args. The R package GenomicOriginsScripts is loaded and the script name and the current working directory are stored inside variables (script_name, plot_comment). This information will later be written into the meta data of the figure to help us tracing back the scripts that created the figures in the future. Then we drop all the imported information besides the arguments following the script name and print the information to the terminal. args &lt;- commandArgs(trailingOnly=FALSE) # setup ----------------------- library(GenomicOriginsScripts) library(hypoimg) library(vroom) library(ggtext) cat(&#39;\\n&#39;) script_name &lt;- args[5] %&gt;% str_remove(.,&#39;--file=&#39;) plot_comment &lt;- script_name %&gt;% str_c(&#39;mother-script = &#39;,getwd(),&#39;/&#39;,.) args &lt;- process_input(script_name, args) #&gt; ── Script: scripts/plot_SF2.R ──────────────────────────────────────────── #&gt; Parameters read: #&gt; ★ 1: 2_analysis/fst/50k/ #&gt; ★ 2: 2_analysis/summaries/fst_outliers_998.tsv #&gt; ★ 3: 2_analysis/summaries/fst_globals.txt #&gt; ─────────────────────────────────────────── /current/working/directory ── # config ----------------------- data_path &lt;- as.character(args[1]) outlier_file &lt;- as.character(args[2]) globals_file &lt;- as.character(args[3]) # load data ------------------- # locate fst data files files &lt;- dir(data_path,pattern = &#39;.50k.windowed.weir.fst.gz&#39;) # extract run names from data file names run_files &lt;- files %&gt;% str_sub(.,1,11) %&gt;% str_replace(.,pattern = &#39;([a-z]{3})-([a-z]{3})-([a-z]{3})&#39;, &#39;\\\\2\\\\1-\\\\3\\\\1&#39;) # load genome wide average fst values for each run globals &lt;- vroom::vroom(globals_file, delim = &#39;\\t&#39;, col_names = c(&#39;loc&#39;,&#39;run&#39;,&#39;mean&#39;,&#39;weighted&#39;)) %&gt;% separate(run, into = c(&#39;pop1&#39;,&#39;pop2&#39;)) %&gt;% mutate(run = str_c(pop1,loc,&#39;-&#39;,pop2,loc), run = fct_reorder(run,weighted)) # load all windowed fst data and collapse in to a single data frame data &lt;- purrr::pmap(tibble(file = str_c(data_path,files), run = run_files), hypo_import_windows) %&gt;% bind_rows() %&gt;% set_names(., nm = c(&#39;CHROM&#39;, &#39;BIN_START&#39;, &#39;BIN_END&#39;, &#39;N_VARIANTS&#39;, &#39;WEIGHTED_FST&#39;, &#39;MEAN_FST&#39;, &#39;GSTART&#39;, &#39;POS&#39;, &#39;GPOS&#39;, &#39;run&#39;)) %&gt;% mutate(pop1 = str_sub(run,1,3), pop2 = str_sub(run,8,10), loc = str_sub(run,4,6), run_label = str_c(&quot;*H. &quot;, sp_names[pop1],&quot;* - *H. &quot;, sp_names[pop2],&quot;*&lt;br&gt;(&quot;,loc_names[loc],&quot;)&quot; )) # create table for the indication of genome wide average fst in the plot background # (rescale covered fst range to the extent of the genome) global_bar &lt;- globals %&gt;% select(weighted,run) %&gt;% mutate(run = as.character(run)) %&gt;% setNames(.,nm = c(&#39;fst&#39;,&#39;run&#39;)) %&gt;% pmap(.,fst_bar_row_run) %&gt;% bind_rows() %&gt;% mutate(pop1 = str_sub(run,1,3), pop2 = str_sub(run,8,10), loc = str_sub(run,4,6), run_label = str_c(&quot;*H. &quot;, sp_names[pop1],&quot;* - *H. &quot;, sp_names[pop2],&quot;*&lt;br&gt;(&quot;,loc_names[loc],&quot;)&quot; ), run_label = fct_reorder(run_label,xmax_org)) # pre-calculate secondary x-axis breaks sc_ax &lt;- scales::cbreaks(c(0,max(globals$weighted)), scales::pretty_breaks(4)) # compose final figure p &lt;- ggplot()+ # general plot structure separated by run facet_grid( run_label ~ ., as.table = TRUE) + # add genome wide average fst in the background geom_rect(data = global_bar %&gt;% mutate(xmax = xmax * hypo_karyotype$GEND[24]), aes(xmin = 0, xmax = xmax, ymin = -Inf, ymax = Inf), color = rgb(1,1,1,0), fill = clr_below) + # add LG borders geom_vline(data = hypogen::hypo_karyotype, aes(xintercept = GEND), color = hypo_clr_lg) + # add fst data points geom_point(data = data %&gt;% mutate(run_label = factor(run_label, levels = levels(global_bar$run_label))), aes(x = GPOS, y = WEIGHTED_FST), size=.2,color = plot_clr) + # axis layout scale_x_hypo_LG(sec.axis = sec_axis(~ ./hypo_karyotype$GEND[24], breaks = (sc_ax$breaks/max(globals$weighted)), labels = sprintf(&quot;%.2f&quot;, sc_ax$breaks), name = expression(Genomic~position/~Genome~wide~weighted~italic(F[ST])))) + scale_y_continuous(name = expression(italic(&#39;F&#39;[ST])), limits = c(-.1,1), breaks = c(0,.5,1)) + # general plot layout theme_hypo() + theme(strip.text.y = element_markdown(angle = 0), strip.background = element_blank(), legend.position = &#39;none&#39;, axis.title.x = element_text(), axis.text.x.bottom = element_text(colour = &#39;darkgray&#39;)) Finally, we can export Supplementary Figure 2. # export final figure hypo_save(filename = &#39;figures/SF2.png&#39;, plot = p, width = 8, height = 12, type = &quot;cairo&quot;, comment = plot_comment) "],
["supplementary-figure-3.html", "20 Supplementary Figure 3 20.1 Summary 20.2 Details of plot_SF.R", " 20 Supplementary Figure 3 20.1 Summary This is the accessory documentation of Supplementary Figure . The Figure can be recreated by running the R script plot_SF.R: cd $BASE_DIR Rscript --vanilla R/fig/plot_SF 20.2 Details of plot_SF.R In the following, the individual steps of the R script are documented. It is an executable R script that depends on the accessory R package GenomicOriginsScripts. 20.2.1 Config The scripts start with a header that contains copy &amp; paste templates to execute or debug the script: The next section processes the input from the command line. It stores the arguments in the vector args. The R package GenomicOriginsScripts is loaded and the script name and the current working directory are stored inside variables (script_name, plot_comment). This information will later be written into the meta data of the figure to help us tracing back the scripts that created the figures in the future. Then we drop all the imported information besides the arguments following the script name and print the information to the terminal. #&gt; ── Script: scripts/plot_SF.R ──────────────────────────────────────────── #&gt; Parameters read: #&gt; #&gt; ─────────────────────────────────────────── /current/working/directory ── "],
["supplementary-figure-4.html", "21 Supplementary Figure 4 21.1 Summary 21.2 Details of plot_SF.R", " 21 Supplementary Figure 4 21.1 Summary This is the accessory documentation of Supplementary Figure . The Figure can be recreated by running the R script plot_SF.R: cd $BASE_DIR Rscript --vanilla R/fig/plot_SF 21.2 Details of plot_SF.R In the following, the individual steps of the R script are documented. It is an executable R script that depends on the accessory R package GenomicOriginsScripts. 21.2.1 Config The scripts start with a header that contains copy &amp; paste templates to execute or debug the script: The next section processes the input from the command line. It stores the arguments in the vector args. The R package GenomicOriginsScripts is loaded and the script name and the current working directory are stored inside variables (script_name, plot_comment). This information will later be written into the meta data of the figure to help us tracing back the scripts that created the figures in the future. Then we drop all the imported information besides the arguments following the script name and print the information to the terminal. #&gt; ── Script: scripts/plot_SF.R ──────────────────────────────────────────── #&gt; Parameters read: #&gt; #&gt; ─────────────────────────────────────────── /current/working/directory ── "],
["supplementary-figure-5.html", "22 Supplementary Figure 5 22.1 Summary 22.2 Details of plot_SF.R", " 22 Supplementary Figure 5 22.1 Summary This is the accessory documentation of Supplementary Figure . The Figure can be recreated by running the R script plot_SF.R: cd $BASE_DIR Rscript --vanilla R/fig/plot_SF 22.2 Details of plot_SF.R In the following, the individual steps of the R script are documented. It is an executable R script that depends on the accessory R package GenomicOriginsScripts. 22.2.1 Config The scripts start with a header that contains copy &amp; paste templates to execute or debug the script: The next section processes the input from the command line. It stores the arguments in the vector args. The R package GenomicOriginsScripts is loaded and the script name and the current working directory are stored inside variables (script_name, plot_comment). This information will later be written into the meta data of the figure to help us tracing back the scripts that created the figures in the future. Then we drop all the imported information besides the arguments following the script name and print the information to the terminal. #&gt; ── Script: scripts/plot_SF.R ──────────────────────────────────────────── #&gt; Parameters read: #&gt; #&gt; ─────────────────────────────────────────── /current/working/directory ── "],
["supplementary-figure-6.html", "23 Supplementary Figure 6 23.1 Summary 23.2 Details of plot_SF.R", " 23 Supplementary Figure 6 23.1 Summary This is the accessory documentation of Supplementary Figure . The Figure can be recreated by running the R script plot_SF.R: cd $BASE_DIR Rscript --vanilla R/fig/plot_SF 23.2 Details of plot_SF.R In the following, the individual steps of the R script are documented. It is an executable R script that depends on the accessory R package GenomicOriginsScripts. 23.2.1 Config The scripts start with a header that contains copy &amp; paste templates to execute or debug the script: The next section processes the input from the command line. It stores the arguments in the vector args. The R package GenomicOriginsScripts is loaded and the script name and the current working directory are stored inside variables (script_name, plot_comment). This information will later be written into the meta data of the figure to help us tracing back the scripts that created the figures in the future. Then we drop all the imported information besides the arguments following the script name and print the information to the terminal. #&gt; ── Script: scripts/plot_SF.R ──────────────────────────────────────────── #&gt; Parameters read: #&gt; #&gt; ─────────────────────────────────────────── /current/working/directory ── "],
["supplementary-figure-7.html", "24 Supplementary Figure 7 24.1 Summary 24.2 Details of plot_SF.R", " 24 Supplementary Figure 7 24.1 Summary This is the accessory documentation of Supplementary Figure . The Figure can be recreated by running the R script plot_SF.R: cd $BASE_DIR Rscript --vanilla R/fig/plot_SF 24.2 Details of plot_SF.R In the following, the individual steps of the R script are documented. It is an executable R script that depends on the accessory R package GenomicOriginsScripts. 24.2.1 Config The scripts start with a header that contains copy &amp; paste templates to execute or debug the script: The next section processes the input from the command line. It stores the arguments in the vector args. The R package GenomicOriginsScripts is loaded and the script name and the current working directory are stored inside variables (script_name, plot_comment). This information will later be written into the meta data of the figure to help us tracing back the scripts that created the figures in the future. Then we drop all the imported information besides the arguments following the script name and print the information to the terminal. #&gt; ── Script: scripts/plot_SF.R ──────────────────────────────────────────── #&gt; Parameters read: #&gt; #&gt; ─────────────────────────────────────────── /current/working/directory ── "],
["supplementary-figure-8.html", "25 Supplementary Figure 8 25.1 Summary 25.2 Details of plot_SF.R", " 25 Supplementary Figure 8 25.1 Summary This is the accessory documentation of Supplementary Figure . The Figure can be recreated by running the R script plot_SF.R: cd $BASE_DIR Rscript --vanilla R/fig/plot_SF 25.2 Details of plot_SF.R In the following, the individual steps of the R script are documented. It is an executable R script that depends on the accessory R package GenomicOriginsScripts. 25.2.1 Config The scripts start with a header that contains copy &amp; paste templates to execute or debug the script: The next section processes the input from the command line. It stores the arguments in the vector args. The R package GenomicOriginsScripts is loaded and the script name and the current working directory are stored inside variables (script_name, plot_comment). This information will later be written into the meta data of the figure to help us tracing back the scripts that created the figures in the future. Then we drop all the imported information besides the arguments following the script name and print the information to the terminal. #&gt; ── Script: scripts/plot_SF.R ──────────────────────────────────────────── #&gt; Parameters read: #&gt; #&gt; ─────────────────────────────────────────── /current/working/directory ── "],
["supplementary-figure-9.html", "26 Supplementary Figure 9 26.1 Summary 26.2 Details of plot_SF.R", " 26 Supplementary Figure 9 26.1 Summary This is the accessory documentation of Supplementary Figure . The Figure can be recreated by running the R script plot_SF.R: cd $BASE_DIR Rscript --vanilla R/fig/plot_SF 26.2 Details of plot_SF.R In the following, the individual steps of the R script are documented. It is an executable R script that depends on the accessory R package GenomicOriginsScripts. 26.2.1 Config The scripts start with a header that contains copy &amp; paste templates to execute or debug the script: The next section processes the input from the command line. It stores the arguments in the vector args. The R package GenomicOriginsScripts is loaded and the script name and the current working directory are stored inside variables (script_name, plot_comment). This information will later be written into the meta data of the figure to help us tracing back the scripts that created the figures in the future. Then we drop all the imported information besides the arguments following the script name and print the information to the terminal. #&gt; ── Script: scripts/plot_SF.R ──────────────────────────────────────────── #&gt; Parameters read: #&gt; #&gt; ─────────────────────────────────────────── /current/working/directory ── "]
]
