[
["index.html", "Script repository (Hench et al. supplement) 1 Intro 1.1 Analysis 1.2 Prerequesites 1.3 Figures", " Script repository (Hench et al. supplement) Kosmas Hench 2020-06-17 1 Intro This repository contains the complete workflow used in the paper “Ancestral variation, hybridization and modularity fuel a marine radiation”. The individual chapters of this documentation follow the separate main steps of the workflow, which each refer to an individual prefix in the git x.x references of the papers method section. The individual steps partly depend on each other - especially git 1 - git 3 should be executed in order and before the other steps. 1.1 Analysis A documentation of the data preparation and the data analysis (git 1.x - 10.x) can be found at: git 1.x: Genotyping git 2.x: Genotyping all base pairs git 3.x: Analysis (FST &amp; GxP) git 4.x: Analysis (dXY &amp; \\(\\pi\\)) git 5.x: Analysis (phylogeny &amp; topolgy weighting) git 6.x: Analysis (\\(\\rho\\)) git 7.x: Analysis (population trees) git 8.x: Analysis (demographic history) git 9.x: Analysis (hybridization) git 10.x: Analysis (admixture) 1.2 Prerequesites All scripts assume two variables to be set within the bash environment: $BASE_DIR is assumed to point to the base folder of this repository $SFTWR is a folder that contains all the software dependencies that are used within the scripts The analysis is controlled using the workflow manager nextflow and uses slightly different configurations across the individual pipelines. The exact commands used to execute the analysis during the development of the publication are stored within the aliases set within sh/nextflow_alias.sh. Furthermore, external dependencies need to be downloaded and deployed at the expected places (s. README.md at the ressources folder). 1.3 Figures The creation of the figures is bundled in a single script (git 11) which can be executed once all nextflow scripts have successfully run. cd $BASE_DIR bash sh/create_figures.sh This is basically just a collection that will run all scripts located under $BASE_DIR/R/fig. Under this location, you will find one R script per figure (and suppl. figure). So if you are only interested in a single figure - that is the place to start looking. Furthermore, a more detailed documentation exists for all the figure scripts used for the manuscript: F1, F2, F3 &amp; F4 as well as for all the supplementary figures: SF1, SF2, SF3, SF4, SF5, SF6, SF7, SF8 and SF9. "],
["git-1-genotyping-i-snps-only.html", "2 (git 1) Genotyping I (SNPs only) 2.1 Summary 2.2 Details of genotyping.nf", " 2 (git 1) Genotyping I (SNPs only) 2.1 Summary The genotyping procedure is controlled by the nextflow script genotyping.nf (located under $BASE_DIR/nf/01_genotyping/). It takes the analysis from the raw sequencing data to the genotyped and phased SNPs. Below is an overview of the steps involved in the genotyping process. (The green dot indicates the raw data input, red arrows depict output that is exported for further use.) 2.2 Details of genotyping.nf 2.2.1 Data preparation The nextflow script starts with a small header and then opens the analysis by reading a table with meta data about the samples. The table is parsed and the values are stored in nextflow variables. #!/usr/bin/env nextflow /* =============================================================== Disclaimer: This pipeline needs a lot of time &amp; memory to run: All in all we used roughly 10 TB and ran for about 1 Month (mainly due to limited bandwidth on the cluster durint the &quot;receive_tuple step) =============================================================== */ // git 1.1 /* open the pipeline based on the metadata spread sheet that includes all information necessary to assign read groups to the sequencing data, split the spread sheet by row and feed it into a channel */ Channel .fromPath(&#39;../../metadata/file_info.txt&#39;) .splitCsv(header:true, sep:&quot;\\t&quot;) .map{ row -&gt; [ id:row.id, label:row.label, file_fwd:row.file_fwd, file_rev:row.file_rev, flowcell_id_fwd:row.flowcell_id_fwd, lane_fwd:row.lane_fwd, company:row.company] } .set { samples_ch } // git 1.2 /* for every sequencing file, convert into ubam format and assign read groups */ process split_samples { label &#39;L_20g2h_split_samples&#39; input: val x from samples_ch output: set val( &quot;${x.label}.${x.lane_fwd}&quot; ), file( &quot;${x.label}.${x.lane_fwd}.ubam.bam&quot; ) into ubams_mark, ubams_merge script: &quot;&quot;&quot; echo -e &quot;---------------------------------&quot; echo -e &quot;Label:\\t\\t${x.label}\\nFwd:\\t\\t${x.file_fwd}\\nRev:\\t\\t${x.file_rev}&quot; echo -e &quot;Flowcell:\\t${x.flowcell_id_fwd}\\nLane:\\t\\t${x.lane_fwd}&quot; echo -e &quot;Read group:\\t${x.flowcell_id_fwd}.${x.lane_fwd}\\nCompany:\\t${x.company}&quot; mkdir -p \\$BASE_DIR/temp_files gatk --java-options &quot;-Xmx20G&quot; \\ FastqToSam \\ -SM=${x.label} \\ -F1=\\$BASE_DIR/data/seqdata/${x.file_fwd} \\ -F2=\\$BASE_DIR/data/seqdata/${x.file_rev} \\ -O=${x.label}.${x.lane_fwd}.ubam.bam \\ -RG=${x.label}.${x.lane_fwd} \\ -LB=${x.label}&quot;.lib1&quot; \\ -PU=${x.flowcell_id_fwd}.${x.lane_fwd} \\ -PL=Illumina \\ -CN=${x.company} \\ --TMP_DIR=\\$BASE_DIR/temp_files; &quot;&quot;&quot; } // git 1.3 /* for every ubam file, mark Illumina adapters */ process mark_adapters { label &#39;L_20g2h_mark_adapters&#39; tag &quot;${sample}&quot; input: set val( sample ), file( input ) from ubams_mark output: set val( sample ), file( &quot;*.adapter.bam&quot;) into adapter_bams file &quot;*.adapter.metrics.txt&quot; into adapter_metrics script: &quot;&quot;&quot; gatk --java-options &quot;-Xmx18G&quot; \\ MarkIlluminaAdapters \\ -I=${input} \\ -O=${sample}.adapter.bam \\ -M=${sample}.adapter.metrics.txt \\ -TMP_DIR=\\$BASE_DIR/temp_files; &quot;&quot;&quot; } // git 1.4 adapter_bams .combine(ubams_merge, by:0) .set {merge_input} // git 1.5 /* this step includes a 3 step pipeline: * - re-transformatikon into fq format * - mapping aginst the reference genome_file * - merging with the basuch ubams to include read group information */ process map_and_merge { label &#39;L_75g24h8t_map_and_merge&#39; tag &quot;${sample}&quot; input: set val( sample ), file( adapter_bam_input ), file( ubam_input ) from merge_input output: set val( sample ), file( &quot;*.mapped.bam&quot; ) into mapped_bams script: &quot;&quot;&quot; set -o pipefail gatk --java-options &quot;-Xmx68G&quot; \\ SamToFastq \\ -I=${adapter_bam_input} \\ -FASTQ=/dev/stdout \\ -INTERLEAVE=true \\ -NON_PF=true \\ -TMP_DIR=\\$BASE_DIR/temp_files | \\ bwa mem -M -t 8 -p \\$BASE_DIR/ressources/HP_genome_unmasked_01.fa /dev/stdin | gatk --java-options &quot;-Xmx68G&quot; \\ MergeBamAlignment \\ --VALIDATION_STRINGENCY SILENT \\ --EXPECTED_ORIENTATIONS FR \\ --ATTRIBUTES_TO_RETAIN X0 \\ -ALIGNED_BAM=/dev/stdin \\ -UNMAPPED_BAM=${ubam_input} \\ -OUTPUT=${sample}.mapped.bam \\ --REFERENCE_SEQUENCE=\\$BASE_DIR/ressources/HP_genome_unmasked_01.fa.gz \\ -PAIRED_RUN true \\ --SORT_ORDER &quot;unsorted&quot; \\ --IS_BISULFITE_SEQUENCE false \\ --ALIGNED_READS_ONLY false \\ --CLIP_ADAPTERS false \\ --MAX_RECORDS_IN_RAM 2000000 \\ --ADD_MATE_CIGAR true \\ --MAX_INSERTIONS_OR_DELETIONS -1 \\ --PRIMARY_ALIGNMENT_STRATEGY MostDistant \\ --UNMAPPED_READ_STRATEGY COPY_TO_TAG \\ --ALIGNER_PROPER_PAIR_FLAGS true \\ --UNMAP_CONTAMINANT_READS true \\ -TMP_DIR=\\$BASE_DIR/temp_files &quot;&quot;&quot; } // git 1.6 /* for every mapped sample,sort and mark duplicates * (intermediate step is required to create .bai file) */ process mark_duplicates { label &#39;L_32g30h_mark_duplicates&#39; publishDir &quot;../../1_genotyping/0_sorted_bams/&quot;, mode: &#39;symlink&#39; tag &quot;${sample}&quot; input: set val( sample ), file( input ) from mapped_bams output: set val { sample - ~/\\.(\\d+)/ }, val( sample ), file( &quot;*.dedup.bam&quot;) into dedup_bams file &quot;*.dedup.metrics.txt&quot; into dedup_metrics script: &quot;&quot;&quot; set -o pipefail gatk --java-options &quot;-Xmx30G&quot; \\ SortSam \\ -I=${input} \\ -O=/dev/stdout \\ --SORT_ORDER=&quot;coordinate&quot; \\ --CREATE_INDEX=false \\ --CREATE_MD5_FILE=false \\ -TMP_DIR=\\$BASE_DIR/temp_files \\ | \\ gatk --java-options &quot;-Xmx30G&quot; \\ SetNmAndUqTags \\ --INPUT=/dev/stdin \\ --OUTPUT=intermediate.bam \\ --CREATE_INDEX=true \\ --CREATE_MD5_FILE=true \\ -TMP_DIR=\\$BASE_DIR/temp_files \\ --REFERENCE_SEQUENCE=\\$BASE_DIR/ressources/HP_genome_unmasked_01.fa.gz gatk --java-options &quot;-Xmx30G&quot; \\ MarkDuplicates \\ -I=intermediate.bam \\ -O=${sample}.dedup.bam \\ -M=${sample}.dedup.metrics.txt \\ -MAX_FILE_HANDLES=1000 \\ -TMP_DIR=\\$BASE_DIR/temp_files rm intermediate* &quot;&quot;&quot; } // git 1.7 /* index al bam files */ process index_bam { label &#39;L_32g1h_index_bam&#39; tag &quot;${sample}&quot; input: set val( sample ), val( sample_lane ), file( input ) from dedup_bams output: set val( sample ), val( sample_lane ), file( input ), file( &quot;*.bai&quot;) into ( indexed_bams, pir_bams ) script: &quot;&quot;&quot; gatk --java-options &quot;-Xmx30G&quot; \\ BuildBamIndex \\ -INPUT=${input} &quot;&quot;&quot; } // git 1.8 /* collect all bam files for each sample */ indexed_bams .groupTuple() .set {tubbled} // git 1.9 /* create one *.g.vcf file per sample */ process receive_tuple { label &#39;L_36g47h_receive_tuple&#39; publishDir &quot;../../1_genotyping/1_gvcfs/&quot;, mode: &#39;symlink&#39; tag &quot;${sample}&quot; input: set sample, sample_lane, bam, bai from tubbled output: file( &quot;*.g.vcf.gz&quot;) into gvcfs file( &quot;*.vcf.gz.tbi&quot;) into tbis script: &quot;&quot;&quot; INPUT=\\$(echo ${bam} | sed &#39;s/\\\\[/-I /g; s/\\\\]//g; s/,/ -I/g&#39;) gatk --java-options &quot;-Xmx35g&quot; HaplotypeCaller \\ -R=\\$BASE_DIR/ressources/HP_genome_unmasked_01.fa \\ \\$INPUT \\ -O ${sample}.g.vcf.gz \\ -ERC GVCF &quot;&quot;&quot; } // git 1.10 /* collect and combine all *.g.vcf files */ process gather_gvcfs { label &#39;L_O88g90h_gather_gvcfs&#39; publishDir &quot;../../1_genotyping/1_gvcfs/&quot;, mode: &#39;symlink&#39; echo true input: file( gvcf ) from gvcfs.collect() file( tbi ) from tbis.collect() output: set file( &quot;cohort.g.vcf.gz&quot; ), file( &quot;cohort.g.vcf.gz.tbi&quot; ) into ( gcvf_snps, gvcf_acs, gvcf_indel ) script: &quot;&quot;&quot; GVCF=\\$(echo &quot; ${gvcf}&quot; | sed &#39;s/ /-V /g; s/vcf.gz/vcf.gz /g&#39;) gatk --java-options &quot;-Xmx85g&quot; \\ CombineGVCFs \\ -R=\\$BASE_DIR/ressources/HP_genome_unmasked_01.fa \\ \\$GVCF \\ -O cohort.g.vcf.gz &quot;&quot;&quot; } // git 1.11 /* actual genotyping step (varinat sites only) */ process joint_genotype_snps { label &#39;L_O88g90h_joint_genotype&#39; publishDir &quot;../../1_genotyping/2_raw_vcfs/&quot;, mode: &#39;symlink&#39; input: set file( vcf ), file( tbi ) from gcvf_snps output: set file( &quot;raw_var_sites.vcf.gz&quot; ), file( &quot;raw_var_sites.vcf.gz.tbi&quot; ) into ( raw_var_sites, raw_var_sites_to_metrics ) script: &quot;&quot;&quot; gatk --java-options &quot;-Xmx85g&quot; \\ GenotypeGVCFs \\ -R=\\$BASE_DIR/ressources/HP_genome_unmasked_01.fa \\ -V=${vcf} \\ -O=intermediate.vcf.gz gatk --java-options &quot;-Xmx85G&quot; \\ SelectVariants \\ -R=\\$BASE_DIR/ressources/HP_genome_unmasked_01.fa \\ -V=intermediate.vcf.gz \\ --select-type-to-include=SNP \\ -O=raw_var_sites.vcf.gz rm intermediate.* &quot;&quot;&quot; } // git 1.12 /* generate a LG channel */ Channel .from( (&#39;01&#39;..&#39;09&#39;) + (&#39;10&#39;..&#39;19&#39;) + (&#39;20&#39;..&#39;24&#39;) ) .into{ LG_ids1; LG_ids2 } // git 1.13 /* produce metrics table to determine filtering thresholds - ups forgot to extract SNPS first*/ process joint_genotype_metrics { label &#39;L_28g5h_genotype_metrics&#39; publishDir &quot;../../1_genotyping/2_raw_vcfs/&quot;, mode: &#39;move&#39; input: set file( vcf ), file( tbi ) from raw_var_sites_to_metrics output: file( &quot;${vcf}.table.txt&quot; ) into raw_metrics script: &quot;&quot;&quot; gatk --java-options &quot;-Xmx25G&quot; \\ VariantsToTable \\ --variant=${vcf} \\ --output=${vcf}.table.txt \\ -F=CHROM -F=POS -F=MQ \\ -F=QD -F=FS -F=MQRankSum -F=ReadPosRankSum \\ --show-filtered &quot;&quot;&quot; } // git 1.14 /* filter snps basaed on locus annotations, missingness and type (bi-allelic only) */ process filterSNPs { label &#39;L_78g10h_filter_Snps&#39; publishDir &quot;../../1_genotyping/3_gatk_filtered/&quot;, mode: &#39;symlink&#39; input: set file( vcf ), file( tbi ) from raw_var_sites output: set file( &quot;filterd_bi-allelic.vcf.gz&quot; ), file( &quot;filterd_bi-allelic.vcf.gz.tbi&quot; ) into filtered_snps script: &quot;&quot;&quot; gatk --java-options &quot;-Xmx75G&quot; \\ VariantFiltration \\ -R=\\$BASE_DIR/ressources/HP_genome_unmasked_01.fa \\ -V ${vcf} \\ -O=intermediate.vcf.gz \\ --filter-expression &quot;QD &lt; 2.5&quot; \\ --filter-name &quot;filter_QD&quot; \\ --filter-expression &quot;FS &gt; 25.0&quot; \\ --filter-name &quot;filter_FS&quot; \\ --filter-expression &quot;MQ &lt; 52.0 || MQ &gt; 65.0&quot; \\ --filter-name &quot;filter_MQ&quot; \\ --filter-expression &quot;MQRankSum &lt; -0.2 || MQRankSum &gt; 0.2&quot; \\ --filter-name &quot;filter_MQRankSum&quot; \\ --filter-expression &quot;ReadPosRankSum &lt; -2.0 || ReadPosRankSum &gt; 2.0 &quot; \\ --filter-name &quot;filter_ReadPosRankSum&quot; gatk --java-options &quot;-Xmx75G&quot; \\ SelectVariants \\ -R=\\$BASE_DIR/ressources/HP_genome_unmasked_01.fa \\ -V=intermediate.vcf.gz \\ -O=intermediate.filterd.vcf.gz \\ --exclude-filtered vcftools \\ --gzvcf intermediate.filterd.vcf.gz \\ --max-missing-count 17 \\ --max-alleles 2 \\ --stdout \\ --recode | \\ bgzip &gt; filterd_bi-allelic.vcf.gz tabix -p vcf filterd_bi-allelic.vcf.gz rm intermediate.* &quot;&quot;&quot; } // git 1.15 // extract phase informative reads from // alignments and SNPs process extractPirs { label &#39;L_78g10h_extract_pirs&#39; input: val( lg ) from LG_ids2 set val( sample ), val( sample_lane ), file( input ), file( index ) from pir_bams.collect() set file( vcf ), file( tbi ) from filtered_snps output: set val( lg ), file( &quot;filterd_bi-allelic.LG${lg}.vcf.gz&quot; ), file( &quot;filterd_bi-allelic.LG${lg}.vcf.gz.tbi&quot; ), file( &quot;PIRsList-LG${lg}.txt&quot; ) into pirs_lg script: &quot;&quot;&quot; LG=&quot;LG${lg}&quot; awk -v OFS=&#39;\\t&#39; -v dir=\\$PWD -v lg=\\$LG &#39;{print \\$1,dir&quot;/&quot;\\$2,lg}&#39; \\$BASE_DIR/metadata/bamlist_proto.txt &gt; bamlist.txt vcftools \\ --gzvcf ${vcf} \\ --chr \\$LG \\ --stdout \\ --recode | \\ bgzip &gt; filterd_bi-allelic.LG${lg}.vcf.gz tabix -p vcf filterd_bi-allelic.LG${lg}.vcf.gz extractPIRs \\ --bam bamlist.txt \\ --vcf filterd_bi-allelic.LG${lg}.vcf.gz \\ --out PIRsList-LG${lg}.txt \\ --base-quality 20 \\ --read-quality 15 &quot;&quot;&quot; } // git 1.16 // run the actual phasing process run_shapeit { label &#39;L_75g24h8t_run_shapeit&#39; input: set val( lg ), file( vcf ), file( tbi ), file( pirs ) from pirs_lg output: file( &quot;phased-LG${lg}.vcf.gz&quot; ) into phased_lgs script: &quot;&quot;&quot; LG=&quot;LG${lg}&quot; shapeit \\ -assemble \\ --input-vcf ${vcf} \\ --input-pir ${pirs} \\ --thread 8 \\ -O phased-LG${lg} shapeit \\ -convert \\ --input-hap phased-LG${lg} \\ --output-vcf phased-LG${lg}.vcf bgzip phased-LG${lg}.vcf &quot;&quot;&quot; } // git 1.17 // merge the phased LGs back together. // the resulting vcf file represents // the &#39;SNPs only&#39; data set process merge_phased { label &#39;L_28g5h_merge_phased_vcf&#39; publishDir &quot;../../1_genotyping/4_phased/&quot;, mode: &#39;move&#39; input: file( vcf ) from phased_lgs.collect() output: set file( &quot;phased.vcf.gz&quot; ), file( &quot;phased.vcf.gz.tbi&quot; ) into phased_vcf set file( &quot;phased_mac2.vcf.gz&quot; ), file( &quot;phased_mac2.vcf.gz.tbi&quot; ) into phased_mac2_vcf script: &quot;&quot;&quot; vcf-concat \\ phased-LG* | \\ grep -v ^\\$ | \\ tee phased.vcf | \\ vcftools --vcf - --mac 2 --recode --stdout | \\ bgzip &gt; phased_mac2.vcf.gz bgzip phased.vcf tabix -p vcf phased.vcf.gz tabix -p vcf phased_mac2.vcf.gz &quot;&quot;&quot; } /* ========================================= */ /* appendix: generate indel masks for msmc: */ // git 1.18 // reopen the gvcf file to also genotype indels process joint_genotype_indel { label &#39;L_O88g90h_genotype_indel&#39; publishDir &quot;../../1_genotyping/2_raw_vcfs/&quot;, mode: &#39;copy&#39; input: set file( vcf ), file( tbi ) from gvcf_indel output: set file( &quot;raw_var_indel.vcf.gz&quot; ), file( &quot;raw_var_indel.vcf.gz.tbi&quot; ) into ( raw_indel, raw_indel_to_metrics ) script: &quot;&quot;&quot; gatk --java-options &quot;-Xmx85g&quot; \\ GenotypeGVCFs \\ -R=\\$REF_GENOME \\ -V=${vcf} \\ -O=intermediate.vcf.gz gatk --java-options &quot;-Xmx85G&quot; \\ SelectVariants \\ -R=\\$REF_GENOME \\ -V=intermediate.vcf.gz \\ --select-type-to-include=INDEL \\ -O=raw_var_indel.vcf.gz rm intermediate.* &quot;&quot;&quot; } // git 1.19 // export indel metrics for filtering process indel_metrics { label &#39;L_28g5h_genotype_metrics&#39; publishDir &quot;../../1_genotyping/2_raw_vcfs/&quot;, mode: &#39;copy&#39; input: set file( vcf ), file( tbi ) from raw_indel_to_metrics output: file( &quot;${vcf}.table.txt&quot; ) into raw_indel_metrics script: &quot;&quot;&quot; gatk --java-options &quot;-Xmx25G&quot; \\ VariantsToTable \\ --variant=${vcf} \\ --output=${vcf}.table.txt \\ -F=CHROM -F=POS -F=MQ \\ -F=QD -F=FS -F=MQRankSum -F=ReadPosRankSum \\ --show-filtered &quot;&quot;&quot; } // git 1.20 // hard filter indels and create mask process filterIndels { label &#39;L_78g10h_filter_indels&#39; publishDir &quot;../../1_genotyping/3_gatk_filtered/&quot;, mode: &#39;copy&#39; input: set file( vcf ), file( tbi ) from raw_indel output: set file( &quot;filterd.indel.vcf.gz&quot; ), file( &quot;filterd.indel.vcf.gz.tbi&quot; ) into filtered_indel file( &quot;indel_mask.bed.gz&quot; ) into indel_mask_ch /* FILTER THRESHOLDS NEED TO BE UPDATED */ script: &quot;&quot;&quot; gatk --java-options &quot;-Xmx75G&quot; \\ VariantFiltration \\ -R=\\$REF_GENOME \\ -V ${vcf} \\ -O=intermediate.vcf.gz \\ --filter-expression &quot;QD &lt; 2.5&quot; \\ --filter-name &quot;filter_QD&quot; \\ --filter-expression &quot;FS &gt; 25.0&quot; \\ --filter-name &quot;filter_FS&quot; \\ --filter-expression &quot;MQ &lt; 52.0 || MQ &gt; 65.0&quot; \\ --filter-name &quot;filter_MQ&quot; \\ --filter-expression &quot;SOR &gt; 3.0&quot; \\ --filter-name &quot;filter_SOR&quot; \\ --filter-expression &quot;InbreedingCoeff &lt; -0.25&quot; \\ --filter-name &quot;filter_InbreedingCoeff&quot; \\ --filter-expression &quot;MQRankSum &lt; -0.2 || MQRankSum &gt; 0.2&quot; \\ --filter-name &quot;filter_MQRankSum&quot; \\ --filter-expression &quot;ReadPosRankSum &lt; -2.0 || ReadPosRankSum &gt; 2.0 &quot; \\ --filter-name &quot;filter_ReadPosRankSum&quot; gatk --java-options &quot;-Xmx75G&quot; \\ SelectVariants \\ -R=\\$REF_GENOME \\ -V=intermediate.vcf.gz \\ -O=filterd.indel.vcf.gz \\ --exclude-filtered zcat filterd.indel.vcf.gz | \\ awk &#39;! /\\\\#/&#39; | \\ awk &#39;{if(length(\\$4) &gt; length(\\$5)) print \\$1&quot;\\\\t&quot;(\\$2-6)&quot;\\\\t&quot;(\\$2+length(\\$4)+4); else print \\$1&quot;\\\\t&quot;(\\$2-6)&quot;\\\\t&quot;(\\$2+length(\\$5)+4)}&#39; | \\ gzip -c &gt; indel_mask.bed.gz rm intermediate.* &quot;&quot;&quot; } // git 1.21 /* create channel of linkage groups */ Channel .from( (&#39;01&#39;..&#39;09&#39;) + (&#39;10&#39;..&#39;19&#39;) + (&#39;20&#39;..&#39;24&#39;) ) .map{ &quot;LG&quot; + it } .into{ lg_ch } // git 1.22 // attach linkage groups to indel masks lg_ch.combine( filtered_indel ).set{ filtered_indel_lg } // git 1.23 // split indel mask by linkage group process split_indel_mask { label &#39;L_loc_split_indel_mask&#39; publishDir &quot;../../ressources/indel_masks/&quot;, mode: &#39;copy&#39; input: set val( lg ), file( bed ) from filtered_indel_lg output: set val( lg ), file( &quot;indel_mask.${lg}.bed.gz &quot; ) into lg_indel_mask script: &quot;&quot;&quot; gzip -cd ${bed} | \\ grep ${lg} | \\ gzip -c &gt; indel_mask.${lg}.bed.gz &quot;&quot;&quot; } All indel masks are exported to the resources folder within the root directory. "],
["git-2-genotyping-ii-all-callable-sites.html", "3 (git 2) Genotyping II (all callable sites) 3.1 Summary 3.2 Details of genotyping_all_basepairs.nf", " 3 (git 2) Genotyping II (all callable sites) 3.1 Summary The genotyping procedure is controlled by the nextflow script genotyping_all_basepairs.nf (located under $BASE_DIR/nf/02_genotyping_all_basepairs/). Based on an intermediate step from genotyping.nf, this script produces a data set that includes all callable sites - that is SNPs as well a invariant sites that are covered by sequence. Below is an overview of the steps involved in the genotyping process. (The green dot indicates the data input, red arrows depict output that is exported for further use.) 3.2 Details of genotyping_all_basepairs.nf 3.2.1 Data preparation The nextflow script starts with a small header and then imports the joint genotyping likelihoods for all samples produced by genotyping_all_basepairs.nf. #!/usr/bin/env nextflow // git 2.1 // open genotype likelyhoods Channel .fromFilePairs(&quot;../../1_genotyping/1_gvcfs/cohort.g.vcf.{gz,gz.tbi}&quot;) .set{ vcf_cohort } // git 2.2 // initialize LG channel Channel .from( (&#39;01&#39;..&#39;09&#39;) + (&#39;10&#39;..&#39;19&#39;) + (&#39;20&#39;..&#39;24&#39;) ) .set{ ch_LG_ids } // git 2.3 // combine genotypes and LGs ch_LG_ids.combine( vcf_cohort ).set{ vcf_lg_combo } // git 2.4 // actual genotyping step (including invariant sites) process joint_genotype_snps { label &quot;L_O88g90h_LGs_genotype&quot; input: set val( lg ), vcfId, file( vcf ) from vcf_lg_combo output: set val( &#39;all&#39; ), val( lg ), file( &quot;all_site*.vcf.gz&quot; ), file( &quot;all_site*.vcf.gz.tbi&quot; ) into all_bp_by_location script: &quot;&quot;&quot; gatk --java-options &quot;-Xmx85g&quot; \\ GenotypeGVCFs \\ -R=\\$BASE_DIR/ressources/HP_genome_unmasked_01.fa \\ -L=LG${lg} \\ -V=${vcf[0]} \\ -O=intermediate.vcf.gz \\ --include-non-variant-sites=true gatk --java-options &quot;-Xmx85G&quot; \\ SelectVariants \\ -R=\\$BASE_DIR/ressources/HP_genome_unmasked_01.fa \\ -V=intermediate.vcf.gz \\ --select-type-to-exclude=INDEL \\ -O=all_sites.LG${lg}.vcf.gz rm intermediate.* &quot;&quot;&quot; } // git 2.5 // merge all LGs process merge_genotypes { label &#39;L_78g5h_merge_genotypes&#39; echo true input: set val( dummy ), val( lg ), file( vcf ), file( tbi ) from all_bp_by_location.groupTuple() output: file( &quot;all_sites.vcf.gz&quot; ) into all_bp_merged script: &quot;&quot;&quot; INPUT=\\$(ls -1 *vcf.gz | sed &#39;s/^/ -I /g&#39; | cat \\$( echo )) gatk --java-options &quot;-Xmx85g&quot; \\ GatherVcfs \\ \\$INPUT \\ -O=all_sites.vcf.gz &quot;&quot;&quot; } // git 2.6 // quality based filtering process filterSNP_first { label &#39;L_105g30h_filter_gt1&#39; input: file( vcf ) from all_bp_merged output: set file( &quot;intermediate.filterd.vcf.gz&quot; ), file( &quot;intermediate.filterd.vcf.gz.tbi&quot; ) into filtered_snps_first script: &quot;&quot;&quot; module load openssl1.0.2 tabix -p vcf ${vcf} gatk --java-options &quot;-Xmx75G&quot; \\ VariantFiltration \\ -R=\\$BASE_DIR/ressources/HP_genome_unmasked_01.fa \\ -V ${vcf} \\ -O=intermediate.vcf.gz \\ --filter-expression &quot;QD &lt; 2.5&quot; \\ --filter-name &quot;filter_QD&quot; \\ --filter-expression &quot;FS &gt; 25.0&quot; \\ --filter-name &quot;filter_FS&quot; \\ --filter-expression &quot;MQ &lt; 52.0 || MQ &gt; 65.0&quot; \\ --filter-name &quot;filter_MQ&quot; \\ --filter-expression &quot;MQRankSum &lt; -0.2 || MQRankSum &gt; 0.2&quot; \\ --filter-name &quot;filter_MQRankSum&quot; \\ --filter-expression &quot;ReadPosRankSum &lt; -2.0 || ReadPosRankSum &gt; 2.0 &quot; \\ --filter-name &quot;filter_ReadPosRankSum&quot; \\ --QUIET true &amp;&gt; var_filt.log gatk --java-options &quot;-Xmx75G&quot; \\ SelectVariants \\ -R=\\$BASE_DIR/ressources/HP_genome_unmasked_01.fa \\ -V=intermediate.vcf.gz \\ -O=intermediate.filterd.vcf.gz \\ --exclude-filtered \\ --QUIET true \\ --verbosity ERROR &amp;&gt; var_select.log &quot;&quot;&quot; } // git 2.7 // missingness based filtering // the resulting vcf file represents // the &#39;all BP&#39; data set process filterSNP_second { label &#39;L_105g30h_filter_gt2&#39; publishDir &quot;../../1_genotyping/3_gatk_filtered/&quot;, mode: &#39;copy&#39; input: set file( vcf ), file( tbi ) from filtered_snps_first output: file( &quot;filterd.allBP.vcf.gz&quot; ) into filtered_snps script: &quot;&quot;&quot; module load openssl1.0.2 vcftools \\ --gzvcf ${vcf} \\ --max-missing-count 17 \\ --stdout \\ --recode | \\ bgzip &gt; filterd.allBP.vcf.gz &quot;&quot;&quot; } Finally, we are done with the second version of genotyping. "],
["git-3-analysis-i-fst-gxp.html", "4 (git 3) Analysis I (FST &amp; GxP) 4.1 Summary 4.2 Details of analysis_fst_gxp.nf", " 4 (git 3) Analysis I (FST &amp; GxP) 4.1 Summary The genetic differentiation, as well as the genotype x phenotype association, are computed within the nextflow script analysis_fst_gxp.nf (located under $BASE_DIR/nf/03_analysis_fst_gxp/). It takes the SNPs only data set and computes FST and the GxP association. Below is an overview of the steps involved in the analysis. (The green dot indicates the genotype input, red arrows depict output that is exported for further use.) 4.2 Details of analysis_fst_gxp.nf 4.2.1 Setup The nextflow script starts by opening the genotype data and feeding it into three different streams. #!/usr/bin/env nextflow // git 3.1 // open genotype data Channel .fromFilePairs(&quot;../../1_genotyping/4_phased/phased_mac2.vcf.{gz,gz.tbi}&quot;) .into{ vcf_locations; vcf_filter; vcf_gxp; vcf_adapt } // git 3.2 // initialize location channel Channel .from( &quot;bel&quot;, &quot;hon&quot;, &quot;pan&quot;) .set{ locations_ch } // git 3.3 // attach genotypes to location locations_ch .combine( vcf_locations ) .set{ vcf_location_combo } // git 3.4 // define location specific sepcies set Channel.from( [[1, &quot;ind&quot;], [2, &quot;may&quot;], [3, &quot;nig&quot;], [4, &quot;pue&quot;], [5, &quot;uni&quot;]] ).into{ bel_spec1_ch; bel_spec2_ch } Channel.from( [[1, &quot;abe&quot;], [2, &quot;gum&quot;], [3, &quot;nig&quot;], [4, &quot;pue&quot;], [5, &quot;ran&quot;], [6, &quot;uni&quot;]] ).into{ hon_spec1_ch; hon_spec2_ch } Channel.from( [[1, &quot;nig&quot;], [2, &quot;pue&quot;], [3, &quot;uni&quot;]] ).into{ pan_spec1_ch; pan_spec2_ch } // git 3.5 // subset data to local hamlets process subset_vcf_by_location { label &quot;L_20g2h_subset_vcf&quot; input: set val( loc ), vcfId, file( vcf ) from vcf_location_combo output: set val( loc ), file( &quot;${loc}.vcf.gz&quot; ), file( &quot;${loc}.pop&quot; ) into ( vcf_loc_pair1, vcf_loc_pair2, vcf_loc_pair3 ) script: &quot;&quot;&quot; vcfsamplenames ${vcf[0]} | \\ grep ${loc} | \\ grep -v tor | \\ grep -v tab &gt; ${loc}.pop vcftools --gzvcf ${vcf[0]} \\ --keep ${loc}.pop \\ --mac 3 \\ --recode \\ --stdout | gzip &gt; ${loc}.vcf.gz &quot;&quot;&quot; } // git 3.6 // subset the global data set to hamlets only process subset_vcf_hamlets_only { label &quot;L_20g15h_filter_hamlets_only&quot; publishDir &quot;../../1_genotyping/4_phased/&quot;, mode: &#39;copy&#39; , pattern: &quot;*.vcf.gz&quot; //module &quot;R3.5.2&quot; input: set vcfId, file( vcf ) from vcf_filter output: file( &quot;hamlets_only.vcf.gz*&quot; ) into vcf_hamlets_only set file( &quot;hamlets_only.vcf.gz*&quot; ), file( &quot;hamlets_only.pop.txt&quot; ) into vcf_multi_fst script: &quot;&quot;&quot; vcfsamplenames ${vcf[0]} | \\ grep -v &quot;abe\\\\|gum\\\\|ind\\\\|may\\\\|nig\\\\|pue\\\\|ran\\\\|uni&quot; &gt; outgroup.pop vcfsamplenames ${vcf[0]} | \\ grep &quot;abe\\\\|gum\\\\|ind\\\\|may\\\\|nig\\\\|pue\\\\|ran\\\\|uni&quot; | \\ awk &#39;{print \\$1&quot;\\\\t&quot;\\$1}&#39; | \\ sed &#39;s/\\\\t.*\\\\(...\\\\)\\\\(...\\\\)\\$/\\\\t\\\\1\\\\t\\\\2/g&#39; &gt; hamlets_only.pop.txt vcftools \\ --gzvcf ${vcf[0]} \\ --remove outgroup.pop \\ --recode \\ --stdout | gzip &gt; hamlets_only.vcf.gz &quot;&quot;&quot; } // ----------- Fst section ----------- // git 3.7 // compute global fst process fst_multi { label &#39;L_20g15h_fst_multi&#39; publishDir &quot;../../2_analysis/fst/50k/&quot;, mode: &#39;copy&#39; , pattern: &quot;*.50k.tsv.gz&quot; publishDir &quot;../../2_analysis/fst/10k/&quot;, mode: &#39;copy&#39; , pattern: &quot;*.10k.tsv.gz&quot; publishDir &quot;../../2_analysis/fst/logs/&quot;, mode: &#39;copy&#39; , pattern: &quot;*.log&quot; publishDir &quot;../../2_analysis/summaries&quot;, mode: &#39;copy&#39; , pattern: &quot;fst_outliers_998.tsv&quot; //conda &quot;$HOME/miniconda2/envs/py3&quot; //module &quot;R3.5.2&quot; input: set file( vcf ), file( pop ) from vcf_multi_fst output: file( &quot;multi_fst*&quot; ) into multi_fst_output file( &quot;fst_outliers_998.tsv&quot; ) into fst_outlier_output script: &quot;&quot;&quot; awk &#39;{print \\$1&quot;\\\\t&quot;\\$2\\$3}&#39; ${pop} &gt; pop.txt for k in abehon gumhon indbel maybel nigbel nighon nigpan puebel puehon puepan ranhon unibel unihon unipan; do grep \\$k pop.txt | cut -f 1 &gt; pop.\\$k.txt done POP=&quot;--weir-fst-pop pop.abehon.txt \\ --weir-fst-pop pop.gumhon.txt \\ --weir-fst-pop pop.indbel.txt \\ --weir-fst-pop pop.maybel.txt \\ --weir-fst-pop pop.nigbel.txt \\ --weir-fst-pop pop.nighon.txt \\ --weir-fst-pop pop.nigpan.txt \\ --weir-fst-pop pop.puebel.txt \\ --weir-fst-pop pop.puehon.txt \\ --weir-fst-pop pop.puepan.txt \\ --weir-fst-pop pop.ranhon.txt \\ --weir-fst-pop pop.unibel.txt \\ --weir-fst-pop pop.unihon.txt \\ --weir-fst-pop pop.unipan.txt&quot; # fst by SNP # ---------- vcftools --gzvcf ${vcf} \\ \\$POP \\ --stdout 2&gt; multi_fst_snp.log | \\ gzip &gt; multi_fst.tsv.gz # fst 50kb window # --------------- vcftools --gzvcf ${vcf} \\ \\$POP \\ --fst-window-step 5000 \\ --fst-window-size 50000 \\ --stdout 2&gt; multi_fst.50k.log | \\ gzip &gt; multi_fst.50k.tsv.gz # fst 10kb window # --------------- vcftools --gzvcf ${vcf} \\ \\$POP \\ --fst-window-step 1000 \\ --fst-window-size 10000 \\ --stdout 2&gt; multi_fst.10k.log | \\ gzip &gt; multi_fst_snp.tsv.gz Rscript --vanilla \\$BASE_DIR/R/table_fst_outliers.R multi_fst.50k.tsv.gz &quot;&quot;&quot; } // git 3.8 // prepare pairwise fsts // ------------------------------ /* (create all possible species pairs depending on location and combine with genotype subset (for the respective location))*/ // ------------------------------ /* channel content after joinig: set [0:val(loc), 1:file(vcf), 2:file(pop), 3:val(spec1), 4:val(spec2)]*/ // ------------------------------ bel_pairs_ch = Channel.from( &quot;bel&quot; ) .join( vcf_loc_pair1 ) .combine(bel_spec1_ch) .combine(bel_spec2_ch) .filter{ it[3] &lt; it[5] } .map{ it[0,1,2,4,6]} hon_pairs_ch = Channel.from( &quot;hon&quot; ) .join( vcf_loc_pair2 ) .combine(hon_spec1_ch) .combine(hon_spec2_ch) .filter{ it[3] &lt; it[5] } .map{ it[0,1,2,4,6]} pan_pairs_ch = Channel.from( &quot;pan&quot; ) .join( vcf_loc_pair3 ) .combine(pan_spec1_ch) .combine(pan_spec2_ch) .filter{ it[3] &lt; it[5] } .map{ it[0,1,2,4,6]} bel_pairs_ch.concat( hon_pairs_ch, pan_pairs_ch ).set { all_fst_pairs_ch } // git 3.9 // comute pairwise fsts process fst_run { label &#39;L_32g4h_fst_run&#39; publishDir &quot;../../2_analysis/fst/50k/&quot;, mode: &#39;copy&#39; , pattern: &quot;*.50k.windowed.weir.fst.gz&quot; publishDir &quot;../../2_analysis/fst/10k/&quot;, mode: &#39;copy&#39; , pattern: &quot;*.10k.windowed.weir.fst.gz&quot; publishDir &quot;../../2_analysis/fst/logs/&quot;, mode: &#39;copy&#39; , pattern: &quot;${loc}-${spec1}-${spec2}.log&quot; input: set val( loc ), file( vcf ), file( pop ), val( spec1 ), val( spec2 ) from all_fst_pairs_ch output: set val( loc ), file( &quot;*.50k.windowed.weir.fst.gz&quot; ), file( &quot;${loc}-${spec1}-${spec2}.log&quot; ) into fst_50k file( &quot;*.10k.windowed.weir.fst.gz&quot; ) into fst_10k_output file( &quot;${loc}-${spec1}-${spec2}.log&quot; ) into fst_logs script: &quot;&quot;&quot; grep ${spec1} ${pop} &gt; pop1.txt grep ${spec2} ${pop} &gt; pop2.txt vcftools --gzvcf ${vcf} \\ --weir-fst-pop pop1.txt \\ --weir-fst-pop pop2.txt \\ --fst-window-step 5000 \\ --fst-window-size 50000 \\ --out ${loc}-${spec1}-${spec2}.50k 2&gt; ${loc}-${spec1}-${spec2}.log vcftools --gzvcf ${vcf} \\ --weir-fst-pop pop1.txt \\ --weir-fst-pop pop2.txt \\ --fst-window-size 10000 \\ --fst-window-step 1000 \\ --out ${loc}-${spec1}-${spec2}.10k gzip *.windowed.weir.fst &quot;&quot;&quot; } // git 3.10 /* collect the VCFtools logs to crate a table with the genome wide fst values */ process fst_globals { label &#39;L_loc_fst_globals&#39; publishDir &quot;../../2_analysis/summaries&quot;, mode: &#39;copy&#39; , pattern: &quot;fst_globals.txt&quot; //module &quot;R3.5.2&quot; input: file( log ) from fst_logs.collect() output: file( &quot;fst_globals.txt&quot; ) into fst_glob script: &quot;&quot;&quot; cat *.log | \\ grep -E &#39;Weir and Cockerham|--out&#39; | \\ grep -A 3 50k | \\ sed &#39;/^--/d; s/^.*--out //g; s/.50k//g; /^Output/d; s/Weir and Cockerham //g; s/ Fst estimate: /\\t/g&#39; | \\ paste - - - | \\ cut -f 1,3,5 | \\ sed &#39;s/^\\\\(...\\\\)-/\\\\1\\\\t/g&#39; &gt; fst_globals.txt &quot;&quot;&quot; } // ----------- G x P section ----------- // git 3.11 // reformat genotypes (1) process plink12 { label &#39;L_20g2h_plink12&#39; input: set vcfId, file( vcf ) from vcf_gxp output: set file( &quot;GxP_plink.map&quot; ), file( &quot;GxP_plink.ped&quot; ) into plink_GxP script: &quot;&quot;&quot; vcfsamplenames ${vcf[0]} | \\ grep -v &quot;tor\\\\|tab\\\\|flo&quot; | \\ awk &#39;{print \\$1&quot;\\\\t&quot;\\$1}&#39; | \\ sed &#39;s/\\\\t.*\\\\(...\\\\)\\\\(...\\\\)\\$/\\\\t\\\\1\\\\t\\\\2/g&#39; &gt; pop.txt vcftools \\ --gzvcf ${vcf[0]} \\ --plink \\ --out GxP_plink plink \\ --file GxP_plink \\ --recode12 \\ --out hapmap &quot;&quot;&quot; } // git 3.12 // reformat genotypes (2) process GxP_run { label &#39;L_20g2h_GxP_binary&#39; input: set file( map ), file( ped ) from plink_GxP output: set file( &quot;*.bed&quot; ), file( &quot;*.bim&quot; ),file( &quot;*.fam&quot; ) into plink_binary script: &quot;&quot;&quot; # convert genotypes into binary format (bed/bim/fam) plink \\ --noweb \\ --file GxP_plink \\ --make-bed \\ --out GxP_plink_binary &quot;&quot;&quot; } // git 3.13 // import phenotypes Channel .fromPath(&quot;../../metadata/phenotypes.sc&quot;) .set{ phenotypes_raw } // git 3.13 // run PCA on phenotypes process phenotye_pca { label &quot;L_loc_phenotype_pca&quot; publishDir &quot;../../2_analysis/phenotype&quot;, mode: &#39;copy&#39; , pattern: &quot;*.txt&quot; //module &quot;R3.5.2&quot; input: file( sc ) from phenotypes_raw output: file( &quot;phenotypes.txt&quot; ) into phenotype_file file( &quot;phenotype_pca*.pdf&quot; ) into phenotype_pca script: &quot;&quot;&quot; Rscript --vanilla \\$BASE_DIR/R/phenotypes_pca.R ${sc} &quot;&quot;&quot; } // git 3.15 // setup GxP traits Channel .from(&quot;Bars&quot;, &quot;Snout&quot;, &quot;Peduncle&quot;) //, &quot;Lines&quot;, &quot;Blue&quot;, &quot;Yellow&quot;, &quot;Orange&quot;, &quot;Tail_transparent&quot;,&quot;PC1&quot;, &quot;PC2&quot;, &quot;PC_d1&quot;, &quot;abe&quot;, &quot;gum&quot;, &quot;ind&quot;, &quot;may&quot;, &quot;nig&quot;, &quot;pue&quot;, &quot;ran&quot;, &quot;uni&quot;, &quot;blue2&quot; ) .set{ traits_ch } // git 3.16 // bundle GxP input traits_ch.combine( plink_binary ).combine( phenotype_file ).set{ trait_plink_combo } // git 3.17 // actually run the GxP process gemma_run { label &#39;L_32g4h_GxP_run&#39; publishDir &quot;../../2_analysis/GxP/bySNP/&quot;, mode: &#39;copy&#39; //module &quot;R3.5.2&quot; input: set val( pheno ), file( bed ), file( bim ), file( fam ), file( pheno_file ) from trait_plink_combo output: file(&quot;*.GxP.txt.gz&quot;) into gemma_results script: &quot;&quot;&quot; source \\$BASE_DIR/sh/body.sh BASE_NAME=\\$(echo ${fam} | sed &#39;s/.fam//g&#39;) mv ${fam} \\$BASE_NAME-old.fam cp \\${BASE_NAME}-old.fam ${fam} # 1) replace the phenotype values Rscript --vanilla \\$BASE_DIR/R/assign_phenotypes.R ${fam} ${pheno_file} ${pheno} # 2) create relatedness matrix of samples using gemma gemma -bfile \\$BASE_NAME -gk 1 -o ${pheno} # 3) fit linear model using gemma (-lm) gemma -bfile \\$BASE_NAME -lm 4 -miss 0.1 -notsnp -o ${pheno}.lm # 4) fit linear mixed model using gemma (-lmm) gemma -bfile \\$BASE_NAME -k output/${pheno}.cXX.txt -lmm 4 -o ${pheno}.lmm # 5) reformat output sed &#39;s/\\\\trs\\\\t/\\\\tCHROM\\\\tPOS\\\\t/g; s/\\\\([0-2][0-9]\\\\):/\\\\1\\\\t/g&#39; output/${pheno}.lm.assoc.txt | \\ cut -f 2,3,9-14 | body sort -k1,1 -k2,2n | gzip &gt; ${pheno}.lm.GxP.txt.gz sed &#39;s/\\\\trs\\\\t/\\\\tCHROM\\\\tPOS\\\\t/g; s/\\\\([0-2][0-9]\\\\):/\\\\1\\\\t/g&#39; output/${pheno}.lmm.assoc.txt | \\ cut -f 2,3,8-10,13-15 | body sort -k1,1 -k2,2n | gzip &gt; ${pheno}.lmm.GxP.txt.gz &quot;&quot;&quot; } // git 3.18 // setup smoothing levels Channel .from([[50000, 5000], [10000, 1000]]) .set{ gxp_smoothing_levels } // git 3.19 // apply all smoothing levels gemma_results.combine( gxp_smoothing_levels ).set{ gxp_smoothing_input } // git 3.20 // actually run the smoothing process gemma_smooth { label &#39;L_20g2h_GxP_smooth&#39; publishDir &quot;../../2_analysis/GxP/${win}&quot;, mode: &#39;copy&#39; input: set file( lm ), file( lmm ), val( win ), val( step ) from gxp_smoothing_input output: set val( win ), file( &quot;*.lm.*k.txt.gz&quot; ) into gxp_lm_smoothing_output set val( win ), file( &quot;*.lmm.*k.txt.gz&quot; ) into gxp_lmm_smoothing_output script: &quot;&quot;&quot; \\$BASE_DIR/sh/gxp_slider ${lm} ${win} ${step} \\$BASE_DIR/sh/gxp_slider ${lmm} ${win} ${step} &quot;&quot;&quot; } // Fst within species --------------------------------------------------------- // git 3.21 // define species set Channel .from( &quot;nig&quot;, &quot;pue&quot;, &quot;uni&quot;) .set{ species_ch } // git 3.22 // define location set Channel.from( [[1, &quot;bel&quot;], [2, &quot;hon&quot;], [3, &quot;pan&quot;]]).into{ locations_ch_1;locations_ch_2 } // git 3.23 // create location pairs locations_ch_1 .combine(locations_ch_2) .filter{ it[0] &lt; it[2] } .map{ it[1,3]} .combine( species_ch ) .combine( vcf_adapt ) .set{ vcf_location_combo_adapt } // git 3.24 // compute pairwise fsts process fst_run_adapt { label &#39;L_20g4h_fst_run_adapt&#39; publishDir &quot;../../2_analysis/fst/adapt/&quot;, mode: &#39;copy&#39; , pattern: &quot;*.log&quot; input: set val( loc1 ), val( loc2 ), val( spec ), val(vcf_indx), file( vcf ) from vcf_location_combo_adapt output: file( &quot;adapt_${spec}${loc1}-${spec}${loc2}.log&quot; ) into fst_adapt_logs script: &quot;&quot;&quot; vcfsamplenames ${vcf[0]} | grep ${spec}${loc1} &gt; pop1.txt vcfsamplenames ${vcf[0]} | grep ${spec}${loc2} &gt; pop2.txt vcftools --gzvcf ${vcf[0]} \\ --weir-fst-pop pop1.txt \\ --weir-fst-pop pop2.txt \\ --out adapt_${spec}${loc1}-${spec}${loc2} 2&gt; adapt_${spec}${loc1}-${spec}${loc2}.log &quot;&quot;&quot; } At this step we are done with differentiation and GxP. "],
["git-4-analysis-ii-dxy-pi.html", "5 (git 4) Analysis II (dXY &amp; pi) 5.1 Summary 5.2 Details of analysis_dxy.nf", " 5 (git 4) Analysis II (dXY &amp; pi) 5.1 Summary The genetic divergence, as well diversity, are computed within the nextflow script analysis_dxy.nf (located under $BASE_DIR/nf/04_analysis_dxy/). It takes the all BP data set and computes dXY and \\(\\pi\\). Below is an overview of the steps involved in the analysis. (The green dot indicates the genotype input, red arrows depict output that is exported for further use.) 5.2 Details of analysis_dxy.nf 5.2.1 Data preparation The nextflow script starts by opening the genotype data and feeding it into three different streams (one for dXY and one for \\(\\pi\\)). #!/usr/bin/env nextflow // This pipeline includes the analysis run on the // all callable sites data sheet (dxy). // git 4.1 // load genotypes Channel .fromFilePairs(&quot;../../1_genotyping/3_gatk_filtered/filterd.allBP.vcf.{gz,gz.tbi}&quot;) .into{ vcf_ch; vcf_pi_ch } // git 4.2 // initialize LGs Channel .from( (&#39;01&#39;..&#39;09&#39;) + (&#39;10&#39;..&#39;19&#39;) + (&#39;20&#39;..&#39;24&#39;) ) .set{ lg_ch } // git 4.3 // split by LG and reformat the genotypes process split_allBP { label &#39;L_32g15h_split_allBP&#39; tag &quot;LG${lg}&quot; input: set val( lg ), vcfId, file( vcf ) from lg_ch.combine( vcf_ch ) output: set val( lg ), file( &#39;filterd.allBP.vcf.gz&#39; ), file( &quot;allBP.LG${lg}.geno.gz&quot; ) into geno_ch script: &quot;&quot;&quot; module load openssl1.0.2 vcftools --gzvcf ${vcf[0]} \\ --chr LG${lg} \\ --recode \\ --stdout | bgzip &gt; allBP.LG${lg}.vcf.gz python \\$SFTWR/genomics_general/VCF_processing/parseVCF.py \\ -i allBP.LG${lg}.vcf.gz | gzip &gt; allBP.LG${lg}.geno.gz &quot;&quot;&quot; } // git 4.4 // define location specific sepcies set Channel.from( [[1, &quot;ind&quot;], [2, &quot;may&quot;], [3, &quot;nig&quot;], [4, &quot;pue&quot;], [5, &quot;uni&quot;]] ).into{ bel_spec1_ch; bel_spec2_ch } Channel.from( [[1, &quot;abe&quot;], [2, &quot;gum&quot;], [3, &quot;nig&quot;], [4, &quot;pue&quot;], [5, &quot;ran&quot;], [6, &quot;uni&quot;]] ).into{ hon_spec1_ch; hon_spec2_ch } Channel.from( [[1, &quot;nig&quot;], [2, &quot;pue&quot;], [3, &quot;uni&quot;]] ).into{ pan_spec1_ch; pan_spec2_ch } // git 4.5 // init all sampled populations (for pi) Channel .from(&#39;indbel&#39;, &#39;maybel&#39;, &#39;nigbel&#39;, &#39;puebel&#39;, &#39;unibel&#39;, &#39;abehon&#39;, &#39;gumhon&#39;, &#39;nighon&#39;, &#39;puehon&#39;, &#39;ranhon&#39;, &#39;unihon&#39;, &#39;nigpan&#39;, &#39;puepan&#39;, &#39;unipan&#39;) .set{spec_dxy} // git 4.6 // init slining window resolutions Channel .from( 1, 5 ) .into{ kb_ch; kb_ch2; kb_ch3 } // git 4.7 // prepare pair wise dxy // ------------------------------ // create all possible species pairs depending on location // and combine with genotype subset (for the respective location) // ------------------------------ // channel content after joining: // set [0:val(loc), 1:file(vcf), 2:file(pop), 3:val(spec1), 4:val(spec2)] // ------------------------------ bel_pairs_ch = Channel.from( &quot;bel&quot; ) .combine( bel_spec1_ch ) .combine( bel_spec2_ch ) .filter{ it[1] &lt; it[3] } .map{ it[0,2,4]} hon_pairs_ch = Channel.from( &quot;hon&quot; ) .combine( hon_spec1_ch ) .combine(hon_spec2_ch) .filter{ it[1] &lt; it[3] } .map{ it[0,2,4]} pan_pairs_ch = Channel.from( &quot;pan&quot; ) .combine( pan_spec1_ch ) .combine(pan_spec2_ch) .filter{ it[1] &lt; it[3] } .map{ it[0,2,4]} // git 4.8 // combine species pair with genotypes (and window size) bel_pairs_ch .concat( hon_pairs_ch, pan_pairs_ch ) .combine( geno_ch ) .combine( kb_ch ) .into { all_dxy_pairs_ch; random_dxy_pairs_ch } // git 4.9 // compute the dxy values process dxy_lg { label &#39;L_G32g15h_dxy_lg&#39; tag &quot;${spec1}${loc}-${spec2}${loc}_LG${lg}&quot; input: set val( loc ), val( spec1 ), val( spec2 ), val( lg ), file( vcf ), file( geno ), val( kb ) from all_dxy_pairs_ch output: set val( &quot;${spec1}${loc}-${spec2}${loc}-${kb}&quot; ), file( &quot;dxy.${spec1}${loc}-${spec2}${loc}.LG${lg}.${kb}0kb-${kb}kb.txt.gz&quot; ), val( lg ), val( &quot;${spec1}${loc}&quot; ), val( &quot;${spec2}${loc}&quot; ), val( kb ) into dxy_lg_ch script: &quot;&quot;&quot; module load openssl1.0.2 module load intel17.0.4 intelmpi17.0.4 zcat ${geno} | \\ head -n 1 | \\ cut -f 3- | \\ sed &#39;s/\\\\t/\\\\n/g&#39; | \\ awk -v OFS=&#39;\\\\t&#39; &#39;{print \\$1, substr( \\$1, length(\\$1) - 5, 6)}&#39; &gt; pop.txt mpirun \\$NQSII_MPIOPTS -np 1 \\ python \\$SFTWR/genomics_general/popgenWindows.py \\ -w ${kb}0000 -s ${kb}000 \\ --popsFile pop.txt \\ -p ${spec1}${loc} -p ${spec2}${loc} \\ -g ${geno} \\ -o dxy.${spec1}${loc}-${spec2}${loc}.LG${lg}.${kb}0kb-${kb}kb.txt.gz \\ -f phased \\ --writeFailedWindows \\ -T 1 &quot;&quot;&quot; } // git 4.10 // collect all LGs for each species pair dxy_lg_ch .groupTuple() .set{ tubbled_dxy } // git 4.11 // concatenate all LGs for each species pair process receive_tuple { label &#39;L_20g2h_receive_tuple&#39; publishDir &quot;../../2_analysis/dxy/${kb[0]}0k/&quot;, mode: &#39;copy&#39; tag &quot;${pop1[0]}-${pop2[0]}&quot; input: set val( comp ), file( dxy ), val( lg ), val( pop1 ), val( pop2 ), val( kb ) from tubbled_dxy output: file( &quot;dxy.${pop1[0]}-${pop2[0]}.${kb[0]}0kb-${kb[0]}kb.tsv.gz&quot; ) into dxy_output_ch script: &quot;&quot;&quot; zcat dxy.${pop1[0]}-${pop2[0]}.LG01.${kb[0]}0kb-${kb[0]}kb.txt.gz | \\ head -n 1 &gt; dxy.${pop1[0]}-${pop2[0]}.${kb[0]}0kb-${kb[0]}kb.tsv; for j in {01..24};do echo &quot;-&gt; LG\\$j&quot; zcat dxy.${pop1[0]}-${pop2[0]}.LG\\$j.${kb[0]}0kb-${kb[0]}kb.txt.gz | \\ awk &#39;NR&gt;1{print}&#39; &gt;&gt; dxy.${pop1[0]}-${pop2[0]}.${kb[0]}0kb-${kb[0]}kb.tsv; done gzip dxy.${pop1[0]}-${pop2[0]}.${kb[0]}0kb-${kb[0]}kb.tsv &quot;&quot;&quot; } // git 4.12 // collect a species pair to randomize Channel .from( [[&#39;bel&#39;, &#39;ind&#39;, &#39;may&#39;]] ) .set{ random_run_ch } // git 4.13 // setup channel content for random channel Channel .from( 1 ) .combine( random_run_ch ) .combine( kb_ch2 ) .filter{ it[4] == 5 } .set{ random_sets_ch } // git 4.14 // permute the population assignment (the randomization) process randomize_samples { label &#39;L_20g15h_randomize_samples&#39; publishDir &quot;../../2_analysis/fst/${kb}0k/random&quot;, mode: &#39;copy&#39; , pattern: &quot;*_windowed.weir.fst.gz&quot; module &quot;R3.5.2&quot; input: set val( random_set ), val( loc ), val(spec1), val(spec2), val( kb ) from random_sets_ch output: set random_set, file( &quot;random_pop.txt&quot; ) into random_pops_ch file( &quot;*_windowed.weir.fst.gz&quot;) into random_fst_out script: &quot;&quot;&quot; cut -f 2,3 \\$BASE_DIR/metadata/sample_info.txt | \\ grep &quot;${loc}&quot; | \\ grep &quot;${spec1}\\\\|${spec2}&quot; &gt; pop_prep.tsv Rscript --vanilla \\$BASE_DIR/R/randomize_pops.R grep A random_pop.txt | cut -f 1 &gt; pop1.txt grep B random_pop.txt | cut -f 1 &gt; pop2.txt vcftools \\ --gzvcf \\$BASE_DIR/1_genotyping/3_gatk_filtered/filterd_bi-allelic.allBP.vcf.gz \\ --weir-fst-pop pop1.txt \\ --weir-fst-pop pop2.txt \\ --fst-window-step ${kb}0000 \\ --fst-window-size ${kb}0000 \\ --stdout | gzip &gt; ${loc}-aaa-bbb.${kb}0k.random_${spec1}_${spec2}_windowed.weir.fst.gz &quot;&quot;&quot; } // git 4.15 // pick random pair of interest random_dxy_pairs_ch .filter{ it[0] == &#39;bel&#39; &amp;&amp; it[1] == &#39;ind&#39; &amp;&amp; it[2] == &#39;may&#39; &amp;&amp; it[6] == 5 } .combine( random_pops_ch ) .set{ random_assigned_ch } // git 4.16 // compute the dxy values process dxy_lg_random { label &#39;L_G32g15h_dxy_lg_random&#39; tag &quot;aaa${loc}-bbb${loc}_LG${lg}&quot; module &quot;R3.5.2&quot; input: set val( loc ), val( spec1 ), val( spec2 ), val( lg ), file( vcf ), file( geno ), val( kb ), val( random_set ), file( pop_file ) from random_assigned_ch output: set val( &quot;aaa${loc}-bbb${loc}-${kb}0kb&quot; ), file( &quot;dxy.aaa${loc}-bbb${loc}.LG${lg}.${kb}0kb-${kb}kb.txt.gz&quot; ), val( lg ), val( &quot;aaa${loc}&quot; ), val( &quot;bbb${loc}&quot; ), val( kb ) into dxy_random_lg_ch script: &quot;&quot;&quot; module load openssl1.0.2 module load intel17.0.4 intelmpi17.0.4 mpirun \\$NQSII_MPIOPTS -np 1 \\ python \\$SFTWR/genomics_general/popgenWindows.py \\ -w ${kb}0000 -s ${kb}000 \\ --popsFile ${pop_file} \\ -p A -p B \\ -g ${geno} \\ -o dxy.aaa${loc}-bbb${loc}.LG${lg}.${kb}0kb-${kb}kb.txt.gz \\ -f phased \\ --writeFailedWindows \\ -T 1 &quot;&quot;&quot; } // git 4.17 // collect all LGs of random run dxy_random_lg_ch .groupTuple() .set{ tubbled_random_dxy } // git 4.18 // concatinate all LGs of random run process receive_random_tuple { label &#39;L_20g2h_receive_random_tuple&#39; publishDir &quot;../../2_analysis/dxy/random/&quot;, mode: &#39;copy&#39; input: set val( comp ), file( dxy ), val( lg ), val( pop1 ), val( pop2 ), val( kb ) from tubbled_random_dxy output: file( &quot;dxy.${pop1[0]}-${pop2[0]}.${kb[0]}0kb-${kb[0]}kb.tsv.gz&quot; ) into dxy_random_output_ch script: &quot;&quot;&quot; zcat dxy.${pop1[0]}-${pop2[0]}.LG01.${kb[0]}0kb-${kb[0]}kb.txt.gz | \\ head -n 1 &gt; dxy.${pop1[0]}-${pop2[0]}.${kb[0]}0kb-${kb[0]}kb.tsv; for j in {01..24};do echo &quot;-&gt; LG\\$j&quot; zcat dxy.${pop1[0]}-${pop2[0]}.LG\\$j.${kb[0]}0kb-${kb[0]}kb.txt.gz | \\ awk &#39;NR&gt;1{print}&#39; &gt;&gt; dxy.${pop1[0]}-${pop2[0]}.${kb[0]}0kb-${kb[0]}kb.tsv; done gzip dxy.${pop1[0]}-${pop2[0]}.${kb[0]}0kb-${kb[0]}kb.tsv &quot;&quot;&quot; } // --------------------------------------------------------------- // The pi part need to be run AFTER the global fst outlier // windows were selected (REMEMBER TO CHECK FST OUTLIER DIRECTORY) // --------------------------------------------------------------- // git 4.19 // calculate pi per species process pi_per_spec { label &#39;L_32g15h_pi&#39; tag &quot;${spec}&quot; publishDir &quot;../../2_analysis/pi/${kb}0k&quot;, mode: &#39;copy&#39; input: set val( spec ), vcfId, file( vcf ), val( kb ) from spec_dxy.combine( vcf_pi_ch ).combine( kb_ch3 ) output: file( &quot;*.${kb}0k.windowed.pi.gz&quot; ) into pi_50k script: &quot;&quot;&quot; module load openssl1.0.2 vcfsamplenames ${vcf[0]} | \\ grep ${spec} &gt; pop.txt vcftools --gzvcf ${vcf[0]} \\ --keep pop.txt \\ --window-pi ${kb}0000 \\ --window-pi-step ${kb}000 \\ --out ${spec}.${kb}0k 2&gt; ${spec}.pi.log gzip ${spec}.${kb}0k.windowed.pi tail -n +2 \\$BASE_DIR/2_analysis/summaries/fst_outliers_998.tsv | \\ cut -f 2,3,4 &gt; outlier.bed vcftools --gzvcf ${vcf[0]} \\ --keep pop.txt \\ --exclude-bed outlier.bed \\ --window-pi ${kb}0000 \\ --window-pi-step ${kb}000\\ --out ${spec}_no_outlier.${kb}0k 2&gt; ${spec}_${kb}0k_no_outllier.pi.log gzip ${spec}_no_outlier.${kb}0k.windowed.pi &quot;&quot;&quot; } At this step we are done with divergence and diversity. "],
["git-5-analysis-iii-phylogeny-topology-weighting.html", "6 (git 5) Analysis III (phylogeny &amp; topology weighting) 6.1 Summary 6.2 Details of analysis_fasttree_twisst.nf", " 6 (git 5) Analysis III (phylogeny &amp; topology weighting) 6.1 Summary The whole genome phylogeny and the topology weighting are prepared within the nextflow script analysis_fasttree_twisst.nf (located under $BASE_DIR/nf/05_analysis_fasttree_twisst/), which runs on the SNPs only data set. Below is an overview of the steps involved in the process. (The green dot indicates the genotype input, red arrows depict output that is exported for further use.) 6.2 Details of analysis_fasttree_twisst.nf 6.2.1 Setup The nextflow script starts by opening the genotype data and feeding it into two different streams (one for the phylogeny and one for topology weighting). #!/usr/bin/env nextflow // git 5.1 // open genotype data Channel .fromFilePairs(&quot;../../1_genotyping/4_phased/phased_mac2.vcf.{gz,gz.tbi}&quot;) .into{ vcf_fasttree_whg; vcf_locations } // git 5.2 // setting the sampling location // (the script is set up to run on diffferent subsets of samples) Channel .from( &quot;all&quot; ) //, &quot;bel&quot;, &quot;hon&quot;, &quot;pan&quot; ) .set{ locations4_ch } // git 5.3 // setting loci restrictions // (keep vs remove outlier regions) Channel .from( &quot;whg&quot; ) //, &quot;no_musks&quot; ) .set{ whg_modes } // git 5.4 // setting the sampling mode // (the script is set up to run on diffferent subsets of samples) Channel .from( &quot;no_outgroups&quot; ) //, &quot;all&quot; ) .into{ sample_modes } // git 5.5 // compile the config settings and add data file locations4_ch .combine( vcf_fasttree_whg ) .combine( whg_modes ) .combine( sample_modes ) .set{ vcf_fasttree_whg_location_combo } // git 5.6 // apply sample filter, subset and convert genotypes process subset_vcf_by_location_whg { label &quot;L_28g5h_subset_vcf_whg&quot; input: set val( loc ), vcfId, file( vcf ), val( mode ), val( sample_mode ) from vcf_fasttree_whg_location_combo output: set val( mode ), val( loc ), val( sample_mode ), file( &quot;${loc}.${mode}.${sample_mode}.whg.geno.gz&quot; ) into snp_geno_tree_whg script: &quot;&quot;&quot; DROP_CHRS=&quot; &quot; # check if samples need to be dropped based on location if [ &quot;${loc}&quot; == &quot;all&quot; ];then vcfsamplenames ${vcf[0]} &gt; prep.pop else vcfsamplenames ${vcf[0]} | \\ grep ${loc} &gt; prep.pop fi # check if outgroups need to be dropped if [ &quot;${sample_mode}&quot; == &quot;all&quot; ];then mv prep.pop ${loc}.pop else cat prep.pop | \\ grep -v tor | \\ grep -v tab &gt; ${loc}.pop fi # check if diverged LGs need to be dropped if [ &quot;${mode}&quot; == &quot;no_musks&quot; ];then DROP_CHRS=&quot;--not-chr LG04 --not-chr LG07 --not-chr LG08 --not-chr LG09 --not-chr LG12 --not-chr LG17 --not-chr LG23&quot; fi vcftools --gzvcf ${vcf[0]} \\ --keep ${loc}.pop \\ \\$DROP_CHRS \\ --mac 3 \\ --recode \\ --stdout | gzip &gt; ${loc}.${mode}.${sample_mode}.vcf.gz python \\$SFTWR/genomics_general/VCF_processing/parseVCF.py \\ -i ${loc}.${mode}.${sample_mode}.vcf.gz | gzip &gt; ${loc}.${mode}.${sample_mode}.whg.geno.gz &quot;&quot;&quot; } // git 5.7 // convert genotypes to fasta process fasttree_whg_prep { label &#39;L_190g4h_fasttree_whg_prep&#39; tag &quot;${mode} - ${loc} - ${sample_mode}&quot; input: set val( mode ), val( loc ), val( sample_mode ), file( geno ) from snp_geno_tree_whg output: set val( mode ), val( loc ), val( sample_mode ), file( &quot;all_samples.${loc}.${mode}.${sample_mode}.whg.SNP.fa&quot; ) into ( fasttree_whg_prep_ch ) script: &quot;&quot;&quot; python \\$SFTWR/genomics_general/genoToSeq.py -g ${geno} \\ -s all_samples.${loc}.${mode}.${sample_mode}.whg.SNP.fa \\ -f fasta \\ --splitPhased &quot;&quot;&quot; } // git 5.8 // create phylogeny process fasttree_whg_run { label &#39;L_300g30h_fasttree_run&#39; tag &quot;${mode} - ${loc} - ${sample_mode}&quot; publishDir &quot;../../2_analysis/fasttree/&quot;, mode: &#39;copy&#39; input: set val( mode ), val( loc ), val( sample_mode ), file( fa ) from fasttree_whg_prep_ch output: file( &quot;${sample_mode}.${loc}.${mode}.SNP.tree&quot; ) into ( fasttree_whg_output ) script: &quot;&quot;&quot; fasttree -nt ${fa} &gt; ${sample_mode}.${loc}.${mode}.SNP.tree &quot;&quot;&quot; } // git 5.9 // initialize the locations for topology weighting Channel .from( &quot;bel&quot;, &quot;hon&quot; ) .set{ locations_ch } // git 5.10 locations_ch .combine( vcf_locations ) .set{ vcf_location_combo } // git 5.11 // initialize LGs Channel .from( (&#39;01&#39;..&#39;09&#39;) + (&#39;10&#39;..&#39;19&#39;) + (&#39;20&#39;..&#39;24&#39;) ) .map{ &quot;LG&quot; + it } .set{ lg_twisst } // git 5.12 // subset the genotypes by location process subset_vcf_by_location { label &quot;L_20g2h_subset_vcf&quot; input: set val( loc ), vcfId, file( vcf ) from vcf_location_combo output: set val( loc ), file( &quot;${loc}.vcf.gz&quot; ), file( &quot;${loc}.pop&quot; ) into ( vcf_loc_twisst ) script: &quot;&quot;&quot; vcfsamplenames ${vcf[0]} | \\ grep ${loc} | \\ grep -v tor | \\ grep -v tab &gt; ${loc}.pop vcftools --gzvcf ${vcf[0]} \\ --keep ${loc}.pop \\ --mac 3 \\ --recode \\ --stdout | gzip &gt; ${loc}.vcf.gz &quot;&quot;&quot; } // --------------------------------------------------------------- // Unfortunately the twisst preparation did not work on the cluster // (&#39;in place&#39;), so I had to setup the files locally and then plug // them into this workflow. // Below is the originally intended clean workflow (commented out), // while the plugin version picks up at git 5.19. // --------------------------------------------------------------- /* MUTE: // git 5.13 // add the lg channel to the genotype subset vcf_loc_twisst .combine( lg_twisst ) .set{ vcf_loc_lg_twisst } */ /* MUTE: python thread conflict - run locally and feed into ressources/plugin // git 5.14 // subset genotypes by LG process vcf2geno_loc { label &#39;L_20g15h_vcf2geno&#39; input: set val( loc ), file( vcf ), file( pop ), val( lg ) from vcf_loc_lg_twisst output: set val( loc ), val( lg ), file( &quot;${loc}.${lg}.geno.gz&quot; ), file( pop ) into snp_geno_twisst script: &quot;&quot;&quot; vcftools \\ --gzvcf ${vcf} \\ --chr ${lg} \\ --recode \\ --stdout | gzip &gt; intermediate.vcf.gz python \\$SFTWR/genomics_general/VCF_processing/parseVCF.py \\ -i intermediate.vcf.gz | gzip &gt; ${loc}.${lg}.geno.gz &quot;&quot;&quot; } */ /* MUTE: python thread conflict - run locally and feed into ressources/plugin // git 5.15 // initialize SNP window size Channel.from( 50, 200 ).set{ twisst_window_types } */ /* MUTE: python thread conflict - run locally and feed into ressources/plugin // git 5.16 // add the SNP window size to the genotype subset snp_geno_twisst.combine( twisst_window_types ).set{ twisst_input_ch } */ /* MUTE: python thread conflict - run locally and feed into ressources/plugin // git 5.17 // create the phylogenies along the sliding window process twisst_prep { label &#39;L_G120g40h_prep_twisst&#39; publishDir &quot;../../2_analysis/twisst/positions/${loc}/&quot;, mode: &#39;copy&#39; input: set val( loc ), val( lg ), file( geno ), file( pop ), val( twisst_w ) from twisst_input_ch.filter { it[0] != &#39;pan&#39; } output: set val( loc ), val( lg ), file( geno ), file( pop ), val( twisst_w ), file( &quot;*.trees.gz&quot; ), file( &quot;*.data.tsv&quot; ) into twisst_prep_ch script: &quot;&quot;&quot; module load intel17.0.4 intelmpi17.0.4 mpirun \\$NQSII_MPIOPTS -np 1 \\ python \\$SFTWR/genomics_general/phylo/phyml_sliding_windows.py \\ -g ${geno} \\ --windType sites \\ -w ${twisst_w} \\ --prefix ${loc}.${lg}.w${twisst_w}.phyml_bionj \\ --model HKY85 \\ --optimise n \\ --threads 1 &quot;&quot;&quot; } */ /* MUTE: python thread conflict - run locally and feed into ressources/plugin // git 5.18 // run the topology weighting on the phylogenies process twisst_run { label &#39;L_G120g40h_run_twisst&#39; publishDir &quot;../../2_analysis/twisst/weights/&quot;, mode: &#39;copy&#39; input: set val( loc ), val( lg ), file( geno ), file( pop ), val( twisst_w ), file( tree ), file( data ) from twisst_prep_ch output: set val( loc ), val( lg ), val( twisst_w ), file( &quot;*.weights.tsv.gz&quot; ), file( &quot;*.data.tsv&quot; ) into ( twisst_output ) script: &quot;&quot;&quot; module load intel17.0.4 intelmpi17.0.4 awk &#39;{print \\$1&quot;\\\\t&quot;\\$1}&#39; ${pop} | \\ sed &#39;s/\\\\(...\\\\)\\\\(...\\\\)\\$/\\\\t\\\\1\\\\t\\\\2/g&#39; | \\ cut -f 1,3 | \\ awk &#39;{print \\$1&quot;_A\\\\t&quot;\\$2&quot;\\\\n&quot;\\$1&quot;_B\\\\t&quot;\\$2}&#39; &gt; ${loc}.${lg}.twisst_pop.txt TWISST_POPS=\\$( cut -f 2 ${loc}.${lg}.twisst_pop.txt | sort | uniq | paste -s -d&#39;,&#39; | sed &#39;s/,/ -g /g; s/^/-g /&#39; ) mpirun \\$NQSII_MPIOPTS -np 1 \\ python \\$SFTWR/twisst/twisst.py \\ --method complete \\ -t ${tree} \\ -T 1 \\ \\$TWISST_POPS \\ --groupsFile ${loc}.${lg}.twisst_pop.txt | \\ gzip &gt; ${loc}.${lg}.w${twisst_w}.phyml_bionj.weights.tsv.gz &quot;&quot;&quot; } */ // git 5.19 // emmulate setting Channel .from(50, 200) .combine( vcf_loc_twisst ) .combine( lg_twisst ) .set{ twisst_modes } // git 5.20 // run the topology weighting on the phylogenies process twisst_plugin { label &#39;L_G120g40h_twisst_plugin&#39; publishDir &quot;../../2_analysis/twisst/weights/&quot;, mode: &#39;copy&#39; tag &quot;${loc}-${lg}-${mode}&quot; input: set val( mode ), val( loc ), file( vcf ), file( pop ), val( lg ) from twisst_modes output: set val( loc ), val( lg ), file( &quot;*.weights.tsv.gz&quot; ) into ( twisst_output ) script: &quot;&quot;&quot; module load intel17.0.4 intelmpi17.0.4 awk &#39;{print \\$1&quot;\\\\t&quot;\\$1}&#39; ${pop} | \\ sed &#39;s/\\\\(...\\\\)\\\\(...\\\\)\\$/\\\\t\\\\1\\\\t\\\\2/g&#39; | \\ cut -f 1,3 | \\ awk &#39;{print \\$1&quot;_A\\\\t&quot;\\$2&quot;\\\\n&quot;\\$1&quot;_B\\\\t&quot;\\$2}&#39; &gt; ${loc}.${lg}.twisst_pop.txt TWISST_POPS=\\$( cut -f 2 ${loc}.${lg}.twisst_pop.txt | sort | uniq | paste -s -d&#39;,&#39; | sed &#39;s/,/ -g /g; s/^/-g /&#39; ) mpirun \\$NQSII_MPIOPTS -np 1 \\ python \\$SFTWR/twisst/twisst.py \\ --method complete \\ -t \\$BASE_DIR/ressources/plugin/trees/${loc}/${loc}.${lg}.w${mode}.phyml_bionj.trees.gz \\ \\$TWISST_POPS \\ --groupsFile ${loc}.${lg}.twisst_pop.txt | \\ gzip &gt; ${loc}.${lg}.w${mode}.phyml_bionj.weights.tsv.gz &quot;&quot;&quot; } At this step we are done with phylogeny and the topology weighting. "],
["git-6-analysis-iv-rho.html", "7 (git 6) Analysis IV (rho) 7.1 Summary 7.2 Details of analysis_recombination.nf", " 7 (git 6) Analysis IV (rho) 7.1 Summary The population recombination rate is estimated within the nextflow script analysis_recombination.nf (located under $BASE_DIR/nf/06_analysis_recombination/), which runs on the SNPs only data set. Below is an overview of the steps involved in the analysis. (The green dot indicates the genotype input, red arrows depict output that is exported for further use.) 7.2 Details of analysis_recombination.nf 7.2.1 Data preparation The nextflow script starts by opening the genotype data. #!/usr/bin/env nextflow // This pipeline includes the recombination anlysis // git 6.1 // load genotypes Channel .fromFilePairs(&quot;../../1_genotyping/4_phased/phased_mac2.vcf.{gz,gz.tbi}&quot;) .set{ vcf_ch } // git 6.2 // initialize LGs Channel .from( 1..24 ) .map{ it.toString().padLeft(2, &quot;0&quot;) } .set{ lg_ch } // git 6.3 // split genotypes by LG process split_allBP { label &#39;L_20g2h_split_by_lg&#39; tag &quot;LG${lg}&quot; input: set val( lg ), vcfId, file( vcf ) from lg_ch.combine( vcf_ch ) output: set val( lg ), file( &quot;phased_mac2.LG${lg}.vcf.gz&quot; ) into vcf_by_lg_ch script: &quot;&quot;&quot; module load openssl1.0.2 vcftools --gzvcf ${vcf[0]} \\ --chr LG${lg} \\ --recode \\ --stdout | bgzip &gt; phased_mac2.LG${lg}.vcf.gz &quot;&quot;&quot; } // git 6.4 // run fasteprr step 1 process fasteprr_s1 { label &#39;L_20g2h_fasteprr_s1&#39; tag &quot;LG${lg}&quot; module &quot;R3.5.2&quot; input: set val( lg ), file( vcf ) from vcf_by_lg_ch output: file( &quot;step1_LG${lg}&quot; ) into step_1_out_ch script: &quot;&quot;&quot; mkdir step1_LG${lg} Rscript --vanilla \\$BASE_DIR/R/fasteprr_step1.R ./${vcf} step1_LG${lg} LG${lg} 50 &quot;&quot;&quot; } // git 6.5 // collect step 1 output process fasteprr_s1_summary { label &#39;L_loc_fasteprr_s1_summmary&#39; input: file( step1 ) from step_1_out_ch.collect() output: file( &quot;step1&quot; ) into ( step1_ch1, step1_ch2 ) script: &quot;&quot;&quot; mkdir step1 cp step1_LG*/* step1/ &quot;&quot;&quot; } // git 6.6 // initialize fasteperr subprocesses and attach them to step 1 output Channel .from( 1..250 ) .map{ it.toString().padLeft(3, &quot;0&quot;) } .combine( step1_ch1 ) .set{ step_2_run_ch } // git 6.7 // run fasteprr step 2 process fasteprr_s2 { label &#39;L_long_loc_fasteprr_s2&#39; tag &quot;run_${idx}&quot; module &quot;R3.5.2&quot; input: set val( idx ), file( step1 ) from step_2_run_ch output: set val( idx ), file( &quot;step2_run${idx}&quot; ) into step_2_out_ch script: &quot;&quot;&quot; mkdir -p step2_run${idx} Rscript --vanilla \\$BASE_DIR/R/fasteprr_step2.R ${step1} step2_run${idx} ${idx} &quot;&quot;&quot; } // git 6.8 // clone step 2 output step_2_out_ch.into{ step_2_indxs; step_2_files } // git 6.9 // collect step 2 output process fasteprr_s2_summary { label &#39;L_loc_fasteprr_s2_summmary&#39; input: val( idx ) from step_2_indxs.map{ it[0] }.collect() file( files ) from step_2_files.map{ it[1] }.collect() output: file( &quot;step2&quot; ) into ( step2_ch ) script: &quot;&quot;&quot; mkdir step2 for k in \\$( echo ${idx} | sed &#39;s/\\\\[//g; s/\\\\]//g; s/,//g&#39;); do cp -r step2_run\\$k/* step2/ done &quot;&quot;&quot; } // git 6.10 // run fasteprr step 3 process fasteprr_s3 { label &#39;L_32g4h_fasteprr_s3&#39; module &quot;R3.5.2&quot; input: set file( step1 ), file( step2 ) from step1_ch2.combine( step2_ch ) output: file( &quot;step3&quot; ) into step_3_out_ch script: &quot;&quot;&quot; mkdir step3 Rscript --vanilla \\$BASE_DIR/R/fasteprr_step3.R ${step1} ${step2} step3 &quot;&quot;&quot; } // git 6.11 // reformat overall fasteprr output process fasteprr_s3_summary { label &#39;L_loc_fasteprr_s3_summmary&#39; publishDir &quot;../../2_analysis/fasteprr&quot;, mode: &#39;copy&#39; input: file( step3 ) from step_3_out_ch output: file( &quot;step4/fasteprr.all.rho.txt.gz&quot; ) into ( step3_ch ) script: &quot;&quot;&quot; mkdir step4 # ------ rewriting the fasteprr output into tidy format -------- for k in {01..24};do j=&quot;LG&quot;\\$k; echo \\$j; \\$BASE_DIR/sh/fasteprr_trans.sh step3/chr_\\$j \\$j step4/fasteprr.\\$j done # --------- combining all LGs into a single data set ----------- cd step4 head -n 1 fasteprr.LG01.rho.txt &gt; fasteprr.all.rho.txt for k in {01..24}; do echo &quot;LG&quot;\\$k awk &#39;NR&gt;1{print \\$0}&#39; fasteprr.LG\\$k.rho.txt &gt;&gt; fasteprr.all.rho.txt done gzip fasteprr.all.rho.txt cd .. &quot;&quot;&quot; } Finally, we are done with recombination rate. "],
["git-7-analysis-v-poptrees.html", "8 (git 7) Analysis V (poptrees) 8.1 Summary 8.2 Details of analysis_xx.nf", " 8 (git 7) Analysis V (poptrees) 8.1 Summary The population recombination rate is estimated within the nextflow script analysis_XX.nf (located under $BASE_DIR/nf/0x_analysis_xx/), which runs on the XX data set. Below is an overview of the steps involved in the analysis. (The green dot indicates the genotype input, red arrows depict output that is exported for further use.) 8.2 Details of analysis_xx.nf 8.2.1 Data preparation The nextflow script starts by opening the genotype data. #!/usr/bin/env nextflow // git 7.1 // open genotype data Channel .fromFilePairs(&quot;../../1_genotyping/4_phased/phased_mac2.vcf.{gz,gz.tbi}&quot;) .set{ vcf_fst } // git 7.2 // load all possible population pairs Channel .fromPath(&quot;../../ressources/plugin/poptrees/all_crosses.tsv&quot;) .splitCsv(header:true, sep:&quot;\\t&quot;) .map{ row -&gt; [ pop1:row.pop1, pop2:row.pop2 ] } .set{ crosses_ch } // git 7.3 // open the focal Fst outlier regions Channel .fromPath(&quot;../../ressources/plugin/poptrees/outlier.bed&quot;) .splitCsv(header:true, sep:&quot;\\t&quot;) .map{ row -&gt; [ chrom:row.chrom, start:row.start, end:row.end, gid:row.gid ] } .combine( vcf_fst ) .combine( crosses_ch ) .set{ crosses_vcf } // git 7.4 // compute the average Fst for all possible pair within the outlier region process outlier_fst { label &quot;L_loc_collect_fst&quot; publishDir &quot;../../2_analysis/fst/poptree/single&quot;, mode: &#39;copy&#39; input: set val( grouping ), val( vcfidx ), file( vcf ), val( cross_pop ) from crosses_vcf output: set val( grouping.gid ), val( cross_pop ), file( &quot;*.fst.tsv&quot; ) into outlier_fst_gid_ch script: &quot;&quot;&quot; echo -e &quot;CHROM\\\\tSTART\\\\tEND&quot; &gt; outl.bed echo -e &quot;${grouping.chrom}\\\\t${grouping.start}\\\\t${grouping.end}&quot; &gt;&gt; outl.bed vcfsamplenames ${vcf[0]} | \\ grep &quot;${cross_pop.pop1}&quot; &gt; pop1.pop vcfsamplenames ${vcf[0]} | \\ grep &quot;${cross_pop.pop2}&quot; &gt; pop2.pop vcftools --gzvcf ${vcf[0]} \\ --bed outl.bed \\ --keep pop1.pop \\ --keep pop2.pop \\ --weir-fst-pop pop1.pop \\ --weir-fst-pop pop2.pop \\ --stdout 2&gt; ${cross_pop.pop1}-${cross_pop.pop2}.50k.log | \\ gzip &gt; ${cross_pop.pop1}-${cross_pop.pop2}.fst.tsv.gz mFST=\\$(grep &quot;Weir and Cockerham mean Fst estimate:&quot; ${cross_pop.pop1}-${cross_pop.pop2}.50k.log | sed &#39;s/Weir and Cockerham mean Fst estimate: //&#39;) wFST=\\$(grep &quot;Weir and Cockerham weighted Fst estimate:&quot; ${cross_pop.pop1}-${cross_pop.pop2}.50k.log | sed &#39;s/Weir and Cockerham weighted Fst estimate: //&#39;) echo -e &quot;${cross_pop.pop1}-${cross_pop.pop2}\\\\t\\$mFST\\\\t\\$wFST&quot; &gt; ${cross_pop.pop1}-${cross_pop.pop2}.${grouping.gid}.fst.tsv &quot;&quot;&quot; } // git 7.5 // collect all population pairs within each region and compile Fst table process outlier_fst_collect { label &quot;L_20g2h_outlier_fst&quot; publishDir &quot;../../2_analysis/fst/poptree/summary&quot;, mode: &#39;copy&#39; input: set val( gid ), val( cross_pop ), file( fst ) from outlier_fst_gid_ch.groupTuple() output: file( &quot;${gid}.fst.all.tsv&quot; ) into outlier_fst_collect_ch script: &quot;&quot;&quot; echo -e &quot;run\\\\tmean_fst\\\\tweighted_fst&quot; &gt; ${gid}.fst.all.tsv cat *.fst.tsv &gt;&gt; ${gid}.fst.all.tsv &quot;&quot;&quot; } // git 7.6 // neighbour joining happens within the R script R/fig/plot_F4.R Finally, we are done with XX. "],
["git-8-analysis-vi-demographic-history.html", "9 (git 8) Analysis VI (Demographic History) 9.1 Summary 9.2 Details of analysis_msmc.nf", " 9 (git 8) Analysis VI (Demographic History) 9.1 Summary The demographic history rate is inferred within the nextflow script analysis_msmc.nf (located under $BASE_DIR/nf/08_analysis_msmc/), which runs on the SNPs only data set. Below is an overview of the steps involved in the inference. (The green dot indicates the genotype input, red arrows depict output that is exported for further use.) 9.2 Details of analysis_msmc.nf 9.2.1 Data preparation The first part of the scripts includes a large block of preparation work. In this initial block, the data masks are being generated based on the samples coverage statistics combined with the locations of idels and the reference genomes mappability. The whole script is opened by a creating a channel for the linkage groups since the coverage statistics are being created on a linkage group basis. #!/usr/bin/env nextflow // git 8.1 // create channel of linkage groups Channel .from( (&#39;01&#39;..&#39;09&#39;) + (&#39;10&#39;..&#39;19&#39;) + (&#39;20&#39;..&#39;24&#39;) ) .map{ &quot;LG&quot; + it } .into{ lg_ch1; lg_ch2; lg_ch3 } // git 8.2 // open phased genotype data Channel .fromFilePairs(&quot;../../1_genotyping/4_phased/phased_mac2.vcf.{gz,gz.tbi}&quot;) .set{ vcf_msmc } // git 8.3 // open unphased genotype data to extract depth information Channel .fromFilePairs(&quot;../../1_genotyping/3_gatk_filtered/filterd_bi-allelic.vcf.{gz,gz.tbi}&quot;) .set{ vcf_depth } // git 8.4 // gather depth per individual process gather_depth { label &#39;L_20g2h_split_by_sample&#39; publishDir &quot;../../metadata&quot;, mode: &#39;copy&#39; input: set vcfID, file( vcf ) from vcf_depth output: file( &quot;depth_by_sample.txt&quot; ) into depth_ch script: &quot;&quot;&quot; vcfsamplenames ${vcf[0]} | \\ grep -v &quot;tor\\\\|tab\\\\|flo&quot; &gt; pop.txt vcftools \\ --gzvcf ${vcf[0]} \\ --keep pop.txt \\ --depth \\ --stdout &gt; depth_by_sample.txt &quot;&quot;&quot; } // git 8.5 // create channel out of sequencing depth table depth_ch .splitCsv(header:true, sep:&quot;\\t&quot;) .map{ row -&gt; [ id:row.INDV, sites:row.N_SITES, depth:row.MEAN_DEPTH] } .map{ [it.id, it.sites, it.depth] } .set { depth_by_sample_ch } // git 8.6 // create channel from bam files and add sample id Channel .fromPath( &#39;../../1_genotyping/0_dedup_bams/*.bam&#39; ) .map{ file -&gt; def key = file.name.toString().tokenize(&#39;.&#39;).get(0) return tuple(key, file)} .set{ sample_bams } // git 8.7 // combine sample bams and sequencing depth sample_bams .join( depth_by_sample_ch ) .set{ sample_bam_and_depth } // git 8.8 // multiply the sample channel by the linkage groups sample_bam_and_depth .combine( vcf_msmc ) .combine( lg_ch1 ) .set{ samples_msmc } // git 8.9 // split vcf by individual process split_vcf_by_individual { label &#39;L_20g15m_split_by_vcf&#39; input: set val( id ), file( bam ), val( sites ), val( depth ), val( vcf_id ), file( vcf ), val( lg ) from samples_msmc output: set val( id ), val( lg ), file( bam ), val( depth ), file( &quot;phased_mac2.${id}.${lg}.vcf.gz&quot; ) into ( sample_vcf, sample_vcf2 ) script: &quot;&quot;&quot; gatk --java-options &quot;-Xmx10G&quot; \\ SelectVariants \\ -R \\$REF_GENOME \\ -V ${vcf[0]} \\ -sn ${id} \\ -L ${lg}\\ -O phased_mac2.${id}.${lg}.vcf.gz &quot;&quot;&quot; } // git 8.10 // create coverage mask from original mapped sequences process bam_caller { label &#39;L_36g47h_bam_caller&#39; publishDir &quot;../../ressources/coverage_masks&quot;, mode: &#39;copy&#39; , pattern: &quot;*.coverage_mask.bed.gz&quot; conda &quot;$HOME/miniconda2/envs/py3&quot; input: set val( id ), val( lg ), file( bam ), val( depth ), file( vcf ) from sample_vcf output: set val( id ), val( lg ), file( &quot;*.bam_caller.vcf.gz&quot; ), file( &quot;*.coverage_mask.bed.gz&quot; ) into coverage_by_sample_lg script: &quot;&quot;&quot; module load openssl1.0.2 samtools index ${bam} samtools mpileup -q 25 -Q 20 -C 50 -u -r ${lg} -f \\$REF_GENOME ${bam} | \\ bcftools call -c -V indels | \\ \\$BASE_DIR/py/bamHamletCaller.py ${depth} ${id}.${lg}.coverage_mask.bed.gz | \\ gzip -c &gt; ${id}.${lg}.bam_caller.vcf.gz &quot;&quot;&quot; } // git 8.11 // create segsites file process generate_segsites { label &quot;L_20g15m_msmc_generate_segsites&quot; publishDir &quot;../../2_analysis/msmc/segsites&quot;, mode: &#39;copy&#39; , pattern: &quot;*.segsites.vcf.gz&quot; input: set val( id ), val( lg ), file( bam ), val( depth ), file( vcf ) from sample_vcf2 output: set val( id ), val( lg ), file( &quot;*.segsites.vcf.gz&quot; ), file( &quot;*.covered_sites.bed.txt.gz&quot; ) into segsites_by_sample_lg script: &quot;&quot;&quot; zcat ${vcf} | \\ vcfAllSiteParser.py ${id} ${id}.${lg}.covered_sites.bed.txt.gz | \\ gzip -c &gt; ${id}.${lg}.segsites.vcf.gz &quot;&quot;&quot; } // git 8.12 // assign samples randomly across MSMC and cross coalescence runs process msmc_sample_grouping { label &quot;L_loc_msmc_grouping&quot; publishDir &quot;../../2_analysis/msmc/setup&quot;, mode: &#39;copy&#39; module &quot;R3.5.2&quot; output: file( &quot;msmc_grouping.txt&quot; ) into msmc_grouping file( &quot;msmc_cc_grouping.txt&quot; ) into cc_grouping script: &quot;&quot;&quot; Rscript --vanilla \\$BASE_DIR/R/sample_assignment_msmc.R \\ \\$BASE_DIR/R/distribute_samples_msmc_and_cc.R \\ \\$BASE_DIR/R/cross_cc.R \\ \\$BASE_DIR/metadata/sample_info.txt \\ msmc &quot;&quot;&quot; } // git 8.13 // read grouping into a channel msmc_grouping .splitCsv(header:true, sep:&quot;\\t&quot;) .map{ row -&gt; [ run:row.msmc_run, spec:row.spec, geo:row.geo, group_nr:row.group_nr, group_size:row.group_size, samples:row.samples ] } .set { msmc_runs } // git 8.14 // wait for bam_caller and generate_segsites to finish: /*this &#39;.collect&#39; is only meant to wait until the channel is done, files are being redirected via publishDir*/ coverage_by_sample_lg.collect().map{ &quot;coverage done!&quot; }.into{ coverage_done; coverage_cc } segsites_by_sample_lg.collect().map{ &quot;segsites done!&quot; }.into{ segsites_done; segsites_cc } // git 8.15 // attach masks to MSMC group assignment lg_ch2 .combine( msmc_runs ) .combine( coverage_done ) .combine( segsites_done ) .map{[it[0], it[1].run, it[1]]} .set{ msmc_grouping_after_segsites } // git 8.16 // generating MSMC input files (4 or 3 inds per species) process generate_multihetsep { label &quot;L_120g40h_msmc_generate_multihetsep&quot; publishDir &quot;../../2_analysis/msmc/input/run_${run}&quot;, mode: &#39;copy&#39; , pattern: &quot;*.multihetsep.txt&quot; conda &quot;$HOME/miniconda2/envs/py3&quot; input: /* content msmc_gr: val( msmc_run ), val( spec ), val( geo ), val( group_nr ), val( group_size ), val( samples ) */ /*[LG20, [msmc_run:45, spec:uni, geo:pan, group_nr:4, group_size:3, samples:ind1, ind2, ind3], coverage done!, segsites done!]*/ set val( lg ), val( run ), msmc_gr from msmc_grouping_after_segsites output: set val( run ), val( lg ), val( msmc_gr.spec ), val( msmc_gr.geo ), val( msmc_gr.group_size ), file( &quot;msmc_run.*.multihetsep.txt&quot; ) into msmc_input_lg script: &quot;&quot;&quot; COVDIR=&quot;\\$BASE_DIR/ressources/coverage_masks/&quot; SMP=\\$(echo ${msmc_gr.samples} | \\ sed &quot;s|, |\\\\n--mask=\\${COVDIR}|g; s|^|--mask=\\${COVDIR}|g&quot; | \\ sed &quot;s/\\$/.${lg}.coverage_mask.bed.gz/g&quot; | \\ echo \\$( cat ) ) SEGDIR=&quot;\\$BASE_DIR/2_analysis/msmc/segsites/&quot; SEG=\\$(echo ${msmc_gr.samples} | \\ sed &quot;s|, |\\\\n\\${SEGDIR}|g; s|^|\\${SEGDIR}|g&quot; | \\ sed &quot;s/\\$/.${lg}.segsites.vcf.gz/g&quot; | \\ echo \\$( cat ) ) generate_multihetsep.py \\ \\$SMP \\ --mask=\\$BASE_DIR/ressources/mappability_masks/${lg}.mapmask.bed.txt.gz \\ --negative_mask=\\$BASE_DIR/ressources/indel_masks/indel_mask.${lg}.bed.gz \\ \\$SEG &gt; msmc_run.${msmc_gr.run}.${msmc_gr.spec}.${msmc_gr.geo}.${lg}.multihetsep.txt &quot;&quot;&quot; } // git 8.17 // collect all linkage groups for each run msmc_input_lg .groupTuple() .set {msmc_input} // git 8.18 // run msmc process msmc_run { label &quot;L_190g100h_msmc_run&quot; publishDir &quot;../../2_analysis/msmc/output/&quot;, mode: &#39;copy&#39; , pattern: &quot;*.final.txt&quot; publishDir &quot;../../2_analysis/msmc/loops/&quot;, mode: &#39;copy&#39; , pattern: &quot;*.loop.txt&quot; input: set msmc_run, lg , spec, geo, group_size, file( hetsep ) from msmc_input output: file(&quot;*.msmc2.*.txt&quot;) into msmc_output script: &quot;&quot;&quot; NHAP=\\$(echo \\$(seq 0 \\$((${group_size[0]}*2-1))) | sed &#39;s/ /,/g&#39; ) INFILES=\\$( echo ${hetsep} ) msmc2 \\ -m 0.00254966 -t 8 \\ -p 1*2+25*1+1*2+1*3 \\ -o run${msmc_run}.${spec[0]}.${geo[0]}.msmc2 \\ -I \\${NHAP} \\ \\${INFILES} &quot;&quot;&quot; } // git 8.19 // generate MSMC cross coalescence input files (2 inds x 2 species) cc_grouping .splitCsv(header:true, sep:&quot;\\t&quot;) .map{ row -&gt; [ run:row.run_nr, geo:row.geo, spec_1:row.spec_1, spec_2:row.spec_2, contrast_nr:row.contrast_nr, samples_1:row.samples_1, samples_2:row.samples_2 ] } .set { cc_runs } // git 8.20 // attach masks to cross coalescence group assignment lg_ch3 .combine( cc_runs ) .combine( coverage_cc ) .combine( segsites_cc ) .map{[it[0], it[1].run, it[1]]} .set{ cc_grouping_after_segsites } // git 8.21 // create multihetsep files (combination off all 4 individuals) process generate_multihetsep_cc { label &quot;L_105g30h_cc_generate_multihetsep&quot; publishDir &quot;../../2_analysis/cross_coalescence/input/run_${run}&quot;, mode: &#39;copy&#39; , pattern: &quot;*.multihetsep.txt&quot; conda &quot;$HOME/miniconda2/envs/py3&quot; input: /* content cc_gr: val( run_nr ), val( geo ), val( spec_1 ), val( spec_2 ), val( contrast_nr ), val( samples_1 ), val( samples_2 ) */ set val( lg ), val( run ), cc_gr from cc_grouping_after_segsites output: set val( cc_gr.run ), val( lg ), val( cc_gr.spec_1 ), val( cc_gr.spec_2 ), val( cc_gr.geo ), val( cc_gr.contrast_nr ), val( cc_gr.samples_1 ), val( cc_gr.samples_2 ), file( &quot;cc_run.*.multihetsep.txt&quot; ) into cc_input_lg script: &quot;&quot;&quot; COVDIR=&quot;\\$BASE_DIR/ressources/coverage_masks/&quot; SMP1=\\$(echo ${cc_gr.samples_1} | \\ sed &quot;s|, |\\\\n--mask=\\${COVDIR}|g; s|^|--mask=\\${COVDIR}|g&quot; | \\ sed &quot;s/\\$/.${lg}.coverage_mask.bed.gz/g&quot; | \\ echo \\$( cat ) ) SMP2=\\$(echo ${cc_gr.samples_2} | \\ sed &quot;s|, |\\\\n--mask=\\${COVDIR}|g; s|^|--mask=\\${COVDIR}|g&quot; | \\ sed &quot;s/\\$/.${lg}.coverage_mask.bed.gz/g&quot; | \\ echo \\$( cat ) ) SEGDIR=&quot;\\$BASE_DIR/2_analysis/msmc/segsites/&quot; SEG1=\\$(echo ${cc_gr.samples_1} | \\ sed &quot;s|, |\\\\n\\${SEGDIR}|g; s|^|\\${SEGDIR}|g&quot; | \\ sed &quot;s/\\$/.${lg}.segsites.vcf.gz/g&quot; | \\ echo \\$( cat ) ) SEG2=\\$(echo ${cc_gr.samples_2} | \\ sed &quot;s|, |\\\\n\\${SEGDIR}|g; s|^|\\${SEGDIR}|g&quot; | \\ sed &quot;s/\\$/.${lg}.segsites.vcf.gz/g&quot; | \\ echo \\$( cat ) ) generate_multihetsep.py \\ \\${SMP1} \\ \\${SMP2} \\ --mask=\\$BASE_DIR/ressources/mappability_masks/${lg}.mapmask.bed.txt.gz \\ --negative_mask=\\$BASE_DIR/ressources/indel_masks/indel_mask.${lg}.bed.gz \\ \\${SEG1} \\ \\${SEG2} \\ &gt; cc_run.${run}.${cc_gr.spec_1}-${cc_gr.spec_2}.${cc_gr.contrast_nr}.${cc_gr.geo}.${lg}.multihetsep.txt &quot;&quot;&quot; } // git 8.22 // collect all linkage groups for each run cc_input_lg .groupTuple() .set {cc_input} // git 8.23 // run cross coalescence process cc_run { label &quot;L_190g10ht24_cc_run&quot; publishDir &quot;../../2_analysis/cross_coalescence/output/&quot;, mode: &#39;copy&#39; tag &quot;${cc_run}-${geo[0]}:${spec1[0]}/${spec2[0]}&quot; conda &quot;$HOME/miniconda2/envs/py3&quot; input: set cc_run, lg , spec1, spec2, geo, contr_nr, samples_1, samples_2, file( hetsep ) from cc_input output: file(&quot;cc_run.*.final.txt.gz&quot;) into cc_output script: &quot;&quot;&quot; INFILES=\\$( echo ${hetsep} ) POP1=\\$( echo &quot;${samples_1}&quot; | sed &#39;s/\\\\[//g; s/, /,/g; s/\\\\]//g&#39; ) POP2=\\$( echo &quot;${samples_2}&quot; | sed &#39;s/\\\\[//g; s/, /,/g; s/\\\\]//g&#39; ) msmc2 \\ -m 0.00255863 -t 24 \\ -p 1*2+25*1+1*2+1*3 \\ -o cc_run.${cc_run}.${spec1[0]}.msmc \\ -I 0,1,2,3 \\ \\${INFILES} msmc2 \\ -m 0.00255863 -t 24 \\ -p 1*2+25*1+1*2+1*3 \\ -o cc_run.${cc_run}.${spec2[0]}.msmc \\ -I 4,5,6,7 \\ \\${INFILES} msmc2 \\ -m 0.00255863 -t 24 \\ -p 1*2+25*1+1*2+1*3 \\ -o cc_run.${cc_run}.cross.msmc \\ -I 0,1,2,3,4,5,6,7 \\ -P 0,0,0,0,1,1,1,1 \\ \\${INFILES} combineCrossCoal.py \\ cc_run.${cc_run}.cross.msmc.final.txt \\ cc_run.${cc_run}.${spec1[0]}.msmc.final.txt \\ cc_run.${cc_run}.${spec2[0]}.msmc.final.txt | \\ gzip &gt; cc_run.${cc_run}.final.txt.gz &quot;&quot;&quot; } Finally, we are done with the inference of the demographic history. "],
["git-9-analysis-vii-hybridization.html", "10 (git 9) Analysis VII (hybridization) 10.1 Summary 10.2 Details of analysis_xx.nf", " 10 (git 9) Analysis VII (hybridization) 10.1 Summary The population recombination rate is estimated within the nextflow script analysis_XX.nf (located under $BASE_DIR/nf/09_analysis_hybridization/), which runs on the XX data set. Below is an overview of the steps involved in the analysis. (The green dot indicates the genotype input, red arrows depict output that is exported for further use.) 10.2 Details of analysis_xx.nf 10.2.1 Data preparation The nextflow script starts by opening the genotype data. #!/usr/bin/env nextflow // git 9.1 // open genotype data Channel .fromFilePairs(&quot;../../1_genotyping/4_phased/phased_mac2.vcf.{gz,gz.tbi}&quot;) .into{ vcf_loc1; vcf_loc2; vcf_loc3 } // git 9.2 // initialize location channel Channel .from( &quot;bel&quot;, &quot;hon&quot;, &quot;pan&quot;) .set{ locations_ch } // git 9.3 // define location specific sepcies set Channel.from( [[1, &quot;ind&quot;], [2, &quot;may&quot;], [3, &quot;nig&quot;], [4, &quot;pue&quot;], [5, &quot;uni&quot;]] ).into{ bel_spec1_ch; bel_spec2_ch } Channel.from( [[1, &quot;abe&quot;], [2, &quot;gum&quot;], [3, &quot;nig&quot;], [4, &quot;pue&quot;], [5, &quot;ran&quot;], [6, &quot;uni&quot;]] ).into{ hon_spec1_ch; hon_spec2_ch } Channel.from( [[1, &quot;nig&quot;], [2, &quot;pue&quot;], [3, &quot;uni&quot;]] ).into{ pan_spec1_ch; pan_spec2_ch } // git 9.4 // prepare pairwise new_hybrids // ------------------------------ /* (create all possible species pairs depending on location and combine with genotype subset (for the respective location))*/ bel_pairs_ch = Channel.from( &quot;bel&quot; ) .combine( vcf_loc1 ) .combine(bel_spec1_ch) .combine(bel_spec2_ch) .filter{ it[3] &lt; it[5] } .map{ it[0,1,2,4,6]} hon_pairs_ch = Channel.from( &quot;hon&quot; ) .combine( vcf_loc2 ) .combine(hon_spec1_ch) .combine(hon_spec2_ch) .filter{ it[3] &lt; it[5] } .map{ it[0,1,2,4,6]} pan_pairs_ch = Channel.from( &quot;pan&quot; ) .combine( vcf_loc3 ) .combine(pan_spec1_ch) .combine(pan_spec2_ch) .filter{ it[3] &lt; it[5] } .map{ it[0,1,2,4,6]} bel_pairs_ch.concat( hon_pairs_ch, pan_pairs_ch ).set { all_fst_pairs_ch } // git 9.5 // comute pairwise fsts for SNP filtering process fst_run { label &#39;L_20g45m_fst_run&#39; tag &quot;${spec1}${loc}-${spec2}${loc}&quot; input: set val( loc ), val( vcfidx ), file( vcf ), val( spec1 ), val( spec2 ) from all_fst_pairs_ch output: set val( loc ), val( spec1 ), val( spec2 ), file( &quot;${vcf[0]}&quot; ), file( &quot;*.fst.tsv.gz&quot; ), file( &quot;${spec1}${loc}.pop&quot;), file( &quot;${spec2}${loc}.pop&quot;) into fst_SNPS script: &quot;&quot;&quot; vcfsamplenames ${vcf[0]} | grep ${spec1}${loc} &gt; ${spec1}${loc}.pop vcfsamplenames ${vcf[0]} | grep ${spec2}${loc} &gt; ${spec2}${loc}.pop vcftools --gzvcf ${vcf[0]} \\ --weir-fst-pop ${spec1}${loc}.pop \\ --weir-fst-pop ${spec2}${loc}.pop \\ --stdout | gzip &gt; ${spec1}${loc}-${spec2}${loc}.fst.tsv.gz &quot;&quot;&quot; } // git 9.6 // select the 800 most differentiated SNPs for each population pair process filter_fst { label &#39;L_8g15m_filter_fst&#39; tag &quot;${spec1}${loc}-${spec2}${loc}&quot; input: set val( loc ), val( spec1 ), val( spec2 ), file( vcf ), file( fst ), file( pop1 ), file( pop2 ) from fst_SNPS output: set val( loc ), val( spec1 ), val( spec2 ), file( vcf ), file( pop1 ), file( pop2 ), file( &quot;*SNPs.snps&quot; ) into filter_SNPs script: &quot;&quot;&quot; Rscript --vanilla \\$BASE_DIR/R/filter_snps.R ${fst} 800 ${spec1}${loc}-${spec2}${loc} &quot;&quot;&quot; } // git 9.7 // filter the SNP set by min distance (5kb), than randomly pick 80 SNPs // then reformat newhybrid input process prep_nh_input { label &#39;L_8g15m_prep_nh&#39; tag &quot;${spec1}${loc}-${spec2}${loc}&quot; input: set val( loc ), val( spec1 ), val( spec2 ), file( vcf ), file( pop1 ), file( pop2 ), file( snps ) from filter_SNPs output: set val( loc ), val( spec1 ), val( spec2 ), file( &quot;*_individuals.txt&quot; ), file( &quot;*.80SNPs.txt&quot;) into newhybrids_input script: &quot;&quot;&quot; vcftools \\ --gzvcf ${vcf} \\ --keep ${pop1} \\ --keep ${pop2} \\ --thin 5000 \\ --out newHyb.${spec1}${loc}-${spec2}${loc} \\ --positions ${snps} \\ --recode grep &#39;#&#39; newHyb.${spec1}${loc}-${spec2}${loc}.recode.vcf &gt; newHyb.${spec1}${loc}-${spec2}${loc}.80SNPs.vcf grep -v &#39;#&#39; newHyb.${spec1}${loc}-${spec2}${loc}.recode.vcf | \\ shuf -n 80 | \\ sort -k 1 -k2 &gt;&gt; newHyb.${spec1}${loc}-${spec2}${loc}.80SNPs.vcf grep &#39;#CHROM&#39; newHyb.${spec1}${loc}-${spec2}${loc}.80SNPs.vcf | \\ cut -f 10- | \\ sed &#39;s/\\\\t/\\\\n/g&#39; &gt; newHyb.${spec1}${loc}-${spec2}${loc}.80SNPs_individuals.txt /usr/bin/java -Xmx1024m -Xms512M \\ -jar \\$SFTWR/PGDSpider/PGDSpider2-cli.jar \\ -inputfile newHyb.${spec1}${loc}-${spec2}${loc}.80SNPs.vcf \\ -inputformat VCF \\ -outputfile newHyb.${spec1}${loc}-${spec2}${loc}.80SNPs.txt \\ -outputformat NEWHYBRIDS \\ -spid \\$BASE_DIR/ressources/vcf2nh.spid &quot;&quot;&quot; } // git 9.8 // Run new hybrids // (copy of nh_input is needed because nh can&#39;t read links) process run_nh { label &#39;L_20g15h4x_run_nh&#39; tag &quot;${spec1}${loc}-${spec2}${loc}&quot; publishDir &quot;../../2_analysis/newhyb/&quot;, mode: &#39;copy&#39; input: set val( loc ), val( spec1 ), val( spec2 ), file( inds ), file( snps ) from newhybrids_input output: set file( &quot;nh_input/NH.Results/newHyb.*/*_individuals.txt&quot; ), file( &quot;nh_input/NH.Results/newHyb.*/*_PofZ.txt&quot; ) into newhybrids_output script: &quot;&quot;&quot; mkdir -p nh_input cp ${snps} nh_input/${snps} cp ${inds} nh_input/${inds} Rscript --vanilla \\$BASE_DIR/R/run_newhybrids.R &quot;&quot;&quot; } Finally, we are done with XX. "],
["git-10-analysis-viii-admixture.html", "11 (git 10) Analysis VIII (admixture) 11.1 Summary 11.2 Details of analysis_xx.nf", " 11 (git 10) Analysis VIII (admixture) 11.1 Summary The population recombination rate is estimated within the nextflow script analysis_XX.nf (located under $BASE_DIR/nf/10_analysis_admixture/), which runs on the XX data set. Below is an overview of the steps involved in the analysis. (The green dot indicates the genotype input, red arrows depict output that is exported for further use.) 11.2 Details of analysis_xx.nf 11.2.1 Data preparation The nextflow script starts by opening the genotype data. #!/usr/bin/env nextflow // git 10.1 // open genotype data Channel .fromFilePairs(&quot;../../1_genotyping/4_phased/phased_mac2.vcf.{gz,gz.tbi}&quot;) .set{ vcf_ch } // git 10.2 // Set different k values for the admixture analysis Channel .from( 2..15 ) .set{ k_ch } // git 10.3 // load Fst outlier regions Channel .fromPath(&quot;../../ressources/plugin/poptrees/outlier.bed&quot;) .splitCsv(header:true, sep:&quot;\\t&quot;) .map{ row -&gt; [ chrom:row.chrom, start:row.start, end:row.end, gid:row.gid ] } .combine( vcf_ch ) .set{ vcf_admx } // git 10.4 // subset genotypes to the outlier region and reformat process plink12 { label &#39;L_20g2h_plink12&#39; tag &quot;${grouping.gid}&quot; input: set val( grouping ), val( vcfidx ), file( vcf ) from vcf_admx output: set val( grouping ), file( &quot;hapmap.*.ped&quot; ), file( &quot;hapmap.*.map&quot; ), file( &quot;hapmap.*.nosex&quot; ), file( &quot;pop.txt&quot; ) into admx_plink script: &quot;&quot;&quot; echo -e &quot;CHROM\\\\tSTART\\\\tEND&quot; &gt; outl.bed echo -e &quot;${grouping.chrom}\\\\t${grouping.start}\\\\t${grouping.end}&quot; &gt;&gt; outl.bed vcfsamplenames ${vcf[0]} | \\ grep -v &quot;tor\\\\|tab\\\\|flo&quot; | \\ awk &#39;{print \\$1&quot;\\\\t&quot;\\$1}&#39; | \\ sed &#39;s/\\\\t.*\\\\(...\\\\)\\\\(...\\\\)\\$/\\\\t\\\\1\\\\t\\\\2/g&#39; &gt; pop.txt vcftools \\ --gzvcf ${vcf[0]} \\ --keep pop.txt \\ --bed outl.bed \\ --plink \\ --out admx_plink plink \\ --file admx_plink \\ --recode12 \\ --out hapmap.${grouping.gid} &quot;&quot;&quot; } // git 10.5 // combine genoutype subsets with k values admx_prep = k_ch.combine( admx_plink ) // git 10.6 // run admixture process admixture_all { label &#39;L_20g4h_admixture_all&#39; publishDir &quot;../../2_analysis/admixture/&quot;, mode: &#39;copy&#39; tag &quot;${grouping.gid}.${k}&quot; input: set val( k ), val( grouping ), file( ped ), file( map ), file( nosex ), file( pop ) from admx_prep output: set val( &quot;dummy&quot; ), file( &quot;*.out&quot; ), file( &quot;*.Q&quot; ), file( &quot;*.txt&quot; ) into admx_log script: &quot;&quot;&quot; mv ${pop} pop.${grouping.gid}.${k}.txt admixture --cv ${ped} ${k} | tee log.${grouping.gid}.${k}.out &quot;&quot;&quot; } Finally, we are done with XX. "],
["git-11-visualization.html", "12 (git 11) Visualization 12.1 All at once 12.2 One by one", " 12 (git 11) Visualization 12.1 All at once cd $BASE_DIR bash sh/create_figures.sh 12.2 One by one In the remaining documentation, the individual Visualization scripts are going to discussed in detail. "]
]
