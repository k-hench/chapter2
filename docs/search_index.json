[
["index.html", "Script repository (Hench et al. supplement) 1 Intro 1.1 Analysis 1.2 Figures 1.3 Prerequesites", " Script repository (Hench et al. supplement) Kosmas Hench 2019-11-01 1 Intro This repository contains the complete workflow used in the paper “The genomic origins of a marine radiation”. The indivdual chapters of this documentation follow the separate main steps of the workflow, which each refer to an individual prefix in the git x.x references of the papers mathod section. The individual steps partly depend on each other and were executed in the order depicted below. 1.1 Analysis A documentation of the data preparation and the data analysis (git 1.x - 6.x) can be found at: git 1.x: Genotyping git 2.x: Genotyping all base pairs git 3.x: Analysis (FST &amp; GxP) git 4.x: Analysis (dXY &amp; \\(\\pi\\)) git 5.x: Analysis (phylogeny &amp; topolgy weighting) git 6.x: Analysis (\\(\\rho\\)) 1.2 Figures The creation of the figures is bundled in a single script (git 7) which can be executed once all nextflow scripts have successfuly run. cd $BASE_DIR bash sh/create_figures.sh This is basically just a collection that will run all scripts located under $BASE_DIR/R/fig. Under this location, you will fin one R script per figure (and suppl. figure). So if you are only interrested in a single figure - thats te place to start looking. Furthermore, a more detailed documentation exists for all the figure scripts used for the manuscript: F1, F2 &amp; F3 as well as for all the supplementary figures: 1.3 Prerequesites All scripts assume two variables to be set within the bash environment: $BASE_DIR is assumed to point to the base folder of this repository $SFTWR is a folder that contains all the software dependencies that are used within the scripts The dependencies need to be downloaded and installed separately. "],
["genotyping-i-snps-only.html", "2 Genotyping I (SNPs only) 2.1 Summary 2.2 Details of genotyping.nf", " 2 Genotyping I (SNPs only) 2.1 Summary The genotyping procedure is controlled by the nextflow script genotyping.nf (located under $BASE_DIR/nf/genotyping/). It takes the analysis from the raw sequencing data to the genotyped and phased SNPs. Below is an overview of the steps involved in the genotyping process. (The green dot indicates the genotype input, red arrows depict output that is exported for further use.) 2.2 Details of genotyping.nf 2.2.1 Data preparation The nextflow script starts with a small header and then opens the analysis by reading a table with meta data about the samples. The table is parsed and the values are stored in nextflow variables. #!/usr/bin/env nextflow /* =============================================================== Disclaimer: This pipeline needs a lot of time &amp; memory to run: All in all we used roughly 10 TB and ran for about 1 Month (mainly due to limited bandwidth on the cluster durint the &quot;receive_tuple step) =============================================================== */ // git 1.1 /* open the pipeline based on the metadata spread sheet that includes all information necessary to assign read groups to the sequencing data, split the spread sheet by row and feed it into a channel */ Channel .fromPath(&apos;../../metadata/file_info.txt&apos;) .splitCsv(header:true, sep:&quot;\\t&quot;) .map{ row -&gt; [ id:row.id, label:row.label, file_fwd:row.file_fwd, file_rev:row.file_rev, flowcell_id_fwd:row.flowcell_id_fwd, lane_fwd:row.lane_fwd, company:row.company] } .set { samples_ch } Below is a little preview of the table containing the sample meta data: id label spec geo date coord_N coord_W company .. 16_21-30 16_21-30nigpan nig pan 2016 NA NA duke … 16_21-30 16_21-30nigpan nig pan 2016 NA NA duke … 16_31-40 16_31-40unipan uni pan 2016 NA NA duke … 16_31-40 16_31-40unipan uni pan 2016 NA NA duke … 17996 17996indbel ind bel 2004-07-27 16.801 -88.079 novogene … 17997 17997indbel ind bel 2004-07-27 16.801 -88.079 novogene … … … … … … … … … The first step to prepare the data for the GATK best practices is to convert the sample sequences from *.fq to *.bam format to assign read groups: // git 1.2 /* for every sequencing file, convert into ubam format and assign read groups */ process split_samples { label &apos;L_20g2h_split_samples&apos; input: val x from samples_ch output: set val( &quot;${x.label}.${x.lane_fwd}&quot; ), file( &quot;${x.label}.${x.lane_fwd}.ubam.bam&quot; ) into ubams_mark, ubams_merge script: &quot;&quot;&quot; echo -e &quot;---------------------------------&quot; echo -e &quot;Label:\\t\\t${x.label}\\nFwd:\\t\\t${x.file_fwd}\\nRev:\\t\\t${x.file_rev}&quot; echo -e &quot;Flowcell:\\t${x.flowcell_id_fwd}\\nLane:\\t\\t${x.lane_fwd}&quot; echo -e &quot;Read group:\\t${x.flowcell_id_fwd}.${x.lane_fwd}\\nCompany:\\t${x.company}&quot; mkdir -p \\$BASE_DIR/temp_files gatk --java-options &quot;-Xmx20G&quot; \\ FastqToSam \\ -SM=${x.label} \\ -F1=\\$BASE_DIR/data/seqdata/${x.file_fwd} \\ -F2=\\$BASE_DIR/data/seqdata/${x.file_rev} \\ -O=${x.label}.${x.lane_fwd}.ubam.bam \\ -RG=${x.label}.${x.lane_fwd} \\ -LB=${x.label}&quot;.lib1&quot; \\ -PU=${x.flowcell_id_fwd}.${x.lane_fwd} \\ -PL=Illumina \\ -CN=${x.company} \\ --TMP_DIR=\\$BASE_DIR/temp_files; &quot;&quot;&quot; } The second step is mariking the Illumina adaptors. // git 1.3 /* for every ubam file, mark Illumina adapters */ process mark_adapters { label &apos;L_20g2h_mark_adapters&apos; tag &quot;${sample}&quot; input: set val( sample ), file( input ) from ubams_mark output: set val( sample ), file( &quot;*.adapter.bam&quot;) into adapter_bams file &quot;*.adapter.metrics.txt&quot; into adapter_metrics script: &quot;&quot;&quot; gatk --java-options &quot;-Xmx18G&quot; \\ MarkIlluminaAdapters \\ -I=${input} \\ -O=${sample}.adapter.bam \\ -M=${sample}.adapter.metrics.txt \\ -TMP_DIR=\\$BASE_DIR/temp_files; &quot;&quot;&quot; } We need to pass on the unaligned .bam file and the file containing the adapter information together, so the output of the first two processes are matched by the combined sample and sequencing lane information. // git 1.4 adapter_bams .combine(ubams_merge, by:0) .set {merge_input} For the actual mapping, the sequences are transformed back into .fq format, aligned using bwa and merged back with their original read group information. // git 1.5 /* this step includes a 3 step pipeline: * - re-transformatikon into fq format * - mapping aginst the reference genome_file * - merging with the basuch ubams to include read group information */ process map_and_merge { label &apos;L_75g24h8t_map_and_merge&apos; tag &quot;${sample}&quot; input: set val( sample ), file( adapter_bam_input ), file( ubam_input ) from merge_input output: set val( sample ), file( &quot;*.mapped.bam&quot; ) into mapped_bams script: &quot;&quot;&quot; set -o pipefail gatk --java-options &quot;-Xmx68G&quot; \\ SamToFastq \\ -I=${adapter_bam_input} \\ -FASTQ=/dev/stdout \\ -INTERLEAVE=true \\ -NON_PF=true \\ -TMP_DIR=\\$BASE_DIR/temp_files | \\ bwa mem -M -t 8 -p \\$BASE_DIR/ressources/HP_genome_unmasked_01.fa /dev/stdin | gatk --java-options &quot;-Xmx68G&quot; \\ MergeBamAlignment \\ --VALIDATION_STRINGENCY SILENT \\ --EXPECTED_ORIENTATIONS FR \\ --ATTRIBUTES_TO_RETAIN X0 \\ -ALIGNED_BAM=/dev/stdin \\ -UNMAPPED_BAM=${ubam_input} \\ -OUTPUT=${sample}.mapped.bam \\ --REFERENCE_SEQUENCE=\\$BASE_DIR/ressources/HP_genome_unmasked_01.fa.gz \\ -PAIRED_RUN true \\ --SORT_ORDER &quot;unsorted&quot; \\ --IS_BISULFITE_SEQUENCE false \\ --ALIGNED_READS_ONLY false \\ --CLIP_ADAPTERS false \\ --MAX_RECORDS_IN_RAM 2000000 \\ --ADD_MATE_CIGAR true \\ --MAX_INSERTIONS_OR_DELETIONS -1 \\ --PRIMARY_ALIGNMENT_STRATEGY MostDistant \\ --UNMAPPED_READ_STRATEGY COPY_TO_TAG \\ --ALIGNER_PROPER_PAIR_FLAGS true \\ --UNMAP_CONTAMINANT_READS true \\ -TMP_DIR=\\$BASE_DIR/temp_files &quot;&quot;&quot; } Next, the duplicates are being marked. // git 1.6 /* for every mapped sample,sort and mark duplicates * (intermediate step is required to create .bai file) */ process mark_duplicates { label &apos;L_32g30h_mark_duplicates&apos; publishDir &quot;../../1_genotyping/0_sorted_bams/&quot;, mode: &apos;symlink&apos; tag &quot;${sample}&quot; input: set val( sample ), file( input ) from mapped_bams output: set val { sample - ~/\\.(\\d+)/ }, val( sample ), file( &quot;*.dedup.bam&quot;) into dedup_bams file &quot;*.dedup.metrics.txt&quot; into dedup_metrics script: &quot;&quot;&quot; set -o pipefail gatk --java-options &quot;-Xmx30G&quot; \\ SortSam \\ -I=${input} \\ -O=/dev/stdout \\ --SORT_ORDER=&quot;coordinate&quot; \\ --CREATE_INDEX=false \\ --CREATE_MD5_FILE=false \\ -TMP_DIR=\\$BASE_DIR/temp_files \\ | \\ gatk --java-options &quot;-Xmx30G&quot; \\ SetNmAndUqTags \\ --INPUT=/dev/stdin \\ --OUTPUT=intermediate.bam \\ --CREATE_INDEX=true \\ --CREATE_MD5_FILE=true \\ -TMP_DIR=\\$BASE_DIR/temp_files \\ --REFERENCE_SEQUENCE=\\$BASE_DIR/ressources/HP_genome_unmasked_01.fa.gz gatk --java-options &quot;-Xmx30G&quot; \\ MarkDuplicates \\ -I=intermediate.bam \\ -O=${sample}.dedup.bam \\ -M=${sample}.dedup.metrics.txt \\ -MAX_FILE_HANDLES=1000 \\ -TMP_DIR=\\$BASE_DIR/temp_files rm intermediate* &quot;&quot;&quot; } As a preparation for the actual genotyping, the .bam files are being indexed. // git 1.7 /* index al bam files */ process index_bam { label &apos;L_32g1h_index_bam&apos; tag &quot;${sample}&quot; input: set val( sample ), val( sample_lane ), file( input ) from dedup_bams output: set val( sample ), val( sample_lane ), file( input ), file( &quot;*.bai&quot;) into ( indexed_bams, pir_bams ) script: &quot;&quot;&quot; gatk --java-options &quot;-Xmx30G&quot; \\ BuildBamIndex \\ -INPUT=${input} &quot;&quot;&quot; } At this point the preparation of the sequencing is done and we can start with the genotyping. (The output of the data preparation is split and one copy is later also used to prepare the read aware phasing in the process called extractPirs.) 2.2.2 Genotying Since some of our samples were split over several lanes, we now need to collect all .bam files for each sample. // git 1.8 /* collect all bam files for each sample */ indexed_bams .groupTuple() .set {tubbled} Now, we can create the genotype likelyhoods for each individual sample. // git 1.9 /* create one *.g.vcf file per sample */ process receive_tuple { label &apos;L_36g47h_receive_tuple&apos; publishDir &quot;../../1_genotyping/1_gvcfs/&quot;, mode: &apos;symlink&apos; tag &quot;${sample}&quot; input: set sample, sample_lane, bam, bai from tubbled output: file( &quot;*.g.vcf.gz&quot;) into gvcfs file( &quot;*.vcf.gz.tbi&quot;) into tbis script: &quot;&quot;&quot; INPUT=\\$(echo ${bam} | sed &apos;s/\\\\[/-I /g; s/\\\\]//g; s/,/ -I/g&apos;) gatk --java-options &quot;-Xmx35g&quot; HaplotypeCaller \\ -R=\\$BASE_DIR/ressources/HP_genome_unmasked_01.fa \\ \\$INPUT \\ -O ${sample}.g.vcf.gz \\ -ERC GVCF &quot;&quot;&quot; } The individual genotype likelyhoods are collected and combined for the entire data set. // git 1.10 /* collect and combine all *.g.vcf files */ process gather_gvcfs { label &apos;L_O88g90h_gather_gvcfs&apos; publishDir &quot;../../1_genotyping/1_gvcfs/&quot;, mode: &apos;symlink&apos; echo true input: file( gvcf ) from gvcfs.collect() file( tbi ) from tbis.collect() output: set file( &quot;cohort.g.vcf.gz&quot; ), file( &quot;cohort.g.vcf.gz.tbi&quot; ) into ( gcvf_snps, gvcf_acs, gvcf_indel ) script: &quot;&quot;&quot; GVCF=\\$(echo &quot; ${gvcf}&quot; | sed &apos;s/ /-V /g; s/vcf.gz/vcf.gz /g&apos;) gatk --java-options &quot;-Xmx85g&quot; \\ CombineGVCFs \\ -R=\\$BASE_DIR/ressources/HP_genome_unmasked_01.fa \\ \\$GVCF \\ -O cohort.g.vcf.gz &quot;&quot;&quot; } All samples are jointly genotyped. // git 1.11 /* actual genotyping step (varinat sites only) */ process joint_genotype_snps { label &apos;L_O88g90h_joint_genotype&apos; publishDir &quot;../../1_genotyping/2_raw_vcfs/&quot;, mode: &apos;symlink&apos; input: set file( vcf ), file( tbi ) from gcvf_snps output: set file( &quot;raw_var_sites.vcf.gz&quot; ), file( &quot;raw_var_sites.vcf.gz.tbi&quot; ) into ( raw_var_sites, raw_var_sites_to_metrics ) script: &quot;&quot;&quot; gatk --java-options &quot;-Xmx85g&quot; \\ GenotypeGVCFs \\ -R=\\$BASE_DIR/ressources/HP_genome_unmasked_01.fa \\ -V=${vcf} \\ -O=intermediate.vcf.gz gatk --java-options &quot;-Xmx85G&quot; \\ SelectVariants \\ -R=\\$BASE_DIR/ressources/HP_genome_unmasked_01.fa \\ -V=intermediate.vcf.gz \\ --select-type-to-include=SNP \\ -O=raw_var_sites.vcf.gz rm intermediate.* &quot;&quot;&quot; } The output of this process is split and used to collect the genotype metrics to inform the hard filtering of SNPs and to pass on the genotypes to the process called filterSNPs. At this point we create a channel containing all 24 hamlet linkage groups (LGs). This is used later (in the process called extractPirs) since all LGs are phased separately and only located at this part of the script for historical reasons (sorry :/). // git 1.12 /* generate a LG channel */ Channel .from( (&apos;01&apos;..&apos;09&apos;) + (&apos;10&apos;..&apos;19&apos;) + (&apos;20&apos;..&apos;24&apos;) ) .into{ LG_ids1; LG_ids2 } The metrics of the raw genotypes are collected. // git 1.13 /* produce metrics table to determine filtering thresholds - ups forgot to extract SNPS first*/ process joint_genotype_metrics { label &apos;L_28g5h_genotype_metrics&apos; publishDir &quot;../../1_genotyping/2_raw_vcfs/&quot;, mode: &apos;move&apos; input: set file( vcf ), file( tbi ) from raw_var_sites_to_metrics output: file( &quot;${vcf}.table.txt&quot; ) into raw_metrics script: &quot;&quot;&quot; gatk --java-options &quot;-Xmx25G&quot; \\ VariantsToTable \\ --variant=${vcf} \\ --output=${vcf}.table.txt \\ -F=CHROM -F=POS -F=MQ \\ -F=QD -F=FS -F=MQRankSum -F=ReadPosRankSum \\ --show-filtered &quot;&quot;&quot; } Based on the thresholds derived from the genotype metrics, the genotypes are first tagged and then filtered. After this, the data is filtered for missingness and only bi-allelic SNPs are selected. // git 1.14 /* filter snps basaed on locus annotations, missingness and type (bi-allelic only) */ process filterSNPs { label &apos;L_78g10h_filter_Snps&apos; publishDir &quot;../../1_genotyping/3_gatk_filtered/&quot;, mode: &apos;symlink&apos; input: set file( vcf ), file( tbi ) from raw_var_sites output: set file( &quot;filterd_bi-allelic.vcf.gz&quot; ), file( &quot;filterd_bi-allelic.vcf.gz.tbi&quot; ) into filtered_snps script: &quot;&quot;&quot; gatk --java-options &quot;-Xmx75G&quot; \\ VariantFiltration \\ -R=\\$BASE_DIR/ressources/HP_genome_unmasked_01.fa \\ -V ${vcf} \\ -O=intermediate.vcf.gz \\ --filter-expression &quot;QD &lt; 2.5&quot; \\ --filter-name &quot;filter_QD&quot; \\ --filter-expression &quot;FS &gt; 25.0&quot; \\ --filter-name &quot;filter_FS&quot; \\ --filter-expression &quot;MQ &lt; 52.0 || MQ &gt; 65.0&quot; \\ --filter-name &quot;filter_MQ&quot; \\ --filter-expression &quot;MQRankSum &lt; -0.2 || MQRankSum &gt; 0.2&quot; \\ --filter-name &quot;filter_MQRankSum&quot; \\ --filter-expression &quot;ReadPosRankSum &lt; -2.0 || ReadPosRankSum &gt; 2.0 &quot; \\ --filter-name &quot;filter_ReadPosRankSum&quot; gatk --java-options &quot;-Xmx75G&quot; \\ SelectVariants \\ -R=\\$BASE_DIR/ressources/HP_genome_unmasked_01.fa \\ -V=intermediate.vcf.gz \\ -O=intermediate.filterd.vcf.gz \\ --exclude-filtered vcftools \\ --gzvcf intermediate.filterd.vcf.gz \\ --max-missing-count 17 \\ --max-alleles 2 \\ --stdout \\ --recode | \\ bgzip &gt; filterd_bi-allelic.vcf.gz tabix -p vcf filterd_bi-allelic.vcf.gz rm intermediate.* &quot;&quot;&quot; } At this point, the genotying is done. 2.2.3 Phasing To get from genotypes to haplotypes, we apply read-aware phasing using Shapeit. This takes the original sequencing reads into account, so in the first step the reads are screened for Phase Informative Reads (i.e. reads containing more than a single SNP). This needs to be done for each LG independedly, so we first need to split the genotypes before running extractPIRs. // git 1.15 // extract phase informative reads from // alignments and SNPs process extractPirs { label &apos;L_78g10h_extract_pirs&apos; input: val( lg ) from LG_ids2 set val( sample ), val( sample_lane ), file( input ), file( index ) from pir_bams.collect() set file( vcf ), file( tbi ) from filtered_snps output: set val( lg ), file( &quot;filterd_bi-allelic.LG${lg}.vcf.gz&quot; ), file( &quot;filterd_bi-allelic.LG${lg}.vcf.gz.tbi&quot; ), file( &quot;PIRsList-LG${lg}.txt&quot; ) into pirs_lg script: &quot;&quot;&quot; LG=&quot;LG${lg}&quot; awk -v OFS=&apos;\\t&apos; -v dir=\\$PWD -v lg=\\$LG &apos;{print \\$1,dir&quot;/&quot;\\$2,lg}&apos; \\$BASE_DIR/metadata/bamlist_proto.txt &gt; bamlist.txt vcftools \\ --gzvcf ${vcf} \\ --chr \\$LG \\ --stdout \\ --recode | \\ bgzip &gt; filterd_bi-allelic.LG${lg}.vcf.gz tabix -p vcf filterd_bi-allelic.LG${lg}.vcf.gz extractPIRs \\ --bam bamlist.txt \\ --vcf filterd_bi-allelic.LG${lg}.vcf.gz \\ --out PIRsList-LG${lg}.txt \\ --base-quality 20 \\ --read-quality 15 &quot;&quot;&quot; } Using those PIRs, we can then proceed with the actual phasing. The resulting haplotypes are converted back into .vcf format. // git 1.16 // run the actual phasing process run_shapeit { label &apos;L_75g24h8t_run_shapeit&apos; input: set val( lg ), file( vcf ), file( tbi ), file( pirs ) from pirs_lg output: file( &quot;phased-LG${lg}.vcf.gz&quot; ) into phased_lgs script: &quot;&quot;&quot; LG=&quot;LG${lg}&quot; shapeit \\ -assemble \\ --input-vcf ${vcf} \\ --input-pir ${pirs} \\ --thread 8 \\ -O phased-LG${lg} shapeit \\ -convert \\ --input-hap phased-LG${lg} \\ --output-vcf phased-LG${lg}.vcf bgzip phased-LG${lg}.vcf &quot;&quot;&quot; } After the phasing, we merge the LGs back together to get a single data set. We export a comple data set as well as one that was filtered for a minor allele count of at least two. // git 1.17 // merge the phased LGs back together. // the resulting vcf file represents // the &apos;SNPs only&apos; data set process merge_phased { label &apos;L_28g5h_merge_phased_vcf&apos; publishDir &quot;../../1_genotyping/4_phased/&quot;, mode: &apos;move&apos; input: file( vcf ) from phased_lgs.collect() output: set file( &quot;phased.vcf.gz&quot; ), file( &quot;phased.vcf.gz.tbi&quot; ) into phased_vcf set file( &quot;phased_mac2.vcf.gz&quot; ), file( &quot;phased_mac2.vcf.gz.tbi&quot; ) into phased_mac2_vcf script: &quot;&quot;&quot; vcf-concat \\ phased-LG* | \\ grep -v ^\\$ | \\ tee phased.vcf | \\ vcftools --vcf - --mac 2 --recode --stdout | \\ bgzip &gt; phased_mac2.vcf.gz bgzip phased.vcf tabix -p vcf phased.vcf.gz tabix -p vcf phased_mac2.vcf.gz &quot;&quot;&quot; } Finally, we are done with the entire genotyping procedure for the SNPs olny data set. "],
["genotyping-ii-all-callable-sites.html", "3 Genotyping II (all callable sites) 3.1 Summary 3.2 Details of genotyping_all_basepairs.nf", " 3 Genotyping II (all callable sites) 3.1 Summary The genotyping procedure is controlled by the nextflow script genotyping_all_basepairs.nf (located under $BASE_DIR/nf/genotyping_all_basepairs/). Based on an intermediate step from genotyping.nf, this script produces a data set that includes all callable sites - that is SNPs as well a invariant sites that are covered by sequence. Below is an overview of the steps involved in the genotyping process. (The green dot indicates the genotype input, red arrows depict output that is exported for further use.) 3.2 Details of genotyping_all_basepairs.nf 3.2.1 Data preparation The nextflow script starts with a small header and then imports the joint genotyping likelyhoods for all samples produced by genotyping_all_basepairs.nf. #!/usr/bin/env nextflow // git 2.1 // open genotype likelyhoods Channel .fromFilePairs(&quot;../../1_genotyping/1_gvcfs/cohort.g.vcf.{gz,gz.tbi}&quot;) .set{ vcf_cohort } The genotyping of the different linkage groups is going to happen in parallel, so we need to initialize a channel for the 24 LGs. // git 2.2 // initialize LG channel Channel .from( (&apos;01&apos;..&apos;09&apos;) + (&apos;10&apos;..&apos;19&apos;) + (&apos;20&apos;..&apos;24&apos;) ) .set{ ch_LG_ids } The genotyping likelyhoods are combined, efectively linking the data set to the 24 parallel LGs. // git 2.3 // combine genotypes and LGs ch_LG_ids.combine( vcf_cohort ).set{ vcf_lg_combo } The samples are jointly genotyped, indepently for each LG and including invariant sites. // git 2.4 // actual genotyping step (including invarinat sites) process joint_genotype_snps { label &quot;L_O88g90h_LGs_genotype&quot; input: set val( lg ), vcfId, file( vcf ) from vcf_lg_combo output: set val( &apos;all&apos; ), val( lg ), file( &quot;all_site*.vcf.gz&quot; ), file( &quot;all_site*.vcf.gz.tbi&quot; ) into all_bp_by_location script: &quot;&quot;&quot; gatk --java-options &quot;-Xmx85g&quot; \\ GenotypeGVCFs \\ -R=\\$BASE_DIR/ressources/HP_genome_unmasked_01.fa \\ -L=LG${lg} \\ -V=${vcf[0]} \\ -O=intermediate.vcf.gz \\ --include-non-variant-sites=true gatk --java-options &quot;-Xmx85G&quot; \\ SelectVariants \\ -R=\\$BASE_DIR/ressources/HP_genome_unmasked_01.fa \\ -V=intermediate.vcf.gz \\ --select-type-to-exclude=INDEL \\ -O=all_sites.LG${lg}.vcf.gz rm intermediate.* &quot;&quot;&quot; } The genotypes of the different LGs are merged. // git 2.5 // merge all LGs process merge_genotypes { label &apos;L_78g5h_merge_genotypes&apos; echo true input: set val( dummy ), val( lg ), file( vcf ), file( tbi ) from all_bp_by_location.groupTuple() output: file( &quot;all_sites.vcf.gz&quot; ) into all_bp_merged script: &quot;&quot;&quot; INPUT=\\$(ls -1 *vcf.gz | sed &apos;s/^/ -I /g&apos; | cat \\$( echo )) gatk --java-options &quot;-Xmx85g&quot; \\ GatherVcfs \\ \\$INPUT \\ -O=all_sites.vcf.gz &quot;&quot;&quot; } The genotypes are hard filtered based on various genotyping scores. // git 2.6 // quality based filtering process filterSNP_first { label &apos;L_105g30h_filter_gt1&apos; input: file( vcf ) from all_bp_merged output: set file( &quot;intermediate.filterd.vcf.gz&quot; ), file( &quot;intermediate.filterd.vcf.gz.tbi&quot; ) into filtered_snps_first script: &quot;&quot;&quot; module load openssl1.0.2 tabix -p vcf ${vcf} gatk --java-options &quot;-Xmx75G&quot; \\ VariantFiltration \\ -R=\\$BASE_DIR/ressources/HP_genome_unmasked_01.fa \\ -V ${vcf} \\ -O=intermediate.vcf.gz \\ --filter-expression &quot;QD &lt; 2.5&quot; \\ --filter-name &quot;filter_QD&quot; \\ --filter-expression &quot;FS &gt; 25.0&quot; \\ --filter-name &quot;filter_FS&quot; \\ --filter-expression &quot;MQ &lt; 52.0 || MQ &gt; 65.0&quot; \\ --filter-name &quot;filter_MQ&quot; \\ --filter-expression &quot;MQRankSum &lt; -0.2 || MQRankSum &gt; 0.2&quot; \\ --filter-name &quot;filter_MQRankSum&quot; \\ --filter-expression &quot;ReadPosRankSum &lt; -2.0 || ReadPosRankSum &gt; 2.0 &quot; \\ --filter-name &quot;filter_ReadPosRankSum&quot; \\ --QUIET true &amp;&gt; var_filt.log gatk --java-options &quot;-Xmx75G&quot; \\ SelectVariants \\ -R=\\$BASE_DIR/ressources/HP_genome_unmasked_01.fa \\ -V=intermediate.vcf.gz \\ -O=intermediate.filterd.vcf.gz \\ --exclude-filtered \\ --QUIET true \\ --verbosity ERROR &amp;&gt; var_select.log &quot;&quot;&quot; } A second filtering is based on the missingness of samples. // git 2.7 // missingness based filtering // the resulting vcf file represents // the &apos;all BP&apos; data set process filterSNP_second { label &apos;L_105g30h_filter_gt2&apos; publishDir &quot;../../1_genotyping/3_gatk_filtered/&quot;, mode: &apos;copy&apos; input: set file( vcf ), file( tbi ) from filtered_snps_first output: file( &quot;filterd.allBP.vcf.gz&quot; ) into filtered_snps script: &quot;&quot;&quot; module load openssl1.0.2 vcftools \\ --gzvcf ${vcf} \\ --max-missing-count 17 \\ --stdout \\ --recode | \\ bgzip &gt; filterd.allBP.vcf.gz &quot;&quot;&quot; } Finally, we are done with the second version of genotyping. "],
["analysis-i-fst-gxp.html", "4 Analysis I (FST &amp; GxP) 4.1 Summary 4.2 Details of analysis_fst_gxp.nf", " 4 Analysis I (FST &amp; GxP) 4.1 Summary The genetic differentiation, as well as the genotype x phenotype association, are computed within the nextflow script analysis_fst_gxp.nf (located under $BASE_DIR/nf/analysis_fst_gxp/). It takes the SNPs only data set and computes FST and the GxP association. Below is an overview of the steps involved in the analysis. (The green dot indicates the genotype input, red arrows depict output that is exported for further use.) 4.2 Details of analysis_fst_gxp.nf 4.2.1 Setup The nextflow script starts by opening the genotype data and feeding it into three different streams. #!/usr/bin/env nextflow // git 3.1 // open genotype data Channel .fromFilePairs(&quot;../../1_genotyping/4_phased/phased_mac2.vcf.{gz,gz.tbi}&quot;) .into{ vcf_locations; vcf_filter; vcf_gxp } Since we are going to work on the three sampling locations independently, we create channel for the locations. // git 3.2 // initialize location channel Channel .from( &quot;bel&quot;, &quot;hon&quot;, &quot;pan&quot;) .set{ locations_ch } Then we attach the genotypes to the locations. // git 3.3 // attach genotypes to location locations_ch .combine( vcf_locations ) .set{ vcf_location_combo } Next, we define the species sets sampled at the individual locations. // git 3.4 // define location specific sepcies set Channel.from( [[1, &quot;ind&quot;], [2, &quot;may&quot;], [3, &quot;nig&quot;], [4, &quot;pue&quot;], [5, &quot;uni&quot;]] ).into{ bel_spec1_ch; bel_spec2_ch } Channel.from( [[1, &quot;abe&quot;], [2, &quot;gum&quot;], [3, &quot;nig&quot;], [4, &quot;pue&quot;], [5, &quot;ran&quot;], [6, &quot;uni&quot;]] ).into{ hon_spec1_ch; hon_spec2_ch } Channel.from( [[1, &quot;nig&quot;], [2, &quot;pue&quot;], [3, &quot;uni&quot;]] ).into{ pan_spec1_ch; pan_spec2_ch } For each location, the data is subset to include only local hamlets. // git 3.5 // subset data to loacal hamlets process subset_vcf_by_location { label &quot;L_20g2h_subset_vcf&quot; input: set val( loc ), vcfId, file( vcf ) from vcf_location_combo output: set val( loc ), file( &quot;${loc}.vcf.gz&quot; ), file( &quot;${loc}.pop&quot; ) into ( vcf_loc_pair1, vcf_loc_pair2, vcf_loc_pair3 ) script: &quot;&quot;&quot; vcfsamplenames ${vcf[0]} | \\ grep ${loc} | \\ grep -v tor | \\ grep -v tab &gt; ${loc}.pop vcftools --gzvcf ${vcf[0]} \\ --keep ${loc}.pop \\ --mac 3 \\ --recode \\ --stdout | gzip &gt; ${loc}.vcf.gz &quot;&quot;&quot; } As we also want to compute global statistics, we create another subset including all sampled hamlets but excluding the outgroups. // git 3.6 // subset the global data set to hamlets only process subset_vcf_hamlets_only { label &quot;L_20g15h_filter_hamlets_only&quot; publishDir &quot;../../1_genotyping/4_phased/&quot;, mode: &apos;copy&apos; , pattern: &quot;*.vcf.gz&quot; module &quot;R3.5.2&quot; input: set vcfId, file( vcf ) from vcf_filter output: file( &quot;hamlets_only.vcf.gz*&quot; ) into vcf_hamlets_only set file( &quot;hamlets_only.vcf.gz*&quot; ), file( &quot;hamlets_only.pop.txt&quot; ) into vcf_multi_fst script: &quot;&quot;&quot; vcfsamplenames ${vcf[0]} | \\ grep -v &quot;abe\\\\|gum\\\\|ind\\\\|may\\\\|nig\\\\|pue\\\\|ran\\\\|uni&quot; &gt; outgroup.pop vcfsamplenames ${vcf[0]} | \\ grep &quot;abe\\\\|gum\\\\|ind\\\\|may\\\\|nig\\\\|pue\\\\|ran\\\\|uni&quot; | \\ awk &apos;{print \\$1&quot;\\\\t&quot;\\$1}&apos; | \\ sed &apos;s/\\\\t.*\\\\(...\\\\)\\\\(...\\\\)\\$/\\\\t\\\\1\\\\t\\\\2/g&apos; &gt; hamlets_only.pop.txt vcftools \\ --gzvcf ${vcf[0]} \\ --remove outgroup.pop \\ --recode \\ --stdout | gzip &gt; hamlets_only.vcf.gz &quot;&quot;&quot; } 4.2.1 FST Using this subset, we compute the global differentiation among all hamlet populations. // ----------- Fst section ----------- // git 3.7 // compute global fst process fst_multi { label &apos;L_20g15h_fst_multi&apos; publishDir &quot;../../2_analysis/fst/50k/&quot;, mode: &apos;copy&apos; , pattern: &quot;*.50k.tsv.gz&quot; publishDir &quot;../../2_analysis/fst/10k/&quot;, mode: &apos;copy&apos; , pattern: &quot;*.10k.tsv.gz&quot; publishDir &quot;../../2_analysis/fst/logs/&quot;, mode: &apos;copy&apos; , pattern: &quot;*.log&quot; publishDir &quot;../../2_analysis/summaries&quot;, mode: &apos;copy&apos; , pattern: &quot;fst_outliers_998.tsv&quot; conda &quot;$HOME/miniconda2/envs/py3&quot; module &quot;R3.5.2&quot; input: set file( vcf ), file( pop ) from vcf_multi_fst output: file( &quot;multi_fst*&quot; ) into multi_fst_output file( &quot;fst_outliers_998.tsv&quot; ) into fst_outlier_output script: &quot;&quot;&quot; awk &apos;{print \\$1&quot;\\\\t&quot;\\$2\\$3}&apos; ${pop} &gt; pop.txt for k in abehon gumhon indbel maybel nigbel nighon nigpan puebel puehon puepan ranhon unibel unihon unipan; do grep \\$k pop.txt | cut -f 1 &gt; pop.\\$k.txt done POP=&quot;--weir-fst-pop pop.abehon.txt \\ --weir-fst-pop pop.gumhon.txt \\ --weir-fst-pop pop.indbel.txt \\ --weir-fst-pop pop.maybel.txt \\ --weir-fst-pop pop.nigbel.txt \\ --weir-fst-pop pop.nighon.txt \\ --weir-fst-pop pop.nigpan.txt \\ --weir-fst-pop pop.puebel.txt \\ --weir-fst-pop pop.puehon.txt \\ --weir-fst-pop pop.puepan.txt \\ --weir-fst-pop pop.ranhon.txt \\ --weir-fst-pop pop.unibel.txt \\ --weir-fst-pop pop.unihon.txt \\ --weir-fst-pop pop.unipan.txt&quot; # fst by SNP # ---------- vcftools --gzvcf ${vcf} \\ \\$POP \\ --stdout 2&gt; multi_fst_snp.log | \\ gzip &gt; multi_fst.tsv.gz # fst 50kb window # --------------- vcftools --gzvcf ${vcf} \\ \\$POP \\ --fst-window-step 5000 \\ --fst-window-size 50000 \\ --stdout 2&gt; multi_fst.50k.log | \\ gzip &gt; multi_fst.50k.tsv.gz # fst 10kb window # --------------- vcftools --gzvcf ${vcf} \\ \\$POP \\ --fst-window-step 1000 \\ --fst-window-size 10000 \\ --stdout 2&gt; multi_fst.10k.log | \\ gzip &gt; multi_fst_snp.tsv.gz Rscript --vanilla \\$BASE_DIR/R/table_fst_outliers.R multi_fst.50k.tsv.gz &quot;&quot;&quot; } Then, we set up the pair wise species comparisons… // git 3.8 // prepare pairwise fsts // ------------------------------ /* (create all possible species pairs depending on location and combine with genotype subset (for the respective location))*/ // ------------------------------ /* channel content after joinig: set [0:val(loc), 1:file(vcf), 2:file(pop), 3:val(spec1), 4:val(spec2)]*/ // ------------------------------ bel_pairs_ch = Channel.from( &quot;bel&quot; ) .join( vcf_loc_pair1 ) .combine(bel_spec1_ch) .combine(bel_spec2_ch) .filter{ it[3] &lt; it[5] } .map{ it[0,1,2,4,6]} hon_pairs_ch = Channel.from( &quot;hon&quot; ) .join( vcf_loc_pair2 ) .combine(hon_spec1_ch) .combine(hon_spec2_ch) .filter{ it[3] &lt; it[5] } .map{ it[0,1,2,4,6]} pan_pairs_ch = Channel.from( &quot;pan&quot; ) .join( vcf_loc_pair3 ) .combine(pan_spec1_ch) .combine(pan_spec2_ch) .filter{ it[3] &lt; it[5] } .map{ it[0,1,2,4,6]} bel_pairs_ch.concat( hon_pairs_ch, pan_pairs_ch ).set { all_fst_pairs_ch } … and run them. // git 3.9 // comute pairwise fsts process fst_run { label &apos;L_32g4h_fst_run&apos; publishDir &quot;../../2_analysis/fst/50k/&quot;, mode: &apos;copy&apos; , pattern: &quot;*.50k.windowed.weir.fst.gz&quot; publishDir &quot;../../2_analysis/fst/10k/&quot;, mode: &apos;copy&apos; , pattern: &quot;*.10k.windowed.weir.fst.gz&quot; publishDir &quot;../../2_analysis/fst/logs/&quot;, mode: &apos;copy&apos; , pattern: &quot;${loc}-${spec1}-${spec2}.log&quot; input: set val( loc ), file( vcf ), file( pop ), val( spec1 ), val( spec2 ) from all_fst_pairs_ch output: set val( loc ), file( &quot;*.50k.windowed.weir.fst.gz&quot; ), file( &quot;${loc}-${spec1}-${spec2}.log&quot; ) into fst_50k file( &quot;*.10k.windowed.weir.fst.gz&quot; ) into fst_10k_output file( &quot;${loc}-${spec1}-${spec2}.log&quot; ) into fst_logs script: &quot;&quot;&quot; grep ${spec1} ${pop} &gt; pop1.txt grep ${spec2} ${pop} &gt; pop2.txt vcftools --gzvcf ${vcf} \\ --weir-fst-pop pop1.txt \\ --weir-fst-pop pop2.txt \\ --fst-window-step 5000 \\ --fst-window-size 50000 \\ --out ${loc}-${spec1}-${spec2}.50k 2&gt; ${loc}-${spec1}-${spec2}.log vcftools --gzvcf ${vcf} \\ --weir-fst-pop pop1.txt \\ --weir-fst-pop pop2.txt \\ --fst-window-size 10000 \\ --fst-window-step 1000 \\ --out ${loc}-${spec1}-${spec2}.10k gzip *.windowed.weir.fst &quot;&quot;&quot; } The genome wide summaries are compiled from the log files of VCFtools. // git 3.10 /* collect the VCFtools logs to crate a table with the genome wide fst values */ process fst_globals { label &apos;L_loc_fst_globals&apos; publishDir &quot;../../2_analysis/summaries&quot;, mode: &apos;copy&apos; , pattern: &quot;fst_globals.txt&quot; module &quot;R3.5.2&quot; input: file( log ) from fst_logs.collect() output: file( &quot;fst_globals.txt&quot; ) into fst_glob script: &quot;&quot;&quot; cat *.log | \\ grep -E &apos;Weir and Cockerham|--out&apos; | \\ grep -A 3 50k | \\ sed &apos;/^--/d; s/^.*--out //g; s/.50k//g; /^Output/d; s/Weir and Cockerham //g; s/ Fst estimate: /\\t/g&apos; | \\ paste - - - | \\ cut -f 1,3,5 | \\ sed &apos;s/^\\\\(...\\\\)-/\\\\1\\\\t/g&apos; &gt; fst_globals.txt &quot;&quot;&quot; } 4.2.1 GxP The software use for the GxP association uses genotypes in plink file format, so the first step was to convert the input. // ----------- G x P section ----------- // git 3.11 // reformat genotypes (1) process plink12 { label &apos;L_20g2h_plink12&apos; input: set vcfId, file( vcf ) from vcf_gxp output: set file( &quot;GxP_plink.map&quot; ), file( &quot;GxP_plink.ped&quot; ) into plink_GxP script: &quot;&quot;&quot; vcfsamplenames ${vcf[0]} | \\ grep -v &quot;tor\\\\|tab\\\\|flo&quot; | \\ awk &apos;{print \\$1&quot;\\\\t&quot;\\$1}&apos; | \\ sed &apos;s/\\\\t.*\\\\(...\\\\)\\\\(...\\\\)\\$/\\\\t\\\\1\\\\t\\\\2/g&apos; &gt; pop.txt vcftools \\ --gzvcf ${vcf[0]} \\ --plink \\ --out GxP_plink plink \\ --file GxP_plink \\ --recode12 \\ --out hapmap &quot;&quot;&quot; } // git 3.12 // reformat genotypes (2) process GxP_run { label &apos;L_20g2h_GxP_binary&apos; input: set file( map ), file( ped ) from plink_GxP output: set file( &quot;*.bed&quot; ), file( &quot;*.bim&quot; ),file( &quot;*.fam&quot; ) into plink_binary script: &quot;&quot;&quot; # convert genotypes into binary format (bed/bim/fam) plink \\ --noweb \\ --file GxP_plink \\ --make-bed \\ --out GxP_plink_binary &quot;&quot;&quot; } Additionally to the genotypes, the phenotypes of the samples are needed. // git 3.12 // import phenotypes Channel .fromPath(&quot;../../metadata/phenotypes.sc&quot;) .set{ phenotypes_raw } For historic reasons, we perform a PCA on the phenotypes and reoport the extended phenotype data including the scoring on PC1 &amp; PC2. // git 3.13 // run PCA on phenotypes process phenotye_pca { label &quot;L_loc_phenotype_pca&quot; publishDir &quot;../../2_analysis/phenotype&quot;, mode: &apos;copy&apos; , pattern: &quot;*.txt&quot; module &quot;R3.5.2&quot; input: file( sc ) from phenotypes_raw output: file( &quot;phenotypes.txt&quot; ) into phenotype_file file( &quot;phenotype_pca*.pdf&quot; ) into phenotype_pca script: &quot;&quot;&quot; Rscript --vanilla \\$BASE_DIR/R/phenotypes_pca.R ${sc} &quot;&quot;&quot; } We set up all the traits for which we want to perform a GxP association. // git 3.14 // setup GxP traits Channel .from(&quot;Bars&quot;, &quot;Snout&quot;, &quot;Peduncle&quot;) //, &quot;Lines&quot;, &quot;Blue&quot;, &quot;Yellow&quot;, &quot;Orange&quot;, &quot;Tail_transparent&quot;,&quot;PC1&quot;, &quot;PC2&quot;, &quot;PC_d1&quot;, &quot;abe&quot;, &quot;gum&quot;, &quot;ind&quot;, &quot;may&quot;, &quot;nig&quot;, &quot;pue&quot;, &quot;ran&quot;, &quot;uni&quot;, &quot;blue2&quot; ) .set{ traits_ch } Then, we combine the reformatted genotypes with the phenotyes and the traits of interrest. // git 3.15 // bundle GxP input traits_ch.combine( plink_binary ).combine( phenotype_file ).set{ trait_plink_combo } Having collectet all the input, we can now run the GxP. // git 3.16 // actually run the GxP process gemma_run { label &apos;L_32g4h_GxP_run&apos; publishDir &quot;../../2_analysis/GxP/bySNP/&quot;, mode: &apos;copy&apos; module &quot;R3.5.2&quot; input: set val( pheno ), file( bed ), file( bim ), file( fam ), file( pheno_file ) from trait_plink_combo output: file(&quot;*.GxP.txt.gz&quot;) into gemma_results script: &quot;&quot;&quot; source \\$BASE_DIR/sh/body.sh BASE_NAME=\\$(echo ${fam} | sed &apos;s/.fam//g&apos;) mv ${fam} \\$BASE_NAME-old.fam cp \\${BASE_NAME}-old.fam ${fam} # 1) replace the phenotype values Rscript --vanilla \\$BASE_DIR/R/assign_phenotypes.R ${fam} ${pheno_file} ${pheno} # 2) create relatedness matrix of samples using gemma gemma -bfile \\$BASE_NAME -gk 1 -o ${pheno} # 3) fit linear model using gemma (-lm) gemma -bfile \\$BASE_NAME -lm 4 -miss 0.1 -notsnp -o ${pheno}.lm # 4) fit linear mixed model using gemma (-lmm) gemma -bfile \\$BASE_NAME -k output/${pheno}.cXX.txt -lmm 4 -o ${pheno}.lmm # 5) reformat output sed &apos;s/\\\\trs\\\\t/\\\\tCHROM\\\\tPOS\\\\t/g; s/\\\\([0-2][0-9]\\\\):/\\\\1\\\\t/g&apos; output/${pheno}.lm.assoc.txt | \\ cut -f 2,3,9-14 | body sort -k1,1 -k2,2n | gzip &gt; ${pheno}.lm.GxP.txt.gz sed &apos;s/\\\\trs\\\\t/\\\\tCHROM\\\\tPOS\\\\t/g; s/\\\\([0-2][0-9]\\\\):/\\\\1\\\\t/g&apos; output/${pheno}.lmm.assoc.txt | \\ cut -f 2,3,8-10,13-15 | body sort -k1,1 -k2,2n | gzip &gt; ${pheno}.lmm.GxP.txt.gz &quot;&quot;&quot; } To smooth the GxP results, we initialize two resolutions (50 kb windows with 5 kb increments and 10 kb windows with 1 kb increments). // git 3.17 // setup smoothing levels Channel .from([[50000, 5000], [10000, 1000]]) .set{ gxp_smoothing_levels } The we apply all smoothing levels to the raw GxP output… // git 3.18 // apply all smoothing levels gemma_results.combine( gxp_smoothing_levels ).set{ gxp_smoothing_input } .. and run the smoothing. // git 3.19 // actually run the smoothing process gemma_smooth { label &apos;L_20g2h_GxP_smooth&apos; publishDir &quot;../../2_analysis/GxP/${win}&quot;, mode: &apos;copy&apos; input: set file( lm ), file( lmm ), val( win ), val( step ) from gxp_smoothing_input output: set val( win ), file( &quot;*.lm.*k.txt.gz&quot; ) into gxp_lm_smoothing_output set val( win ), file( &quot;*.lmm.*k.txt.gz&quot; ) into gxp_lmm_smoothing_output script: &quot;&quot;&quot; \\$BASE_DIR/sh/gxp_slider ${lm} ${win} ${step} \\$BASE_DIR/sh/gxp_slider ${lmm} ${win} ${step} &quot;&quot;&quot; } At this step we are done with differentiation and GxP. "],
["analysis-ii-dxy-pi.html", "5 Analysis II (dXY &amp; pi) 5.1 Summary 5.2 Details of analysis_dxy.nf", " 5 Analysis II (dXY &amp; pi) 5.1 Summary The genetic divergence, as well diversity, are computed within the nextflow script analysis_dxy.nf (located under $BASE_DIR/nf/analysis_dxy/). It takes the all BP data set and computes dXY and \\(\\pi\\). Below is an overview of the steps involved in the analysis. (The green dot indicates the genotype input, red arrows depict output that is exported for further use.) 5.2 Details of analysis_dxy.nf 5.2.1 Data preparation The nextflow script starts by opening the genotype data and feeding it into three different streams (one for dXY and one for \\(\\pi\\)). #!/usr/bin/env nextflow // This pipelie includes the anlysis run on the // all callable sites data sheet (dxy). // git 4.1 // load genotypes Channel .fromFilePairs(&quot;../../1_genotyping/3_gatk_filtered/filterd.allBP.vcf.{gz,gz.tbi}&quot;) .into{ vcf_ch; vcf_pi_ch } The computation of dXY is split by linkage group, so we neet to initializ a channel for the LGs. // git 4.2 // initialize LGs Channel .from( (&apos;01&apos;..&apos;09&apos;) + (&apos;10&apos;..&apos;19&apos;) + (&apos;20&apos;..&apos;24&apos;) ) .set{ lg_ch } Now, we can subset the data set and convert it to a custom genotype format. // git 4.3 // split by LG and reformat the genotypes process split_allBP { label &apos;L_32g15h_split_allBP&apos; tag &quot;LG${lg}&quot; input: set val( lg ), vcfId, file( vcf ) from lg_ch.combine( vcf_ch ) output: set val( lg ), file( &apos;filterd.allBP.vcf.gz&apos; ), file( &quot;allBP.LG${lg}.geno.gz&quot; ) into geno_ch script: &quot;&quot;&quot; module load openssl1.0.2 vcftools --gzvcf ${vcf[0]} \\ --chr LG${lg} \\ --recode \\ --stdout | bgzip &gt; allBP.LG${lg}.vcf.gz python \\$SFTWR/genomics_general/VCF_processing/parseVCF.py \\ -i allBP.LG${lg}.vcf.gz | gzip &gt; allBP.LG${lg}.geno.gz &quot;&quot;&quot; } Since the species composition differs between locations, we need to initialize three seperate sets of species. These are going to be used to create the divergence species pairs. // git 4.4 // define location specific sepcies set Channel.from( [[1, &quot;ind&quot;], [2, &quot;may&quot;], [3, &quot;nig&quot;], [4, &quot;pue&quot;], [5, &quot;uni&quot;]] ).into{ bel_spec1_ch; bel_spec2_ch } Channel.from( [[1, &quot;abe&quot;], [2, &quot;gum&quot;], [3, &quot;nig&quot;], [4, &quot;pue&quot;], [5, &quot;ran&quot;], [6, &quot;uni&quot;]] ).into{ hon_spec1_ch; hon_spec2_ch } Channel.from( [[1, &quot;nig&quot;], [2, &quot;pue&quot;], [3, &quot;uni&quot;]] ).into{ pan_spec1_ch; pan_spec2_ch } For the diversity, we also initialize the full set of populations of the study. // git 4.5 // init all sampled populations (for pi) Channel .from(&apos;indbel&apos;, &apos;maybel&apos;, &apos;nigbel&apos;, &apos;puebel&apos;, &apos;unibel&apos;, &apos;abehon&apos;, &apos;gumhon&apos;, &apos;nighon&apos;, &apos;puehon&apos;, &apos;ranhon&apos;, &apos;unihon&apos;, &apos;nigpan&apos;, &apos;puepan&apos;, &apos;unipan&apos;) .set{spec_dxy} We want to run a slining window at different resolutions, so we set up a channel for these. // git 4.6 // init slining window resolutions Channel .from( 1, 5 ) .into{ kb_ch; kb_ch2; kb_ch3 } 5.2.1 dXY To prepare all species comparisons used to estimate divergence, we combine each species witch all other species within a location. // git 4.7 // prepare pair wise dxy // ------------------------------ // create all possible species pairs depending on location // and combine with genotype subset (for the respective location) // ------------------------------ // channel content after joinig: // set [0:val(loc), 1:file(vcf), 2:file(pop), 3:val(spec1), 4:val(spec2)] // ------------------------------ bel_pairs_ch = Channel.from( &quot;bel&quot; ) .combine( bel_spec1_ch ) .combine( bel_spec2_ch ) .filter{ it[1] &lt; it[3] } .map{ it[0,2,4]} hon_pairs_ch = Channel.from( &quot;hon&quot; ) .combine( hon_spec1_ch ) .combine(hon_spec2_ch) .filter{ it[1] &lt; it[3] } .map{ it[0,2,4]} pan_pairs_ch = Channel.from( &quot;pan&quot; ) .combine( pan_spec1_ch ) .combine(pan_spec2_ch) .filter{ it[1] &lt; it[3] } .map{ it[0,2,4]} Now we attach the genotypes and the resolution level to the species pairs. // git 4.8 // combine species pair with genotypes (and window size) bel_pairs_ch .concat( hon_pairs_ch, pan_pairs_ch ) .combine( geno_ch ) .combine( kb_ch ) .into { all_dxy_pairs_ch; random_dxy_pairs_ch } At this point, we can calculate dXY. // git 4.9 // compute the dxy values process dxy_lg { label &apos;L_G32g15h_dxy_lg&apos; tag &quot;${spec1}${loc}-${spec2}${loc}_LG${lg}&quot; input: set val( loc ), val( spec1 ), val( spec2 ), val( lg ), file( vcf ), file( geno ), val( kb ) from all_dxy_pairs_ch output: set val( &quot;${spec1}${loc}-${spec2}${loc}-${kb}&quot; ), file( &quot;dxy.${spec1}${loc}-${spec2}${loc}.LG${lg}.${kb}0kb-${kb}kb.txt.gz&quot; ), val( lg ), val( &quot;${spec1}${loc}&quot; ), val( &quot;${spec2}${loc}&quot; ), val( kb ) into dxy_lg_ch script: &quot;&quot;&quot; module load openssl1.0.2 module load intel17.0.4 intelmpi17.0.4 zcat ${geno} | \\ head -n 1 | \\ cut -f 3- | \\ sed &apos;s/\\\\t/\\\\n/g&apos; | \\ awk -v OFS=&apos;\\\\t&apos; &apos;{print \\$1, substr( \\$1, length(\\$1) - 5, 6)}&apos; &gt; pop.txt mpirun \\$NQSII_MPIOPTS -np 1 \\ python \\$SFTWR/genomics_general/popgenWindows.py \\ -w ${kb}0000 -s ${kb}000 \\ --popsFile pop.txt \\ -p ${spec1}${loc} -p ${spec2}${loc} \\ -g ${geno} \\ -o dxy.${spec1}${loc}-${spec2}${loc}.LG${lg}.${kb}0kb-${kb}kb.txt.gz \\ -f phased \\ --writeFailedWindows \\ -T 1 &quot;&quot;&quot; } Since the calculation was split across LGs, we now need to collect all LGs of a particular species pair… // git 4.10 // collect all LGs for each species pair dxy_lg_ch .groupTuple() .set{ tubbled_dxy } … and merge the results. // git 4.11 // concatinate all LGs for each species pair process receive_tuple { label &apos;L_20g2h_receive_tuple&apos; publishDir &quot;../../2_analysis/dxy/${kb[0]}0k/&quot;, mode: &apos;copy&apos; tag &quot;${pop1[0]}-${pop2[0]}&quot; input: set val( comp ), file( dxy ), val( lg ), val( pop1 ), val( pop2 ), val( kb ) from tubbled_dxy output: file( &quot;dxy.${pop1[0]}-${pop2[0]}.${kb[0]}0kb-${kb[0]}kb.tsv.gz&quot; ) into dxy_output_ch script: &quot;&quot;&quot; zcat dxy.${pop1[0]}-${pop2[0]}.LG01.${kb[0]}0kb-${kb[0]}kb.txt.gz | \\ head -n 1 &gt; dxy.${pop1[0]}-${pop2[0]}.${kb[0]}0kb-${kb[0]}kb.tsv; for j in {01..24};do echo &quot;-&gt; LG\\$j&quot; zcat dxy.${pop1[0]}-${pop2[0]}.LG\\$j.${kb[0]}0kb-${kb[0]}kb.txt.gz | \\ awk &apos;NR&gt;1{print}&apos; &gt;&gt; dxy.${pop1[0]}-${pop2[0]}.${kb[0]}0kb-${kb[0]}kb.tsv; done gzip dxy.${pop1[0]}-${pop2[0]}.${kb[0]}0kb-${kb[0]}kb.tsv &quot;&quot;&quot; } For control, we also create a “random” dXY run, where we take the most diverged species pair and randomize the population assginment of the samples. Therefore, we first pick the most diverged species pair. // git 4.12 // collect a species pair to randomize Channel .from( [[&apos;bel&apos;, &apos;ind&apos;, &apos;may&apos;]] ) .set{ random_run_ch } Then we set the sliding windiw resolution. // git 4.13 // setup channel contnent for random channel Channel .from( 1 ) .combine( random_run_ch ) .combine( kb_ch2 ) .filter{ it[4] == 5 } .set{ random_sets_ch } Now, we randomize the population assignment of the sampels and calculate differentiation. // git 4.14 // permute the population assignment (the randomization) process randomize_samples { label &apos;L_20g15h_randomize_samples&apos; publishDir &quot;../../2_analysis/fst/${kb}0k/random&quot;, mode: &apos;copy&apos; , pattern: &quot;*_windowed.weir.fst.gz&quot; module &quot;R3.5.2&quot; input: set val( random_set ), val( loc ), val(spec1), val(spec2), val( kb ) from random_sets_ch output: set random_set, file( &quot;random_pop.txt&quot; ) into random_pops_ch file( &quot;*_windowed.weir.fst.gz&quot;) into random_fst_out script: &quot;&quot;&quot; cut -f 2,3 \\$BASE_DIR/metadata/sample_info.txt | \\ grep &quot;${loc}&quot; | \\ grep &quot;${spec1}\\\\|${spec2}&quot; &gt; pop_prep.tsv Rscript --vanilla \\$BASE_DIR/R/randomize_pops.R grep A random_pop.txt | cut -f 1 &gt; pop1.txt grep B random_pop.txt | cut -f 1 &gt; pop2.txt vcftools \\ --gzvcf \\$BASE_DIR/1_genotyping/3_gatk_filtered/filterd_bi-allelic.allBP.vcf.gz \\ --weir-fst-pop pop1.txt \\ --weir-fst-pop pop2.txt \\ --fst-window-step ${kb}0000 \\ --fst-window-size ${kb}0000 \\ --stdout | gzip &gt; ${loc}-aaa-bbb.${kb}0k.random_${spec1}_${spec2}_windowed.weir.fst.gz &quot;&quot;&quot; } We set up another channel to prepare the random dXY… // git 4.15 // pick random pair of interest random_dxy_pairs_ch .filter{ it[0] == &apos;bel&apos; &amp;&amp; it[1] == &apos;ind&apos; &amp;&amp; it[2] == &apos;may&apos; &amp;&amp; it[6] == 5 } .combine( random_pops_ch ) .set{ random_assigned_ch } .. and compute the divergence. // git 4.16 // compute the dxy values process dxy_lg_random { label &apos;L_G32g15h_dxy_lg_random&apos; tag &quot;aaa${loc}-bbb${loc}_LG${lg}&quot; module &quot;R3.5.2&quot; input: set val( loc ), val( spec1 ), val( spec2 ), val( lg ), file( vcf ), file( geno ), val( kb ), val( random_set ), file( pop_file ) from random_assigned_ch output: set val( &quot;aaa${loc}-bbb${loc}-${kb}0kb&quot; ), file( &quot;dxy.aaa${loc}-bbb${loc}.LG${lg}.${kb}0kb-${kb}kb.txt.gz&quot; ), val( lg ), val( &quot;aaa${loc}&quot; ), val( &quot;bbb${loc}&quot; ), val( kb ) into dxy_random_lg_ch script: &quot;&quot;&quot; module load openssl1.0.2 module load intel17.0.4 intelmpi17.0.4 mpirun \\$NQSII_MPIOPTS -np 1 \\ python \\$SFTWR/genomics_general/popgenWindows.py \\ -w ${kb}0000 -s ${kb}000 \\ --popsFile ${pop_file} \\ -p A -p B \\ -g ${geno} \\ -o dxy.aaa${loc}-bbb${loc}.LG${lg}.${kb}0kb-${kb}kb.txt.gz \\ -f phased \\ --writeFailedWindows \\ -T 1 &quot;&quot;&quot; } Again, we collect the output of the individual LGs…. // git 4.17 // collect all LGs of random run dxy_random_lg_ch .groupTuple() .set{ tubbled_random_dxy } … and we merge them // git 4.18 // concatinate all LGs of random run process receive_random_tuple { label &apos;L_20g2h_receive_random_tuple&apos; publishDir &quot;../../2_analysis/dxy/random/&quot;, mode: &apos;copy&apos; input: set val( comp ), file( dxy ), val( lg ), val( pop1 ), val( pop2 ), val( kb ) from tubbled_random_dxy output: file( &quot;dxy.${pop1[0]}-${pop2[0]}.${kb[0]}0kb-${kb[0]}kb.tsv.gz&quot; ) into dxy_random_output_ch script: &quot;&quot;&quot; zcat dxy.${pop1[0]}-${pop2[0]}.LG01.${kb[0]}0kb-${kb[0]}kb.txt.gz | \\ head -n 1 &gt; dxy.${pop1[0]}-${pop2[0]}.${kb[0]}0kb-${kb[0]}kb.tsv; for j in {01..24};do echo &quot;-&gt; LG\\$j&quot; zcat dxy.${pop1[0]}-${pop2[0]}.LG\\$j.${kb[0]}0kb-${kb[0]}kb.txt.gz | \\ awk &apos;NR&gt;1{print}&apos; &gt;&gt; dxy.${pop1[0]}-${pop2[0]}.${kb[0]}0kb-${kb[0]}kb.tsv; done gzip dxy.${pop1[0]}-${pop2[0]}.${kb[0]}0kb-${kb[0]}kb.tsv &quot;&quot;&quot; } 5.2.1 \\(\\pi\\) Estimating the diveristy is quite staight foreward: We take the prepared, population identifier, the data set and the window resolutions and run VCFtools on each combination. // --------------------------------------------------------------- // The pi part need to be run AFTER the global fst outlier // windows were selected (REMEMBER TO CHECK FST OUTLIER DIRECTORY) // --------------------------------------------------------------- // git 4.19 // calculate pi per species process pi_per_spec { label &apos;L_32g15h_pi&apos; tag &quot;${spec}&quot; publishDir &quot;../../2_analysis/pi/${kb}0k&quot;, mode: &apos;copy&apos; input: set val( spec ), vcfId, file( vcf ), val( kb ) from spec_dxy.combine( vcf_pi_ch ).combine( kb_ch3 ) output: file( &quot;*.${kb}0k.windowed.pi.gz&quot; ) into pi_50k script: &quot;&quot;&quot; module load openssl1.0.2 vcfsamplenames ${vcf[0]} | \\ grep ${spec} &gt; pop.txt vcftools --gzvcf ${vcf[0]} \\ --keep pop.txt \\ --window-pi ${kb}0000 \\ --window-pi-step ${kb}000 \\ --out ${spec}.${kb}0k 2&gt; ${spec}.pi.log gzip ${spec}.${kb}0k.windowed.pi tail -n +2 \\$BASE_DIR/2_analysis/fst/outliers/all_multi_fst_outliers_998.tsv | \\ cut -f 2,3,4 &gt; outlier.bed vcftools --gzvcf ${vcf[0]} \\ --keep pop.txt \\ --exclude-bed outlier.bed \\ --window-pi ${kb}0000 \\ --window-pi-step ${kb}000\\ --out ${spec}_no_outlier.${kb}0k 2&gt; ${spec}_${kb}0k_no_outllier.pi.log gzip ${spec}_no_outlier.${kb}0k.windowed.pi &quot;&quot;&quot; } At this step we are done with divergence and diversity. "],
["analysis-iii-phylogeny-topology-weighting.html", "6 Analysis III (phylogeny &amp; topology weighting) 6.1 Summary 6.2 Details of analysis_fasttree_twisst.nf", " 6 Analysis III (phylogeny &amp; topology weighting) 6.1 Summary The whole genome phylogeny and the topology weighting are prepared within the nextflow script analysis_fasttree_twisst.nf (located under $BASE_DIR/nf/analysis_fasttree_twisst/), which runs on the SNPs only data set. Below is an overview of the steps involved in the process. (The green dot indicates the genotype input, red arrows depict output that is exported for further use.) 6.2 Details of analysis_fasttree_twisst.nf 6.2.1 Setup The nextflow script starts by opening the genotype data and feeding it into two different streams (one for the phylogeny and one for topology weighting). #!/usr/bin/env nextflow // git 5.1 // open genotype data Channel .fromFilePairs(&quot;../../1_genotyping/4_phased/phased_mac2.vcf.{gz,gz.tbi}&quot;) .into{ vcf_fasttree_whg; vcf_locations } 6.2.1 Whole genome phylogeny During data exploration, various subsets of the data set were investigated, including the different sampling locations, whole genome vs. non-outlier regions and all samples vs. hamlets only. (Now, most of the oftions have been muted by commenting most options out.) Here, we set up a channel managing the subset by location. // git 5.2 // setting the sampling location // (the script is set up to run on diffferent subsets of samples) Channel .from( &quot;all&quot; ) //, &quot;bel&quot;, &quot;hon&quot;, &quot;pan&quot; ) .set{ locations4_ch } This channel toggles the inclusion of FST outlier regions. // git 5.3 // setting loci restrictions // (keep vs remove outlier regions) Channel .from( &quot;whg&quot; ) //, &quot;no_musks&quot; ) .set{ whg_modes } This channel toggles the inclusion of the non-hamlet outgroup. // git 5.4 // setting the sampling mode // (the script is set up to run on diffferent subsets of samples) Channel .from( &quot;no_outgroups&quot; ) //, &quot;all&quot; ) .into{ sample_modes } We combine the different selector channels to create all possible combinations of the settings. // git 5.5 // compile the config settings and add data file locations4_ch .combine( vcf_fasttree_whg ) .combine( whg_modes ) .combine( sample_modes ) .set{ vcf_fasttree_whg_location_combo } To prepare the input for the phylogeny, the fine tuned selection is applied to subset the genotypes. // git 5.6 // apply sample filter, subset and convert genotypes process subset_vcf_by_location_whg { label &quot;L_28g5h_subset_vcf_whg&quot; input: set val( loc ), vcfId, file( vcf ), val( mode ), val( sample_mode ) from vcf_fasttree_whg_location_combo output: set val( mode ), val( loc ), val( sample_mode ), file( &quot;${loc}.${mode}.${sample_mode}.whg.geno.gz&quot; ) into snp_geno_tree_whg script: &quot;&quot;&quot; DROP_CHRS=&quot; &quot; # check if samples need to be dropped based on location if [ &quot;${loc}&quot; == &quot;all&quot; ];then vcfsamplenames ${vcf[0]} &gt; prep.pop else vcfsamplenames ${vcf[0]} | \\ grep ${loc} &gt; prep.pop fi # check if outgroups need to be dropped if [ &quot;${sample_mode}&quot; == &quot;all&quot; ];then mv prep.pop ${loc}.pop else cat prep.pop | \\ grep -v tor | \\ grep -v tab &gt; ${loc}.pop fi # check if diverged LGs need to be dropped if [ &quot;${mode}&quot; == &quot;no_musks&quot; ];then DROP_CHRS=&quot;--not-chr LG04 --not-chr LG07 --not-chr LG08 --not-chr LG09 --not-chr LG12 --not-chr LG17 --not-chr LG23&quot; fi vcftools --gzvcf ${vcf[0]} \\ --keep ${loc}.pop \\ \\$DROP_CHRS \\ --mac 3 \\ --recode \\ --stdout | gzip &gt; ${loc}.${mode}.${sample_mode}.vcf.gz python \\$SFTWR/genomics_general/VCF_processing/parseVCF.py \\ -i ${loc}.${mode}.${sample_mode}.vcf.gz | gzip &gt; ${loc}.${mode}.${sample_mode}.whg.geno.gz &quot;&quot;&quot; } The subset is converted to fasta format, creating two whole genome pseudo haplotypes per sample. // git 5.7 // convert genotypes to fasta process fasttree_whg_prep { label &apos;L_190g4h_fasttree_whg_prep&apos; tag &quot;${mode} - ${loc} - ${sample_mode}&quot; input: set val( mode ), val( loc ), val( sample_mode ), file( geno ) from snp_geno_tree_whg output: set val( mode ), val( loc ), val( sample_mode ), file( &quot;all_samples.${loc}.${mode}.${sample_mode}.whg.SNP.fa&quot; ) into ( fasttree_whg_prep_ch ) script: &quot;&quot;&quot; python \\$SFTWR/genomics_general/genoToSeq.py -g ${geno} \\ -s all_samples.${loc}.${mode}.${sample_mode}.whg.SNP.fa \\ -f fasta \\ --splitPhased &quot;&quot;&quot; } Then, the phylogeny is reconstucted based on the converted geneotypes. // git 5.8 // create phylogeny process fasttree_whg_run { label &apos;L_300g30h_fasttree_run&apos; tag &quot;${mode} - ${loc} - ${sample_mode}&quot; publishDir &quot;../../2_analysis/fasttree/&quot;, mode: &apos;copy&apos; input: set val( mode ), val( loc ), val( sample_mode ), file( fa ) from fasttree_whg_prep_ch output: file( &quot;${sample_mode}.${loc}.${mode}.SNP.tree&quot; ) into ( fasttree_whg_output ) script: &quot;&quot;&quot; fasttree -nt ${fa} &gt; ${sample_mode}.${loc}.${mode}.SNP.tree &quot;&quot;&quot; } 6.2.1 Topology weighting The complexety of topology weighting increases non-linearely with the number of included populations as the number of possible unrooted topologies skyrockets. The maximum number of possible populations allowed within twisst is eight, so we are running the topology weighting for Belize and Honduras independently (for the three species at Panama only one unrooted topology is possible, so we don’t run twisst here). We start by setting up a channel for the sampling location. // git 5.9 // initialize the locations for topology weighting Channel .from( &quot;bel&quot;, &quot;hon&quot; ) .set{ locations_ch } Then, we attach the genotypes to the location. // git 5.10 locations_ch .combine( vcf_locations ) .set{ vcf_location_combo } The anlysis is split by linkage group, so we need to initialize the LGs. // git 5.11 // initialize LGs Channel .from( (&apos;01&apos;..&apos;09&apos;) + (&apos;10&apos;..&apos;19&apos;) + (&apos;20&apos;..&apos;24&apos;) ) .map{ &quot;LG&quot; + it } .set{ lg_twisst } The data is subset to include only the samples of the respective location. // git 5.12 // subset the genotypes by location process subset_vcf_by_location { label &quot;L_20g2h_subset_vcf&quot; input: set val( loc ), vcfId, file( vcf ) from vcf_location_combo output: set val( loc ), file( &quot;${loc}.vcf.gz&quot; ), file( &quot;${loc}.pop&quot; ) into ( vcf_loc_twisst ) script: &quot;&quot;&quot; vcfsamplenames ${vcf[0]} | \\ grep ${loc} | \\ grep -v tor | \\ grep -v tab &gt; ${loc}.pop vcftools --gzvcf ${vcf[0]} \\ --keep ${loc}.pop \\ --mac 3 \\ --recode \\ --stdout | gzip &gt; ${loc}.vcf.gz &quot;&quot;&quot; } While runing twisst, we ran into an issues regarding incompatibilities of the used scheduling system used on our computing cluster and the threading within the python scripts used in the preparation of twisst. Unfortunately, this prevented us running the preparation in place (as part of the nextflow script). Instead, we rand the preparation separately on a local computer and clumsitly plugged the results into the nextflow process. Below we include the originally intended workflow which we muted so that the script is runnable using the plugg in approach. Still, the the original workflow describes the steps excuted locally and convays the intermediate steps more clearly. Also, on a different computer cluster, the original script should work allright. The next step is to attach the LGs to the genotype subsets. // --------------------------------------------------------------- // Unfortunately the twisst preparation did not work on the cluster // (&apos;in place&apos;), so I had to setup the files locally and then plug // them into this workflow. // Below is the originally intended clean workflow (commented out), // while the plugin vesion picks up at git 5.19. // --------------------------------------------------------------- /* MUTE: // git 5.13 // add the lg channel to the genotype subset vcf_loc_twisst .combine( lg_twisst ) .set{ vcf_loc_lg_twisst } */ Based on the LGs the genotypes are split. /* MUTE: python thread conflict - run locally and feed into ressources/plugin // git 5.14 // subset genotypes by LG process vcf2geno_loc { label &apos;L_20g15h_vcf2geno&apos; input: set val( loc ), file( vcf ), file( pop ), val( lg ) from vcf_loc_lg_twisst output: set val( loc ), val( lg ), file( &quot;${loc}.${lg}.geno.gz&quot; ), file( pop ) into snp_geno_twisst script: &quot;&quot;&quot; vcftools \\ --gzvcf ${vcf} \\ --chr ${lg} \\ --recode \\ --stdout | gzip &gt; intermediate.vcf.gz python \\$SFTWR/genomics_general/VCF_processing/parseVCF.py \\ -i intermediate.vcf.gz | gzip &gt; ${loc}.${lg}.geno.gz &quot;&quot;&quot; } */ We initialize the window size size (as SNPs) used for the topology weighting… /* MUTE: python thread conflict - run locally and feed into ressources/plugin // git 5.15 // initialize SNP window size Channel.from( 50, 200 ).set{ twisst_window_types } */ …and attach them to the genotypes. /* MUTE: python thread conflict - run locally and feed into ressources/plugin // git 5.16 // add the SNP window size to the genotype subset snp_geno_twisst.combine( twisst_window_types ).set{ twisst_input_ch } */ To conduct topology weighting, we need some undelying phylogenies, so we run PhyML along the sliding window. /* MUTE: python thread conflict - run locally and feed into ressources/plugin // git 5.17 // create the phylogenies along the sliding window process twisst_prep { label &apos;L_G120g40h_prep_twisst&apos; publishDir &quot;../../2_analysis/twisst/positions/${loc}/&quot;, mode: &apos;copy&apos; input: set val( loc ), val( lg ), file( geno ), file( pop ), val( twisst_w ) from twisst_input_ch.filter { it[0] != &apos;pan&apos; } output: set val( loc ), val( lg ), file( geno ), file( pop ), val( twisst_w ), file( &quot;*.trees.gz&quot; ), file( &quot;*.data.tsv&quot; ) into twisst_prep_ch script: &quot;&quot;&quot; module load intel17.0.4 intelmpi17.0.4 mpirun \\$NQSII_MPIOPTS -np 1 \\ python \\$SFTWR/genomics_general/phylo/phyml_sliding_windows.py \\ -g ${geno} \\ --windType sites \\ -w ${twisst_w} \\ --prefix ${loc}.${lg}.w${twisst_w}.phyml_bionj \\ --model HKY85 \\ --optimise n \\ --threads 1 &quot;&quot;&quot; } */ The last step then is to run twisst prepared phylogenies. /* MUTE: python thread conflict - run locally and feed into ressources/plugin // git 5.18 // run the topology weighting on the phylogenies process twisst_run { label &apos;L_G120g40h_run_twisst&apos; publishDir &quot;../../2_analysis/twisst/weights/&quot;, mode: &apos;copy&apos; input: set val( loc ), val( lg ), file( geno ), file( pop ), val( twisst_w ), file( tree ), file( data ) from twisst_prep_ch output: set val( loc ), val( lg ), val( twisst_w ), file( &quot;*.weights.tsv.gz&quot; ), file( &quot;*.data.tsv&quot; ) into ( twisst_output ) script: &quot;&quot;&quot; module load intel17.0.4 intelmpi17.0.4 awk &apos;{print \\$1&quot;\\\\t&quot;\\$1}&apos; ${pop} | \\ sed &apos;s/\\\\(...\\\\)\\\\(...\\\\)\\$/\\\\t\\\\1\\\\t\\\\2/g&apos; | \\ cut -f 1,3 | \\ awk &apos;{print \\$1&quot;_A\\\\t&quot;\\$2&quot;\\\\n&quot;\\$1&quot;_B\\\\t&quot;\\$2}&apos; &gt; ${loc}.${lg}.twisst_pop.txt TWISST_POPS=\\$( cut -f 2 ${loc}.${lg}.twisst_pop.txt | sort | uniq | paste -s -d&apos;,&apos; | sed &apos;s/,/ -g /g; s/^/-g /&apos; ) mpirun \\$NQSII_MPIOPTS -np 1 \\ python \\$SFTWR/twisst/twisst.py \\ --method complete \\ -t ${tree} \\ -T 1 \\ \\$TWISST_POPS \\ --groupsFile ${loc}.${lg}.twisst_pop.txt | \\ gzip &gt; ${loc}.${lg}.w${twisst_w}.phyml_bionj.weights.tsv.gz &quot;&quot;&quot; } */ Here, the plug in approach picks up. At this point we have run PhyML locally and deposited the results under $BASE_DIR/ressources/plugin/trees. To restart, we need to emulate the settings needed for the twisst process. // git 5.19 // emmulate setting Channel .from(50, 200) .combine( vcf_loc_twisst ) .combine( lg_twisst ) .set{ twisst_modes } Then, we feed the phylogenies from an external directory into twisst. // git 5.20 // run the topology weighting on the phylogenies process twisst_plugin { label &apos;L_G120g40h_twisst_plugin&apos; publishDir &quot;../../2_analysis/twisst/weights/&quot;, mode: &apos;copy&apos; tag &quot;${loc}-${lg}-${mode}&quot; input: set val( mode ), val( loc ), file( vcf ), file( pop ), val( lg ) from twisst_modes output: set val( loc ), val( lg ), file( &quot;*.weights.tsv.gz&quot; ) into ( twisst_output ) script: &quot;&quot;&quot; module load intel17.0.4 intelmpi17.0.4 awk &apos;{print \\$1&quot;\\\\t&quot;\\$1}&apos; ${pop} | \\ sed &apos;s/\\\\(...\\\\)\\\\(...\\\\)\\$/\\\\t\\\\1\\\\t\\\\2/g&apos; | \\ cut -f 1,3 | \\ awk &apos;{print \\$1&quot;_A\\\\t&quot;\\$2&quot;\\\\n&quot;\\$1&quot;_B\\\\t&quot;\\$2}&apos; &gt; ${loc}.${lg}.twisst_pop.txt TWISST_POPS=\\$( cut -f 2 ${loc}.${lg}.twisst_pop.txt | sort | uniq | paste -s -d&apos;,&apos; | sed &apos;s/,/ -g /g; s/^/-g /&apos; ) mpirun \\$NQSII_MPIOPTS -np 1 \\ python \\$SFTWR/twisst/twisst.py \\ --method complete \\ -t \\$BASE_DIR/ressources/plugin/trees/${loc}/${loc}.${lg}.w${mode}.phyml_bionj.trees.gz \\ \\$TWISST_POPS \\ --groupsFile ${loc}.${lg}.twisst_pop.txt | \\ gzip &gt; ${loc}.${lg}.w${mode}.phyml_bionj.weights.tsv.gz &quot;&quot;&quot; } At this step we are done with phlyogeny and the topology weighting. "],
["analysis-iv-rho.html", "7 Analysis IV (rho) 7.1 Summary 7.2 Details of analysis_recombination.nf", " 7 Analysis IV (rho) 7.1 Summary The population recombination rate is estimated within the nextflow script analysis_recombination.nf (located under $BASE_DIR/nf/analysis_recombination/), which runs on the SNPs only data set. Below is an overview of the steps involved in the genotyping analysis. (The green dot indicates the genotype input, red arrows depict output that is exported for further use.) 7.2 Details of analysis_recombination.nf 7.2.1 Data preparation The nextflow script starts by opening the genotype data. #!/usr/bin/env nextflow // This pipelie includes the recombination anlysis // git 6.1 // load genotypes Channel .fromFilePairs(&quot;../../1_genotyping/4_phased/phased_mac2.vcf.{gz,gz.tbi}&quot;) .set{ vcf_ch } The estimation of the population recombination rate using FastEPRR happens in three steps. The first step is run independently for all linkage groups, so we set up a channel for the LGs. // git 6.2 // initialize LGs Channel .from( 1..24 ) .map{ it.toString().padLeft(2, &quot;0&quot;) } .set{ lg_ch } To prepare the input, the genotypes are split by LG. // git 6.3 // split genotypes by LG process split_allBP { label &apos;L_20g2h_split_by_lg&apos; tag &quot;LG${lg}&quot; input: set val( lg ), vcfId, file( vcf ) from lg_ch.combine( vcf_ch ) output: set val( lg ), file( &quot;phased_mac2.LG${lg}.vcf.gz&quot; ) into vcf_by_lg_ch script: &quot;&quot;&quot; module load openssl1.0.2 vcftools --gzvcf ${vcf[0]} \\ --chr LG${lg} \\ --recode \\ --stdout | bgzip &gt; phased_mac2.LG${lg}.vcf.gz &quot;&quot;&quot; } The prepared data is then fed to the first step of FastEPRR. // git 6.4 // run fasteprr step 1 process fasteprr_s1 { label &apos;L_20g2h_fasteprr_s1&apos; tag &quot;LG${lg}&quot; module &quot;R3.5.2&quot; input: set val( lg ), file( vcf ) from vcf_by_lg_ch output: file( &quot;step1_LG${lg}&quot; ) into step_1_out_ch script: &quot;&quot;&quot; mkdir step1_LG${lg} Rscript --vanilla \\$BASE_DIR/R/fasteprr_step1.R ./${vcf} step1_LG${lg} LG${lg} 50 &quot;&quot;&quot; } Since nextflow manages the results of its processes in a complex file structure, we need to collect all results of step 1 and bundle them before proceeding. // git 6.5 // collect step 1 output process fasteprr_s1_summary { label &apos;L_loc_fasteprr_s1_summmary&apos; input: file( step1 ) from step_1_out_ch.collect() output: file( &quot;step1&quot; ) into ( step1_ch1, step1_ch2 ) script: &quot;&quot;&quot; mkdir step1 cp step1_LG*/* step1/ &quot;&quot;&quot; } The second step of FastEPRR is parallelized over an arbitrary number of subprocesses. Here, we initialize 250 parallel processes and combine the paralleliyation index with the results from step 1. // git 6.6 // initialize fasteperr subprocesses and attach them to step 1 output Channel .from( 1..250 ) .map{ it.toString().padLeft(3, &quot;0&quot;) } .combine( step1_ch1 ) .set{ step_2_run_ch } Taking this prepared bundle, we now can start the second step of FastEPRR. // git 6.7 // run fasteprr step 2 process fasteprr_s2 { label &apos;L_long_loc_fasteprr_s2&apos; tag &quot;run_${idx}&quot; module &quot;R3.5.2&quot; input: set val( idx ), file( step1 ) from step_2_run_ch output: set val( idx ), file( &quot;step2_run${idx}&quot; ) into step_2_out_ch script: &quot;&quot;&quot; mkdir -p step2_run${idx} Rscript --vanilla \\$BASE_DIR/R/fasteprr_step2.R ${step1} step2_run${idx} ${idx} &quot;&quot;&quot; } The file management of nextflow can be a bit complicated at times, so here duplicate the ouput of step 2 to later easily acess the paralleliyation indicees and the actual output. // git 6.8 // clone step 2 output step_2_out_ch.into{ step_2_indxs; step_2_files } We collect both clones of the step 2 results and bundle the results in a single directory. // git 6.9 // collect step 2 output process fasteprr_s2_summary { label &apos;L_loc_fasteprr_s2_summmary&apos; input: val( idx ) from step_2_indxs.map{ it[0] }.collect() file( files ) from step_2_files.map{ it[1] }.collect() output: file( &quot;step2&quot; ) into ( step2_ch ) script: &quot;&quot;&quot; mkdir step2 for k in \\$( echo ${idx} | sed &apos;s/\\\\[//g; s/\\\\]//g; s/,//g&apos;); do cp -r step2_run\\$k/* step2/ done &quot;&quot;&quot; } Then we feed the bundled results into the third step of FastEPRR. // git 6.10 // run fasteprr step 3 process fasteprr_s3 { label &apos;L_32g4h_fasteprr_s3&apos; module &quot;R3.5.2&quot; input: set file( step1 ), file( step2 ) from step1_ch2.combine( step2_ch ) output: file( &quot;step3&quot; ) into step_3_out_ch script: &quot;&quot;&quot; mkdir step3 Rscript --vanilla \\$BASE_DIR/R/fasteprr_step3.R ${step1} ${step2} step3 &quot;&quot;&quot; } To ease the usage of the FastEPRR results downstream, we reformat them and compile a tidy table. // git 6.11 // reformat overall fasteprr output process fasteprr_s3_summary { label &apos;L_loc_fasteprr_s3_summmary&apos; publishDir &quot;../../2_analysis/fasteprr&quot;, mode: &apos;copy&apos; input: file( step3 ) from step_3_out_ch output: file( &quot;step4/fasteprr.all.rho.txt.gz&quot; ) into ( step3_ch ) script: &quot;&quot;&quot; mkdir step4 # ------ rewriting the fasteprr output into tidy format -------- for k in {01..24};do j=&quot;LG&quot;\\$k; echo \\$j; \\$BASE_DIR/sh/fasteprr_trans.sh step3/chr_\\$j \\$j step4/fasteprr.\\$j done # --------- combining all LGs into a single data set ----------- cd step4 head -n 1 fasteprr.LG01.rho.txt &gt; fasteprr.all.rho.txt for k in {01..24}; do echo &quot;LG&quot;\\$k awk &apos;NR&gt;1{print \\$0}&apos; fasteprr.LG\\$k.rho.txt &gt;&gt; fasteprr.all.rho.txt done gzip fasteprr.all.rho.txt cd .. &quot;&quot;&quot; } Finally, we are done with recombination rate. "],
["figure-1.html", "8 Figure 1 8.1 Summary 8.2 Details of plot_F1.R", " 8 Figure 1 8.1 Summary This is the accessory documentation of Figure 1. The Figure can be recreated by running the R script plot_F1.R: cd $BASE_DIR Rscript --vanilla R/fig/plot_F1.R 2_analysis/dxy/50k/ 2_analysis/fst/50k/ 8.2 Details of plot_F1.R In the following, the individual steps of the R script are documented. It is an executable R script that depends on the accessory R package GenomicOriginsScripts. 8.2.1 Config The scripts start with a header that contains copy &amp; paste templates to execute or debug the script: #!/usr/bin/env Rscript # run from terminal: # Rscript --vanilla R/fig/plot_F1.R 2_analysis/dxy/50k/ 2_analysis/fst/50k/ # =============================================================== # This script produces Figure 1 of the study &quot;The genomic origins of a marine radiation&quot; # by Hench, McMillan an Puebla # --------------------------------------------------------------- # =============================================================== # args &lt;- c(&#39;2_analysis/dxy/50k/&#39;, &#39;2_analysis/fst/50k/&#39;) The next section processes the input from the command line. It stores the arguments in the vector args. The R package GenomicOriginsScripts is loaded and the script name and the current working directory are stored inside variables (script_name, plot_comment). This information will later be written into the meta data of the figure to help us tracing back the scripts that created the figures in the future. Then we drop all the imported information besides the arguments following the script name and print the information to the terminal. args = commandArgs(trailingOnly=FALSE) # setup ----------------------- library(GenomicOriginsScripts) library(hypoimg) cat(&#39;\\n&#39;) script_name &lt;- args[5] %&gt;% str_remove(.,&#39;--file=&#39;) plot_comment &lt;- script_name %&gt;% str_c(&#39;mother-script = &#39;,getwd(),&#39;/&#39;,.) args &lt;- process_input(script_name, args) #&gt; ── Script: scripts/plot_F1.R ──────────────────────────────────────────── #&gt; Parameters read: #&gt; ★ 1: 2_analysis/dxy/50k/ #&gt; ★ 2: 2_analysis/fst/50k/ #&gt; ─────────────────────────────────────────── /current/working/directory ── The directories for the different data types are received and stored in respective variables. Also, we set a few parameters for the plot layout: # config ----------------------- dxy_dir &lt;- as.character(args[1]) fst_dir &lt;- as.character(args[2]) wdh &lt;- .3 # The width of the boxplots scaler &lt;- 20 # the ratio of the Fst and the dxy axis clr_sec &lt;- &#39;gray&#39; # the color of the secondary axis (dxy) 8.2.2 Data import We begin with the data import by first collecting the paths to all files containing either FST or dXY data (dir()), then iterating the import function over all files (map(summarize_fst)) and finally combining the outputs into a single tibble (bind_rows()). This is done for both FST and dXY. # start script ------------------- # import Fst fst_files &lt;- dir(fst_dir, pattern = &#39;.50k.windowed.weir.fst.gz&#39;) fst_data &lt;- str_c(fst_dir,fst_files) %&gt;% purrr::map(summarize_fst) %&gt;% bind_rows() # import dxy dxy_files &lt;- dir(dxy_dir) dxy_data &lt;- str_c(dxy_dir,dxy_files) %&gt;% purrr::map(summarize_dxy) %&gt;% bind_rows() We use the genome wide average FST to rank the individual pair wise comparisons. fst_order &lt;- fst_data %&gt;% select(run, `mean_weighted-fst`) %&gt;% mutate(run = fct_reorder(run,`mean_weighted-fst`)) Then, we merge the FST and dXY data sets and do quite a bit of data wrangling to create a rescaled dXY value and to prepare the placement of the boxplots. data &lt;- left_join(fst_data, dxy_data) %&gt;% select(c(8,1:7,9:15)) %&gt;% # transfrom table from short to long format gather(key = &#39;stat&#39;, value = &#39;val&#39;,2:15) %&gt;% # separating the boxplot element from the population genetic parameter separate(stat, into = c(&#39;sumstat&#39;,&#39;popstat&#39;),sep = &#39;_&#39;) %&gt;% # rescaleing the dxy values to fall in the range of Fst mutate(val_scaled = ifelse(popstat == &#39;dxy&#39;, val * scaler , val)) %&gt;% # combining original and rescaled values unite(temp, val, val_scaled) %&gt;% # back transforming to short format spread(.,key = &#39;sumstat&#39;,value = &#39;temp&#39;) %&gt;% # separating original and rescaled values for all boxplot elements separate(mean, into = c(&#39;mean&#39;,&#39;mean_scaled&#39;),sep = &#39;_&#39;, convert = TRUE) %&gt;% separate(median, into = c(&#39;median&#39;,&#39;median_scaled&#39;),sep = &#39;_&#39;, convert = TRUE) %&gt;% separate(sd, into = c(&#39;sd&#39;,&#39;sd_scaled&#39;),sep = &#39;_&#39;, convert = TRUE) %&gt;% separate(lower, into = c(&#39;lower&#39;,&#39;lower_scaled&#39;),sep = &#39;_&#39;, convert = TRUE) %&gt;% separate(upper, into = c(&#39;upper&#39;,&#39;upper_scaled&#39;),sep = &#39;_&#39;, convert = TRUE) %&gt;% separate(lowpoint, into = c(&#39;lowpoint&#39;,&#39;lowpoint_scaled&#39;),sep = &#39;_&#39;, convert = TRUE) %&gt;% separate(highpoint, into = c(&#39;highpoint&#39;,&#39;highpoint_scaled&#39;),sep = &#39;_&#39;, convert = TRUE) %&gt;% # adding preparations for plotting mutate(loc = str_sub(run,4,6), # extract location from run run = factor(run, levels = levels(fst_order$run)), # introduce the run ranking x = as.numeric(run) , # extract rank of runs (base x) x_dodge = ifelse(popstat == &#39;dxy&#39;,x + .25,x - .25), # shift dxy to the right, fst left x_start_dodge = x_dodge - wdh/2, # boxplot left border x_end_dodge = x_dodge + wdh/2, # boxplot right border popstat_loc = str_c(popstat,&#39;[&#39;,loc,&#39;]&#39;)) # parameter X location (for color) At this point, the data is ready for the boxplots. But first we are going to prepare the networks of pairwise comparisons. For this we create a tibble of the runs with their respective rank. Then, we prepare a config table with one row per location, storing the parameters neede for the layout function for the networks. We need to define the location, the number of species at the location, the short three letter ID of those species and a wheight parameter that is shifting the comparison label on the link within the networks. Finally, we create one network plot per location. # preparing a run/rnk lookup table run_ord &lt;- tibble(run = levels(data$run), run_ord = 1:length(levels(data$run))) # network config and layout networx &lt;- tibble( loc = c(&#39;bel&#39;,&#39;hon&#39;, &#39;pan&#39;), n = c(5,6,3), label = list(str_c(c(&#39;ind&#39;,&#39;may&#39;,&#39;nig&#39;,&#39;pue&#39;,&#39;uni&#39;),&#39;bel&#39;), str_c(c(&#39;abe&#39;,&#39;gum&#39;,&#39;nig&#39;,&#39;pue&#39;,&#39;ran&#39;,&#39;uni&#39;),&#39;hon&#39;), str_c(c(&#39;nig&#39;,&#39;pue&#39;,&#39;uni&#39;),&#39;pan&#39;)), weight = c(1,1.45,1)) %&gt;% purrr::pmap(network_layout) %&gt;% bind_rows() plot_list &lt;- networx %&gt;% purrr::pmap(plot_network, node_lab_shift = .2) 8.2.3 Plotting To create the first panel of Figure 1, we combine the three networks and label the locations. p1 &lt;- cowplot::plot_grid( grid::textGrob(&#39;Belize&#39;), grid::textGrob(&#39;Honduras&#39;), grid::textGrob(&#39;Panama&#39;), plot_list[[1]], plot_list[[2]], plot_list[[3]], ncol = 3, rel_heights = c(.1,1)) Now, we can create the second panel of Figure 1, by plotting our prepared data tibble. We are going to plot each boxplot element as a single layer. (This, might seem a little cumbersome given geom_boxplot(), but this approach was choosen for specific fine tuning of the postioning, dropping of outliers and reducing runtime during the plotting phase - otherwise the entire genome wide data set would have been carried though whole script.) p2 &lt;- data %&gt;% ggplot(aes(color = popstat_loc)) + # adding whiskers geom_segment(aes(x = x_dodge, xend = x_dodge, y = lowpoint_scaled,yend = highpoint_scaled)) + # adding box (inter quartile range) geom_rect(aes(xmin = x_start_dodge, xmax = x_end_dodge, ymin = lower_scaled, ymax = upper_scaled), fill=&#39;white&#39;)+ # adding bar (median) geom_segment(aes(x = x_start_dodge, xend = x_end_dodge, y = median_scaled, yend = median_scaled),lwd = .9)+ # adding point (mean) geom_point(aes(x = x_dodge, y = mean_scaled),shape = 21, size = .7, fill = &#39;white&#39;)+ # layout x ayis scale_x_continuous(breaks = 1:28) + # layout primary y axis (Fst) scale_y_continuous(breaks = c(0,.05,.1,.15), name = expression(italic(F[ST])), # layout secondary y axis (dxy) sec.axis = sec_axis(~ . /scaler, name = expression(italic(d[XY])), breaks = c(0,.005,.01)))+ # define color scheme scale_color_manual(values = c(make_faint_clr(&#39;bel&#39;), make_faint_clr(&#39;hon&#39;), make_faint_clr(&#39;pan&#39;))[c(1,3,5,2,4,6)])+ # set plot range coord_cartesian(xlim = c(-1,29), expand = c(0,0))+ # tune plot appreance theme_minimal()+ theme(axis.title.x = element_blank(), legend.position = &#39;none&#39;, strip.placement = &#39;outside&#39;, strip.text = element_text(size = 12), panel.grid.major.x = element_blank(), panel.grid.minor.y = element_blank(), axis.text.y.right = element_text(color = clr_sec), axis.title.y.right = element_text(color = clr_sec)) To create the final figure, we combine the two sub panels unsing the cowplot package. Here, we add the panel labels and define the space each panel is going to take. p_done &lt;- cowplot::plot_grid(p1,p2, ncol = 1, rel_heights = c(.7,1), labels = letters[1:2] %&gt;% project_case()) Finally, we can export Figure 1. hypo_save(p_done, filename = &#39;figures/F1.pdf&#39;, width = 9, height = 7, comment = plot_comment) The function hypo_save() is simply a wrapper around ggsave(), that will write the name of the currently running script into the meta data of the plot (after the plot has been exported). The benfit of this is that you can read this information later to remember how a specifc plot was created using hypo_show_metadata(). This is done using exiftool and has currently only been tested on my linux system. If this does not work for you, simple replace hypo_save() with ggsave() and drop the comment parameter. hypo_show_metadata(&#39;figures/F1.pdf&#39;) #&gt; [1] &quot;mother-script = /path/to/scripts/plot_F1.R&quot; "],
["figure-2.html", "9 Figure 2 9.1 Summary 9.2 Details of plot_F2.R", " 9 Figure 2 9.1 Summary This is the accessory documentation of Figure 2. The Figure can be recreated by running the R script plot_F2.R: cd $BASE_DIR Rscript --vanilla R/fig/plot_F2.R 2_analysis/dxy/50k/ \\ 2_analysis/fst/50k/multi_fst.50k.tsv.gz 2_analysis/GxP/50000/ \\ 2_analysis/summaries/fst_outliers_998.tsv \\ https://raw.githubusercontent.com/simonhmartin/twisst/master/plot_twisst.R \\ 2_analysis/twisst/weights/ ressources/plugin/trees/ \\ 2_analysis/fasteprr/step4/fasteprr.all.rho.txt.gz \\ 2_analysis/summaries/fst_globals.txt 9.2 Details of plot_F2.R In the following, the individual steps of the R script are documented. It is an executable R script that depends on the accessory R package GenomicOriginsScripts. 9.2.1 Config The scripts start with a header that contains copy &amp; paste templates to execute or debug the script: #!/usr/bin/env Rscript # run from terminal: # Rscript --vanilla R/fig/plot_F2.R 2_analysis/dxy/50k/ \\ # 2_analysis/fst/50k/multi_fst.50k.tsv.gz 2_analysis/GxP/50000/ \\ # 2_analysis/summaries/fst_outliers_998.tsv \\ # https://raw.githubusercontent.com/simonhmartin/twisst/master/plot_twisst.R \\ # 2_analysis/twisst/weights/ ressources/plugin/trees/ \\ # 2_analysis/fasteprr/step4/fasteprr.all.rho.txt.gz \\ # 2_analysis/summaries/fst_globals.txt # =============================================================== # This script produces Figure 2 of the study &quot;The genomic origins of a marine radiation&quot; # by Hench, McMillan an Puebla # --------------------------------------------------------------- # =============================================================== # args &lt;- c(&#39;2_analysis/dxy/50k/&#39;,&#39;2_analysis/fst/50k/multi_fst.50k.tsv.gz&#39;, # &#39;2_analysis/GxP/50000/&#39;, &#39;2_analysis/summaries/fst_outliers_998.tsv&#39;, # &#39;https://raw.githubusercontent.com/simonhmartin/twisst/master/plot_twisst.R&#39;, # &#39;2_analysis/twisst/weights/&#39;, &#39;ressources/plugin/trees/&#39;, # &#39;2_analysis/fasteprr/step4/fasteprr.all.rho.txt.gz&#39;, &#39;2_analysis/summaries/fst_globals.txt&#39;) The next section processes the input from the command line. It stores the arguments in the vector args. The R package GenomicOriginsScripts is loaded and the script name and the current working directory are stored inside variables (script_name, plot_comment). This information will later be written into the meta data of the figure to help us tracing back the scripts that created the figures in the future. Then we drop all the imported information besides the arguments following the script name and print the information to the terminal. args = commandArgs(trailingOnly=FALSE) # setup ----------------------- library(GenomicOriginsScripts) library(hypoimg) cat(&#39;\\n&#39;) script_name &lt;- args[5] %&gt;% str_remove(.,&#39;--file=&#39;) plot_comment &lt;- script_name %&gt;% str_c(&#39;mother-script = &#39;,getwd(),&#39;/&#39;,.) args &lt;- process_input(script_name, args) #&gt; ── Script: scripts/plot_F2.R ──────────────────────────────────────────── #&gt; Parameters read: #&gt; ★ 1: 2_analysis/dxy/50k/ #&gt; ★ 2: 2_analysis/fst/50k/multi_fst.50k.tsv.gz #&gt; ★ 3: 2_analysis/GxP/50000/ #&gt; ★ 4: 2_analysis/summaries/fst_outliers_998.tsv #&gt; ★ 5: https://raw.githubusercontent.com/simonhmartin/twisst/master/plot_twisst.R #&gt; ★ 6: 2_analysis/twisst/weights/ #&gt; ★ 7: ressources/plugin/trees/ #&gt; ★ 8: 2_analysis/fasteprr/step4/fasteprr.all.rho.txt.gz #&gt; ★ 9: 2_analysis/summaries/fst_globals.txt #&gt; ─────────────────────────────────────────── /current/working/directory ── The directories for the different data types are received and stored in respective variables. Also, we source an external r script from the original twisst github repository that we need to import the twisst data: # config ----------------------- dxy_dir &lt;- as.character(args[1]) fst_file &lt;- as.character(args[2]) gxp_dir &lt;- as.character(args[3]) outlier_table &lt;- as.character(args[4]) twisst_script &lt;- as.character(args[5]) w_path &lt;- as.character(args[6]) d_path &lt;- as.character(args[7]) recombination_file &lt;- as.character(args[8]) global_fst_file &lt;- as.character(args[9]) source(twisst_script) 9.2.2 Data import Figure 2 contains wuite a lot of different data sets. The main part of this script is just importing and organizing all of this data: In the following we’ll go step by step through the import of: differentiation data (\\(F_{ST}\\)) divergence data (\\(d_{XY}\\), also containing diversity data - \\(\\pi\\)) genotype \\(\\times\\) phenotype association data (\\(p_{Wald}\\)) recombination data (\\(\\rho\\)) topology weighting data We start with the import of the \\(F_{ST}\\) data, specifically the data set containing the genome wide \\(F_{ST}\\) computed for all populations simultaneously (joint \\(F_{ST}\\)). The data file is read, the columns are renamed and the genomic position is added. Then, only the genomic position and the \\(F_{ST}\\) columns are selected and a window column is added for facetting in ggplot(). # start script ------------------- # import fst data fst_data &lt;- vroom::vroom(fst_file,delim = &#39;\\t&#39;) %&gt;% select(CHROM, BIN_START, BIN_END, N_VARIANTS, WEIGHTED_FST) %&gt;% setNames(., nm = c(&#39;CHROM&#39;,&#39;BIN_START&#39;, &#39;BIN_END&#39;, &#39;n_snps&#39;, &#39;fst&#39;) ) %&gt;% add_gpos() %&gt;% select(GPOS, fst) %&gt;% setNames(., nm = c(&#39;GPOS&#39;,&#39;value&#39;)) %&gt;% mutate(window = str_c(&#39;bold(&#39;,project_case(&#39;a&#39;),&#39;):joint~italic(F[ST])&#39;)) Next, we import the \\(d_{XY}\\) data. Here we are importing all 28 pairwise comparisons, so we first collect all the file paths and the iterate the data import over all files. # import dxy data dxy_files &lt;- dir(dxy_dir) dxy_data &lt;- str_c(dxy_dir,dxy_files) %&gt;% purrr::map(get_dxy) %&gt;% bind_rows() %&gt;% select(N_SITES:GPOS, run) %&gt;% mutate(pop1 = str_sub(run,1,6), pop2 = str_sub(run,8,13)) From this data, we compute the divergence difference (\\(\\Delta d_{XY}\\)). dxy_summary &lt;- dxy_data %&gt;% group_by(GPOS) %&gt;% summarise(delta_dxy = max(dxy)-min(dxy), sd_dxy = sd(dxy), delt_pi = max(c(max(PI_POP1),max(PI_POP2))) - min(c(min(PI_POP1),min(PI_POP2)))) %&gt;% ungroup() %&gt;% setNames(., nm = c(&#39;GPOS&#39;, str_c(&#39;bold(&#39;,project_case(&#39;e&#39;),&#39;):Delta~italic(d[xy])&#39;), str_c(&#39;bold(&#39;,project_case(&#39;e&#39;),&#39;):italic(d[xy])~(sd)&#39;), str_c(&#39;bold(&#39;,project_case(&#39;e&#39;),&#39;):Delta~italic(pi)&#39;))) %&gt;% gather(key = &#39;window&#39;, value = &#39;value&#39;,2:4) %&gt;% filter(window == str_c(&#39;bold(&#39;,project_case(&#39;e&#39;),&#39;):Delta~italic(d[xy])&#39;)) Then we import the genotype \\(\\times\\) phenotype association data. For this, we list all the traits we want to include and then iterate the import funtion over all traits. We combine the data sets and transform the table to long format. # import G x P data traits &lt;- c(&quot;Bars.lm.50k.5k.txt.gz&quot;, &quot;Peduncle.lm.50k.5k.txt.gz&quot;, &quot;Snout.lm.50k.5k.txt.gz&quot;) trait_panels &lt;- c(Bars = str_c(&#39;bold(&#39;,project_case(&#39;h&#39;),&#39;)&#39;), Peduncle = str_c(&#39;bold(&#39;,project_case(&#39;i&#39;),&#39;)&#39;), Snout = str_c(&#39;bold(&#39;,project_case(&#39;j&#39;),&#39;)&#39;)) gxp_data &lt;- str_c(gxp_dir,traits) %&gt;% purrr::map(get_gxp) %&gt;% join_list() %&gt;% gather(key = &#39;window&#39;, value = &#39;value&#39;,2:4) Then, we import the genome wide \\(F_{ST}\\) summary for all 28 pair wise comparisons to be able to pick a divergence data set of an intermediatly differentiated species pair (the species pair of rank 15, close to 14.5 - the median of rank 1 to 28). # import genome wide Fst data summary -------- globals &lt;- vroom::vroom(global_fst_file, delim = &#39;\\t&#39;, col_names = c(&#39;loc&#39;,&#39;run&#39;,&#39;mean&#39;,&#39;weighted&#39;)) %&gt;% mutate(run = str_c(str_sub(run,1,3),loc,&#39;-&#39;,str_sub(run,5,7),loc), run = fct_reorder(run,weighted)) # select dxy data selectors_dxy &lt;- globals %&gt;% arrange(weighted) %&gt;% .$weighted %&gt;% .[15] select_dxy_runs &lt;- globals %&gt;% filter(weighted %in% selectors_dxy) %&gt;% .$run %&gt;% as.character() dxy_select &lt;- dxy_data %&gt;% filter(run %in% select_dxy_runs) %&gt;% mutate(window = str_c(&#39;bold(&#39;,project_case(&#39;d&#39;),&#39;): italic(d[XY])&#39;)) The \\(d_{XY}\\) data set includes also \\(\\pi\\) of the involved populations. We first extract the diversity data for each population (pop1 &amp; pop2), combine them and compute the statistics needed for ranking the populations based on their diversity. # select pi data pi_summary_1 &lt;- dxy_data %&gt;% group_by(pop1,run) %&gt;% summarise(avg_pi = mean(PI_POP1)) %&gt;% ungroup() %&gt;% set_names(., nm = c(&#39;pop&#39;,&#39;run&#39;,&#39;avg_pi&#39;)) pi_summary &lt;- dxy_data %&gt;% group_by(pop2,run) %&gt;% summarise(avg_pi = mean(PI_POP2)) %&gt;% ungroup() %&gt;% set_names(., nm = c(&#39;pop&#39;,&#39;run&#39;,&#39;avg_pi&#39;)) %&gt;% bind_rows(pi_summary_1) %&gt;% group_by(pop) %&gt;% summarise(n = length(pop), mean_pi = mean(avg_pi), min_pi = min(avg_pi), max_pi = max(avg_pi), sd_pi = sd(avg_pi)) %&gt;% arrange(n) Then, we determine an intermediatly diverse candidate of our 14 populations (rank 7, again: \\(7 \\approx median(1:14)\\)) and average over the diversities estimated in all pairwise comparisons this population was involved in. selectors_pi &lt;- pi_summary %&gt;% .$mean_pi %&gt;% sort() %&gt;% .[7] select_pi_pops &lt;- pi_summary %&gt;% filter(mean_pi %in% selectors_pi) %&gt;% .$pop %&gt;% as.character pi_data_select &lt;- dxy_data %&gt;% select(GPOS, PI_POP1, pop1 )%&gt;% set_names(., nm = c(&#39;GPOS&#39;,&#39;pi&#39;,&#39;pop&#39;)) %&gt;% bind_rows(.,dxy_data %&gt;% select(GPOS, PI_POP2, pop2 )%&gt;% set_names(., nm = c(&#39;GPOS&#39;,&#39;pi&#39;,&#39;pop&#39;))) %&gt;% group_by(GPOS,pop) %&gt;% summarise(n = length(pop), mean_pi = mean(pi), min_pi = min(pi), max_pi = max(pi), sd_pi = sd(pi)) %&gt;% filter(pop %in% select_pi_pops) %&gt;% mutate(window = str_c(&#39;bold(&#39;,project_case(&#39;b&#39;),&#39;): italic(pi)&#39;)) The import of the recombination data is pretty sraight foreward: Reading one file, adding genomic position and window coluumn for facetting. # import recombination data recombination_data &lt;- vroom::vroom(recombination_file,delim = &#39;\\t&#39;) %&gt;% add_gpos() %&gt;% mutate(window = str_c(&#39;bold(&#39;,project_case(&#39;c&#39;),&#39;): rho&#39;)) ## Rows: 11,206 ## Cols: 4 ## chr [1]: CHROM ## dbl [3]: BIN_START, BIN_END, RHO ## ## Call `spec()` for a copy-pastable column specification ## Specify the column types with `col_types` to quiet this message Then we import the topology weighting data. This is done once per location, the data sets are combined and specific columns are selected: The gnomic position, the topologie number (format: three digits with leading zeros, hence “topo3”), relative topology rank ranging from 0 to 1, the facceting column annd the actual weight data. We also create a dummy tibble that contains the null expectation of the topology weight for the two locations (1/n, with n = number of possible topologies - n = 15 for Belize and 105 for Honduras). # import topology weighting data twisst_data &lt;- tibble(loc = c(&#39;bel&#39;,&#39;hon&#39;), panel = c(&#39;f&#39;,&#39;g&#39;) %&gt;% project_case() %&gt;% str_c(&#39;bold(&#39;,.,&#39;)&#39;)) %&gt;% purrr::pmap(match_twisst_files) %&gt;% bind_rows() %&gt;% select(GPOS, topo3,topo_rel,window,weight) twisst_null &lt;- tibble(window = c(str_c(&#39;bold(&#39;,project_case(&#39;f&#39;),&#39;):~weighting[bel]&#39;), str_c(&#39;bold(&#39;,project_case(&#39;g&#39;),&#39;):~weighting[hon]&#39;)), weight = c(1/15, 1/105)) We craete a single data set for \\(d_{XY}\\), \\(F_{ST}\\) and genotype \\(\\times\\) phenotype data. # combine data types -------- data &lt;- bind_rows(dxy_summary, fst_data, gxp_data) Then we load the positions of the the \\(F_{ST}\\) outlier windows, select the focal outliers that will receive individual labels and create a tibnble and two parameters for the label placement within the plot. # import fst outliers outliers &lt;- vroom::vroom(outlier_table, delim = &#39;\\t&#39;) outlier_pick &lt;- c(&#39;LG04_1&#39;, &#39;LG12_2&#39;, &#39;LG12_3&#39;) outlier_label &lt;- outliers %&gt;% filter(gid %in% outlier_pick) %&gt;% mutate(label = letters[row_number()] %&gt;% project_inv_case(), x_shift_label = c(-1,-1.2,1)*10^7, gpos_label = gpos + x_shift_label, gpos_label2 = gpos_label - sign(x_shift_label) *.5*10^7, window = str_c(&#39;bold(&#39;,project_case(&#39;a&#39;),&#39;):joint~italic(F[ST])&#39;)) outlier_y &lt;- .45 outlier_yend &lt;- .475 9.2.3 Plotting Finally it is time to put the pieces together with one giant ggplot(): p_done &lt;- ggplot()+ # add lg indication in the backgroud geom_hypo_LG()+ # add fst outlier window indication in the background geom_vline(data = outliers, aes(xintercept = gpos), color = outlr_clr)+ # add outlier label flags geom_segment(data = outlier_label, aes(x = gpos, xend = gpos_label2, y = outlier_y, yend = outlier_yend), color = alpha(outlr_clr,1),size = .2)+ # add outlier labels geom_text(data = outlier_label, aes(x = gpos_label, y = outlier_yend, label = label), color = alpha(outlr_clr,1), fontface = &#39;bold&#39;)+ # add fst, delta dxy and gxp data geom_point(data = data, aes(x = GPOS, y = value),size = plot_size, color = plot_clr) + # add dxy data geom_point(data = dxy_select,aes(x= GPOS, y = dxy),size = plot_size, color = plot_clr)+ # add pi data geom_point(data = pi_data_select, aes(x = GPOS, y = mean_pi),size = plot_size, color = plot_clr) + # add recombination data (points) geom_point(data = recombination_data, aes(x = GPOS, y = RHO),size = plot_size, color = plot_clr) + # add recombination data (smoothed) geom_smooth(data = recombination_data, aes(x = GPOS, y = RHO, group = CHROM), color = &#39;red&#39;, se = FALSE, size = .7) + # add topology weighting data geom_line(data = twisst_data, aes(x = GPOS, y = weight, color = topo_rel),size = .4) + # add topology weighting &quot;null expectation&quot; geom_hline(data = twisst_null,aes(yintercept = weight), color = rgb(1,1,1,.5), size=.4) + # color scheme lg indication scale_fill_hypo_LG_bg() + # layout x ayis scale_x_hypo_LG()+ # color scheme topology weighting scale_color_gradient( low = &quot;#f0a830ff&quot;, high = &quot;#084082ff&quot;, guide = FALSE)+ # facetting to separate the different stats facet_grid(window~.,scales = &#39;free&#39;,switch = &#39;y&#39;, labeller = label_parsed)+ # tune plot appreance theme_hypo()+ theme(legend.position = &#39;bottom&#39;, axis.title = element_blank(), strip.background = element_blank(), strip.placement = &#39;outside&#39;) hypo_save(p_done, filename = &#39;figures/F2.png&#39;, width = 297*.95, height = 275*.95, units = &#39;mm&#39;, comment = plot_comment) "],
["figure-3.html", "10 Figure 3 10.1 Summary 10.2 Details of plot_F3.R", " 10 Figure 3 10.1 Summary This is the accessory documentation of Figure 3. The Figure can be recreated by running the R script plot_F3.R: cd $BASE_DIR Rscript --vanilla R/fig/plot_F3.R \\ 2_analysis/twisst/weights/ ressources/plugin/trees/ \\ https://raw.githubusercontent.com/simonhmartin/twisst/master/plot_twisst.R \\ 2_analysis/summaries/fst_outliers_998.tsv 2_analysis/dxy/50k/ \\ 2_analysis/fst/50k/ 2_analysis/summaries/fst_globals.txt \\ 2_analysis/GxP/50000/ 200 5 10.2 Details of plot_F3.R In the following, the individual steps of the R script are documented. It is an executable R script that depends on the accessory R package GenomicOriginsScripts. 10.2.1 Config The scripts start with a header that contains copy &amp; paste templates to execute or debug the script: #!/usr/bin/env Rscript # run from terminal: # Rscript --vanilla R/fig/plot_F3.R \\ # 2_analysis/twisst/weights/ ressources/plugin/trees/ \\ # https://raw.githubusercontent.com/simonhmartin/twisst/master/plot_twisst.R \\ # 2_analysis/summaries/fst_outliers_998.tsv 2_analysis/dxy/50k/ \\ # 2_analysis/fst/50k/ 2_analysis/summaries/fst_globals.txt \\ # 2_analysis/GxP/50000/ 200 5 # =============================================================== # This script produces Figure 3 of the study &quot;The genomic origins of a marine radiation&quot; # by Hench, McMillan an Puebla # --------------------------------------------------------------- # =============================================================== # args &lt;- c(&#39;2_analysis/twisst/weights/&#39;, &#39;ressources/plugin/trees/&#39;, # &#39;https://raw.githubusercontent.com/simonhmartin/twisst/master/plot_twisst.R&#39;, # &#39;2_analysis/summaries/fst_outliers_998.tsv&#39;, # &#39;2_analysis/dxy/50k/&#39;, &#39;2_analysis/fst/50k/&#39;, # &#39;2_analysis/summaries/fst_globals.txt&#39;, # &#39;2_analysis/GxP/50000/&#39;, 200, 5) The next section processes the input from the command line. It stores the arguments in the vector args. The R package GenomicOriginsScripts is loaded and the script name and the current working directory are stored inside variables (script_name, plot_comment). This information will later be written into the meta data of the figure to help us tracing back the scripts that created the figures in the future. Then we drop all the imported information besides the arguments following the script name and print the information to the terminal. args &lt;- commandArgs(trailingOnly=FALSE) # setup ----------------------- library(GenomicOriginsScripts) library(hypoimg) library(furrr) cat(&#39;\\n&#39;) script_name &lt;- args[5] %&gt;% str_remove(.,&#39;--file=&#39;) plot_comment &lt;- script_name %&gt;% str_c(&#39;mother-script = &#39;,getwd(),&#39;/&#39;,.) args &lt;- process_input(script_name, args) ── Script: scripts/plot_F3.R ──────────────────────────────────────────── Parameters read: ★ 1: figures/data/twisst_weights/ ★ 2: figures/data/twisst_positions/ ★ 3: https://raw.githubusercontent.com/simonhmartin/twisst/master/plot_twisst.R ★ 4: figures/data/summaries/all_multi_fst_outliers_998.tsv ★ 5: figures/data/dxy/ ★ 6: figures/data/fst/ ★ 7: figures/data/summaries/fst_globals.txt ★ 8: figures/data/GxP/ ★ 9: 200 ─────────────────────────────────────────── /current/working/directory ── The directories for the different data types are received and stored in respective variables. Also, we source an external r script from the original twisst github repository that we need to import the twisst data: # config ----------------------- w_path &lt;- as.character(args[1]) d_path &lt;- as.character(args[2]) twisst_functions &lt;- as.character(args[3]) out_table &lt;- as.character(args[4]) dxy_dir &lt;- as.character(args[5]) fst_dir &lt;- as.character(args[6]) fst_globals &lt;- as.character(args[7]) gxp_dir &lt;- as.character(args[8]) twisst_size &lt;- as.numeric(args[9]) source(twisst_functions, local = TRUE) plan(multiprocess) The we define a buffer width. This is the extent left and right of the \\(F_{ST}\\) outlier windows that is included in the plots. We also load the R packages ape and igraph that will help us workung with phylogenetic objects (the twisst topologies). window_buffer &lt;- 2.5*10^5 #------------------- library(ape) library(igraph) 10.2.2 Data import Then, we start with the data import. For the figure we are going to need: \\(d_{XY}\\) data genotype \\(\\times\\) phenotype data \\(F_{ST}\\) data topology weighing data the positions of the genome annotations the positions of the \\(F_{ST}\\) outlier windows We start by importing \\(d_{XY}\\) by first listing all \\(d_{XY}\\) data files and then iterating the \\(d_{XY}\\) import function over the files. # actual script ========================================================= dxy_files &lt;- dir(dxy_dir, pattern = str_c(&#39;dxy.*[a-z]{3}.*.50kb-5kb.tsv.gz&#39;)) dxy_data &lt;- tibble(file = str_c(dxy_dir, dxy_files)) %&gt;% purrr::pmap(get_dxy) %&gt;% bind_rows() Next we iterate the genotype \\(\\times\\) phenotype import function over the trait names Bars, Snout and Peduncle. gxp_traits &lt;- c(&#39;Bars&#39;, &#39;Snout&#39;, &#39;Peduncle&#39;) gxp_data &lt;- str_c(gxp_dir,gxp_traits,&#39;.lm.50k.5k.txt.gz&#39;) %&gt;% future_map(get_gxp_long) %&gt;% bind_rows() Then, we define two sets of colors - one for the topology highlighting schemes and one for the traits of the genotype \\(\\times\\) phenotype association. twisst_clr &lt;- c(Blue = &quot;#0140E5&quot;, Bars = &quot;#E32210&quot;, Butter = &quot;#E4E42E&quot;) gxp_clr &lt;- c(Bars = &quot;#79009f&quot;, Snout = &quot;#E48A00&quot;, Peduncle = &quot;#5B9E2D&quot;) %&gt;% darken(factor = .95) %&gt;% set_names(., nm = gxp_traits) Next, we compute the average genome wide \\(d_{XY}\\) and load the average genome wide \\(F_{ST}\\) values for all 28 pair wise species comparisons. dxy_globals &lt;- dxy_data %&gt;% filter(BIN_START %% 50000 == 1 ) %&gt;% group_by( run ) %&gt;% summarise(mean_global_dxy = sum(dxy*N_SITES)/sum(N_SITES)) %&gt;% mutate(run = fct_reorder(run,mean_global_dxy)) fst_globals &lt;- vroom::vroom(fst_globals,delim = &#39;\\t&#39;, col_names = c(&#39;loc&#39;,&#39;run_prep&#39;,&#39;mean_fst&#39;,&#39;weighted_fst&#39;)) %&gt;% separate(run_prep,into = c(&#39;pop1&#39;,&#39;pop2&#39;),sep = &#39;-&#39;) %&gt;% mutate(run = str_c(pop1,loc,&#39;-&#39;,pop2,loc), run = fct_reorder(run,weighted_fst)) After this, we import \\(F_{ST}\\) by first listing all \\(F_{ST}\\) data files and then iterating the \\(F_{ST}\\) import function over the files. fst_files &lt;- dir(fst_dir ,pattern = &#39;.50k.windowed.weir.fst.gz&#39;) fst_data &lt;- str_c(fst_dir,fst_files) %&gt;% future_map(get_fst) %&gt;% bind_rows()%&gt;% left_join(dxy_globals) %&gt;% left_join(fst_globals) %&gt;% mutate(run = refactor(., fst_globals), BIN_MID = (BIN_START+BIN_END)/2) We then add the genome wide averages of \\(F_{ST}\\) and \\(d_{XY}\\) as new columns to the \\(d_{XY}\\) data. This will be used later for coloring the \\(d_{XY}\\) panel. dxy_data &lt;- dxy_data %&gt;% left_join(dxy_globals) %&gt;% left_join(fst_globals) %&gt;% mutate(run = refactor(dxy_data, fst_globals), window = &#39;bold(italic(d[xy]))&#39;) Then, we summarise the \\(d_{XY}\\) data to compute \\(\\Delta d_{XY}\\). data_dxy_summary &lt;- dxy_data %&gt;% group_by(GPOS) %&gt;% summarise(scaffold = CHROM[1], start = BIN_START[1], end = BIN_END[1], mid = BIN_MID[1], min_dxy = min(dxy), max_dxy = max(dxy), mean_dxy = mean(dxy), median_dxy = median(dxy), sd_dxy = sd(dxy), delta_dxy = max(dxy)-min(dxy)) To only load the relevant twisst data, we first load the positions of the \\(F_{ST}\\) outlier regions. We also define a set of outliers of interest. # twisst part ------------------ outlier_table &lt;- vroom::vroom(out_table, delim = &#39;\\t&#39;) %&gt;% setNames(., nm = c(&quot;outlier_id&quot;,&quot;lg&quot;, &quot;start&quot;, &quot;end&quot;, &quot;gstart&quot;,&quot;gend&quot;,&quot;gpos&quot;)) ## Rows: 18 ## Cols: 7 ## chr [2]: gid, chrom ## dbl [5]: start, end, gstart, gend, gpos ## ## Call `spec()` for a copy-pastable column specification ## Specify the column types with `col_types` to quiet this message outlier_pick = c(&#39;LG04_1&#39;, &#39;LG12_2&#39;, &#39;LG12_3&#39;) Then we define a set of genes of interest. These are the ones, that later will be labelled in the annotation panel. cool_genes &lt;- c(&#39;arl3&#39;,&#39;kif16b&#39;,&#39;cdx1&#39;,&#39;hmcn2&#39;, &#39;sox10&#39;,&#39;smarca4&#39;, &#39;rorb&#39;, &#39;alox12b&#39;,&#39;egr1&#39;, &#39;ube4b&#39;,&#39;casz1&#39;, &#39;hoxc8a&#39;,&#39;hoxc9&#39;,&#39;hoxc10a&#39;,#&#39;hoxc6a&#39;, #&#39;hoxc8a&#39;, &#39;hoxc13a&#39;,&#39;rarga&#39;,&#39;rarg&#39;, &#39;snai1&#39;,&#39;fam83d&#39;,&#39;mafb&#39;,&#39;sws2abeta&#39;,&#39;sws2aalpha&#39;,&#39;sws2b&#39;,&#39;lws&#39;,&#39;grm8&#39;) Next, we load the twisst data for both locations and list all species from Belize (This will be needed to calculate their pair wise distances for the topology highlighting). data_tables &lt;- list(bel = prep_data(loc = &#39;bel&#39;), hon = prep_data(loc = &#39;hon&#39;)) ## Rows: 1,195 ## Cols: 15 ## dbl [15]: topo1, topo2, topo3, topo4, topo5, topo6, topo7, topo8, topo9, topo10, topo... ## ## Call `spec()` for a copy-pastable column specification ## Specify the column types with `col_types` to quiet this message ## Rows: 1,195 ## Cols: 6 ## chr [1]: scaffold ## dbl [5]: start, end, mid, sites, lnL ## ## Call `spec()` for a copy-pastable column specification ## Specify the column types with `col_types` to quiet this message ## Rows: 1,080 ## Cols: 15 ## dbl [15]: topo1, topo2, topo3, topo4, topo5, topo6, topo7, topo8, topo9, topo10, topo... ## ## Call `spec()` for a copy-pastable column specification ## Specify the column types with `col_types` to quiet this message ## Rows: 1,080 ## Cols: 6 ## chr [1]: scaffold ## dbl [5]: start, end, mid, sites, lnL ## ## Call `spec()` for a copy-pastable column specification ## Specify the column types with `col_types` to quiet this message ## Rows: 1,269 ## Cols: 105 ## dbl [105]: topo1, topo2, topo3, topo4, topo5, topo6, topo7, topo8, topo9, topo10, topo1... ## ## Call `spec()` for a copy-pastable column specification ## Specify the column types with `col_types` to quiet this message ## Rows: 1,269 ## Cols: 6 ## chr [1]: scaffold ## dbl [5]: start, end, mid, sites, lnL ## ## Call `spec()` for a copy-pastable column specification ## Specify the column types with `col_types` to quiet this message ## Rows: 1,141 ## Cols: 105 ## dbl [105]: topo1, topo2, topo3, topo4, topo5, topo6, topo7, topo8, topo9, topo10, topo1... ## ## Call `spec()` for a copy-pastable column specification ## Specify the column types with `col_types` to quiet this message ## Rows: 1,141 ## Cols: 6 ## chr [1]: scaffold ## dbl [5]: start, end, mid, sites, lnL ## ## Call `spec()` for a copy-pastable column specification ## Specify the column types with `col_types` to quiet this message pops_bel &lt;- c(&#39;ind&#39;,&#39;may&#39;,&#39;nig&#39;,&#39;pue&#39;,&#39;uni&#39;) 10.2.3 Plotting As a last step before the actual plotting, we are defing a list of outliers to be included within the final plots. neighbour_tibbles &lt;- tibble(outlier_id = outlier_pick, loc = c(rep(&#39;bel&#39;,3)), label = c(&#39;A&#39;,&#39;B&#39;,&#39;C&#39;)) Then, we iterate the main plotting function over all selected \\(F_{ST}\\) outlier windows and combine the resulting plots into a multi panel plot. p_single &lt;- outlier_table %&gt;% filter(outlier_id %in% outlier_pick) %&gt;% left_join(neighbour_tibbles) %&gt;% mutate(outlier_nr = row_number(), text = ifelse(outlier_nr == 1,TRUE,FALSE)) %&gt;% pmap(plot_curtain, cool_genes = cool_genes) %&gt;% cowplot::plot_grid(plotlist = ., nrow = 1, labels = letters[1:length(outlier_pick)] %&gt;% project_case()) At this point all that we miss is the figure legend. So, for the \\(F_{ST}\\), \\(d_{XY}\\) and genotype \\(\\times\\) phenotype color shemes we create two dummy plots from where we can export the legends. We combine those two classical color legends into what will become the left column of the legend. p_dummy_fst &lt;- outlier_table %&gt;% filter(row_number() == 1) %&gt;% purrr::pmap(plot_panel_fst) %&gt;% .[[1]] p_dummy_gxp &lt;- outlier_table %&gt;% filter(row_number() == 1) %&gt;% purrr::pmap(plot_panel_gxp) %&gt;% .[[1]] p_leg_fst &lt;- (p_dummy_fst+theme(legend.position = &#39;bottom&#39;)) %&gt;% get_legend() p_leg_gxp &lt;- (p_dummy_gxp+theme(legend.position = &#39;bottom&#39;)) %&gt;% get_legend() p_leg1 &lt;- cowplot::plot_grid(p_leg_fst,p_leg_gxp, ncol = 1) Then, we construct the topology highlighting color legend. We first define the three higlighting scenarios, the involved species and their base color and then iterate the legend plotting functions over those configurations. The resulting legend elements are then combined to create the right side of the figure legend and the two main legend elements are combined. p_leg2 &lt;- tibble(spec1 = c(&#39;indigo&#39;, &#39;indigo&#39;,&#39;unicolor&#39;), spec2 = c(&#39;maya&#39;, &#39;puella&#39;,NA), color = twisst_clr %&gt;% unname() %&gt;% darken(.,factor = .8), mode = c(rep(&#39;pair&#39;,2),&#39;isolation&#39;)) %&gt;% future_pmap(plot_leg) %&gt;% cowplot::plot_grid(plotlist = ., nrow = 1) p_leg &lt;- cowplot::plot_grid(p_leg1, p_leg2,nrow = 1, rel_widths = c(.6, 1)) After adding the legend to the main part, Figure 3 is done. p_done &lt;- cowplot::plot_grid(p_single, p_leg,ncol = 1, rel_heights = c(1, .2)) Finally, we can export Figure 3. hypo_save(plot = p_done, filename = &#39;figures/F3.pdf&#39;, width = 14, height = 11.2, comment = script_name, device = cairo_pdf) "]
]
