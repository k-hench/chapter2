[
["index.html", "Script repository (Hench et al. supplement) 1 Intro 1.1 Analysis 1.2 Prerequesites 1.3 Figures", " Script repository (Hench et al. supplement) Kosmas Hench 2020-06-16 1 Intro This repository contains the complete workflow used in the paper “Ancestral variation, hybridization and modularity fuel a marine radiation”. The individual chapters of this documentation follow the separate main steps of the workflow, which each refer to an individual prefix in the git x.x references of the papers method section. The individual steps partly depend on each other - especially git 1 - git 3 should be executed in order and before the other steps. 1.1 Analysis A documentation of the data preparation and the data analysis (git 1.x - 9.x) can be found at: git 1.x: Genotyping git 2.x: Genotyping all base pairs git 3.x: Analysis (FST &amp; GxP) git 4.x: Analysis (dXY &amp; \\(\\pi\\)) git 5.x: Analysis (phylogeny &amp; topolgy weighting) git 6.x: Analysis (\\(\\rho\\)) git 7.x: Analysis (HO ) git 8.x: Analysis (demographic history) git 10.x: Analysis (Stankowski et al. 2019) 1.2 Prerequesites All scripts assume two variables to be set within the bash environment: $BASE_DIR is assumed to point to the base folder of this repository $SFTWR is a folder that contains all the software dependencies that are used within the scripts Furthermore, external dependencies need to be downloaded and deployed at the expected places (s. README.md at the ressources folder). 1.3 Figures The creation of the figures is bundled in a single script (git 11) which can be executed once all nextflow scripts have successfully run. cd $BASE_DIR bash sh/create_figures.sh This is basically just a collection that will run all scripts located under $BASE_DIR/R/fig. Under this location, you will find one R script per figure (and suppl. figure). So if you are only interested in a single figure - that is the place to start looking. Furthermore, a more detailed documentation exists for all the figure scripts used for the manuscript: F1, F2, F3 &amp; F4 as well as for all the supplementary figures: SF1, SF2, SF3, SF4, SF5, SF6, SF7, SF8 and SF9. "],
["genotyping-i-snps-only.html", "2 Genotyping I (SNPs only) 2.1 Summary 2.2 Details of genotyping.nf", " 2 Genotyping I (SNPs only) 2.1 Summary The genotyping procedure is controlled by the nextflow script genotyping.nf (located under $BASE_DIR/nf/genotyping/). It takes the analysis from the raw sequencing data to the genotyped and phased SNPs. Below is an overview of the steps involved in the genotyping process. (The green dot indicates the raw data input, red arrows depict output that is exported for further use.) 2.2 Details of genotyping.nf 2.2.1 Data preparation The nextflow script starts with a small header and then opens the analysis by reading a table with meta data about the samples. The table is parsed and the values are stored in nextflow variables. Below is a little preview of the table containing the sample meta data: id label spec geo date coord_N coord_W company .. 16_21-30 16_21-30nigpan nig pan 2016 NA NA duke … 16_21-30 16_21-30nigpan nig pan 2016 NA NA duke … 16_31-40 16_31-40unipan uni pan 2016 NA NA duke … 16_31-40 16_31-40unipan uni pan 2016 NA NA duke … 17996 17996indbel ind bel 2004-07-27 16.801 -88.079 novogene … 17997 17997indbel ind bel 2004-07-27 16.801 -88.079 novogene … … … … … … … … … The first step to prepare the data for the GATK best practices, is to convert the sample sequences from *.fq to *.bam format to assign read groups: The second step is marking the Illumina adapters. We need to pass on the unaligned .bam file and the file containing the adapter information together, so the output of the first two processes are matched by the combined sample and sequencing lane information. For the actual mapping, the sequences are transformed back into .fq format, aligned using bwa and merged back with their original read group information. Next, the duplicates are being marked. As a preparation for the actual genotyping, the .bam files are being indexed. At this point the preparation of the sequencing is done and we can start with the genotyping. (The output of the data preparation is split and one copy is later also used to prepare the read aware phasing in the process called extractPirs.) 2.2.2 Genotying Since some of our samples were split over several lanes, we now need to collect all .bam files for each sample. Now, we can create the genotype likelihoods for each individual sample. The individual genotype likelihoods are collected and combined for the entire data set. All samples are jointly genotyped. The output of this process is split and used to collect the genotype metrics to inform the hard filtering of SNPs and to pass on the genotypes to the process called filterSNPs. At this point we create a channel containing all 24 hamlet linkage groups (LGs). This is used later (in the process called extractPirs) since all LGs are phased separately and only located at this part of the script for historical reasons (sorry :/). The metrics of the raw genotypes are collected. Based on the thresholds derived from the genotype metrics, the genotypes are first tagged and then filtered. After this, the data is filtered for missingness and only bi-allelic SNPs are selected. At this point, the genotying is done. 2.2.3 Phasing To get from genotypes to haplotypes, we apply read-aware phasing using Shapeit. This takes the original sequencing reads into account, so in the first step the reads are screened for Phase Informative Reads (i.e. reads containing more than a single SNP). This needs to be done for each LG independently, so we first need to split the genotypes before running extractPIRs. Using those PIRs, we can then proceed with the actual phasing. The resulting haplotypes are converted back into .vcf format. After the phasing, we merge the LGs back together to get a single data set. We export a comple data set as well as one that was filtered for a minor allele count of at least two. Finally, we are done with the entire genotyping procedure for the SNPs olny data set. 2.2.4 Indel masks The genotyping.nf workflow contains an appendix that makes use of the genotyping likelihoods created in step git 1.10 to create an indel mask that is later used in the inference of the hamlet demographic history (git 8.x). (This part is excluded from the initial visualization of this script) We restart by reopening the joint-sample genotype likelyhoods file and calling the indels from it. We export the the indel genotype metrics to determine cutoff values for the hard filtering step. Based on the exported metrics the genotypes are being filtered. Since we need one indel mask per linkage group, we create a channel of LGs. The linkage group channel is combined with the filterd indels. Finally, one mask per linkage group is created from the indel positions. All indel masks are exported to the resources folder within the root directory. "],
["genotyping-ii-all-callable-sites.html", "3 Genotyping II (all callable sites) 3.1 Summary 3.2 Details of genotyping_all_basepairs.nf", " 3 Genotyping II (all callable sites) 3.1 Summary The genotyping procedure is controlled by the nextflow script genotyping_all_basepairs.nf (located under $BASE_DIR/nf/genotyping_all_basepairs/). Based on an intermediate step from genotyping.nf, this script produces a data set that includes all callable sites - that is SNPs as well a invariant sites that are covered by sequence. Below is an overview of the steps involved in the genotyping process. (The green dot indicates the data input, red arrows depict output that is exported for further use.) 3.2 Details of genotyping_all_basepairs.nf 3.2.1 Data preparation The nextflow script starts with a small header and then imports the joint genotyping likelihoods for all samples produced by genotyping_all_basepairs.nf. The genotyping of the different linkage groups is going to happen in parallel, so we need to initialize a channel for the 24 LGs. The genotyping likelihoods are combined, effectively linking the data set to the 24 parallel LGs. The samples are jointly genotyped, independently for each LG and including invariant sites. The genotypes of the different LGs are merged. The genotypes are hard filtered based on various genotyping scores. A second filtering is based on the missingness of samples. Finally, we are done with the second version of genotyping. "],
["analysis-i-fst-gxp.html", "4 Analysis I (FST &amp; GxP) 4.1 Summary 4.2 Details of analysis_fst_gxp.nf", " 4 Analysis I (FST &amp; GxP) 4.1 Summary The genetic differentiation, as well as the genotype x phenotype association, are computed within the nextflow script analysis_fst_gxp.nf (located under $BASE_DIR/nf/analysis_fst_gxp/). It takes the SNPs only data set and computes FST and the GxP association. Below is an overview of the steps involved in the analysis. (The green dot indicates the genotype input, red arrows depict output that is exported for further use.) 4.2 Details of analysis_fst_gxp.nf 4.2.1 Setup The nextflow script starts by opening the genotype data and feeding it into three different streams. Since we are going to work on the three sampling locations independently, we create channel for the locations. Then we attach the genotypes to the locations. Next, we define the species sets sampled at the individual locations. For each location, the data is subset to include only local hamlets. As we also want to compute global statistics, we create another subset including all sampled hamlets but excluding the outgroups. 4.2.1 FST Using this subset, we compute the global differentiation among all hamlet populations. Then, we set up the pair wise species comparisons… … and run them. The genome wide summaries are compiled from the log files of VCFtools. 4.2.1 GxP The software use for the GxP association uses genotypes in plink file format, so the first step was to convert the input. Additionally to the genotypes, the phenotypes of the samples are needed. For historic reasons, we perform a PCA on the phenotypes and report the extended phenotype data including the scoring on PC1 &amp; PC2. We set up all the traits for which we want to perform a GxP association. Then, we combine the reformatted genotypes with the phenotyes and the traits of interest. Having collected all the input, we can now run the GxP. To smooth the GxP results, we initialize two resolutions (50 kb windows with 5 kb increments and 10 kb windows with 1 kb increments). The we apply all smoothing levels to the raw GxP output… .. and run the smoothing. At this step we are done with differentiation and GxP. "],
["analysis-ii-dxy-pi.html", "5 Analysis II (dXY &amp; pi) 5.1 Summary 5.2 Details of analysis_dxy.nf", " 5 Analysis II (dXY &amp; pi) 5.1 Summary The genetic divergence, as well diversity, are computed within the nextflow script analysis_dxy.nf (located under $BASE_DIR/nf/analysis_dxy/). It takes the all BP data set and computes dXY and \\(\\pi\\). Below is an overview of the steps involved in the analysis. (The green dot indicates the genotype input, red arrows depict output that is exported for further use.) 5.2 Details of analysis_dxy.nf 5.2.1 Data preparation The nextflow script starts by opening the genotype data and feeding it into three different streams (one for dXY and one for \\(\\pi\\)). The computation of dXY is split by linkage group, so we need to initialize a channel for the LGs. Now, we can subset the data set and convert it to a custom genotype format. Since the species composition differs between locations, we need to initialize three separate sets of species. These are going to be used to create the divergence species pairs. For the diversity, we also initialize the full set of populations of the study. We want to run a sliding window at different resolutions, so we set up a channel for these. 5.2.1 dXY To prepare all species comparisons used to estimate divergence, we combine each species witch all other species within a location. Now we attach the genotypes and the resolution level to the species pairs. At this point, we can calculate dXY. Since the calculation was split across LGs, we now need to collect all LGs of a particular species pair… … and merge the results. For control, we also create a “random” dXY run, where we take the most diverged species pair and randomize the population assignment of the samples. Therefore, we first pick the most diverged species pair. Then we set the sliding window resolution. Now, we randomize the population assignment of the samples and calculate differentiation. We set up another channel to prepare the random dXY… .. and compute the divergence. Again, we collect the output of the individual LGs…. … and we merge them 5.2.1 \\(\\pi\\) Estimating the diversity is quite straight forward: We take the prepared, population identifier, the data set and the window resolutions and run VCFtools on each combination. At this step we are done with divergence and diversity. "],
["analysis-iii-phylogeny-topology-weighting.html", "6 Analysis III (phylogeny &amp; topology weighting) 6.1 Summary 6.2 Details of analysis_fasttree_twisst.nf", " 6 Analysis III (phylogeny &amp; topology weighting) 6.1 Summary The whole genome phylogeny and the topology weighting are prepared within the nextflow script analysis_fasttree_twisst.nf (located under $BASE_DIR/nf/analysis_fasttree_twisst/), which runs on the SNPs only data set. Below is an overview of the steps involved in the process. (The green dot indicates the genotype input, red arrows depict output that is exported for further use.) 6.2 Details of analysis_fasttree_twisst.nf 6.2.1 Setup The nextflow script starts by opening the genotype data and feeding it into two different streams (one for the phylogeny and one for topology weighting). 6.2.1 Whole genome phylogeny During data exploration, various subsets of the data set were investigated, including the different sampling locations, whole genome vs. non-outlier regions and all samples vs. hamlets only. (Now, most of the options have been muted by commenting these options out.) Here, we set up a channel managing the subset by location. This channel toggles the inclusion of FST outlier regions. This channel toggles the inclusion of the non-hamlet outgroup. We combine the different selector channels to create all possible combinations of the settings. To prepare the input for the phylogeny, the fine tuned selection is applied to subset the genotypes. The subset is converted to fasta format, creating two whole genome pseudo haplotypes per sample. Then, the phylogeny is reconstructed based on the converted geneotypes. 6.2.1 Topology weighting The complexity of topology weighting increases non-linearely with the number of included populations as the number of possible unrooted topologies skyrockets. The maximum number of possible populations allowed within twisst is eight, so we are running the topology weighting for Belize and Honduras independently (for the three species at Panama only one unrooted topology is possible, so we don’t run twisst here). We start by setting up a channel for the sampling location. Then, we attach the genotypes to the location. The analysis is split by linkage group, so we need to initialize the LGs. The data is subset to include only the samples of the respective location. While running twisst, we ran into an issues regarding incompatibilities of the used scheduling system used on our computing cluster and the threading within the python scripts used in the preparation of twisst. Unfortunately, this prevented us running the preparation in place (as part of the nextflow script). Instead, we rand the preparation separately on a local computer and clumsily plugged the results into the nextflow process. Below we include the originally intended workflow which we muted so that the script is runnable using the plug-in approach. Still, the the original workflow describes the steps executed locally and conveys the intermediate steps more clearly. Also, on a different computer cluster, the original script should work alright. The next step is to attach the LGs to the genotype subsets. Based on the LGs the genotypes are split. We initialize the window size size (as SNPs) used for the topology weighting… …and attach them to the genotypes. To conduct topology weighting, we need some underlying phylogenies, so we run PhyML along the sliding window. The last step then is to run twisst prepared phylogenies. Here, the plug in approach picks up. At this point we have run PhyML locally and deposited the results under $BASE_DIR/ressources/plugin/trees. To restart, we need to emulate the settings needed for the twisst process. Then, we feed the phylogenies from an external directory into twisst. At this step we are done with phylogeny and the topology weighting. "],
["analysis-iv-rho.html", "7 Analysis IV (rho) 7.1 Summary 7.2 Details of analysis_recombination.nf", " 7 Analysis IV (rho) 7.1 Summary The population recombination rate is estimated within the nextflow script analysis_recombination.nf (located under $BASE_DIR/nf/analysis_recombination/), which runs on the SNPs only data set. Below is an overview of the steps involved in the analysis. (The green dot indicates the genotype input, red arrows depict output that is exported for further use.) 7.2 Details of analysis_recombination.nf 7.2.1 Data preparation The nextflow script starts by opening the genotype data. The estimation of the population recombination rate using FastEPRR happens in three steps. The first step is run independently for all linkage groups, so we set up a channel for the LGs. To prepare the input, the genotypes are split by LG. The prepared data is then fed to the first step of FastEPRR. Since nextflow manages the results of its processes in a complex file structure, we need to collect all results of step 1 and bundle them before proceeding. The second step of FastEPRR is parallelized over an arbitrary number of sub-processes. Here, we initialize 250 parallel processes and combine the parallelization index with the results from step 1. Taking this prepared bundle, we now can start the second step of FastEPRR. The file management of nextflow can be a bit complicated at times, so here duplicate the output of step 2 to later easily access the parallelization indices and the actual output. We collect both clones of the step 2 results and bundle the results in a single directory. Then we feed the bundled results into the third step of FastEPRR. To ease the usage of the FastEPRR results downstream, we reformat them and compile a tidy table. Finally, we are done with recombination rate. "],
["analysis-v-ho.html", "8 Analysis V (HO) 8.1 Summary 8.2 Details of analysis_het.nf", " 8 Analysis V (HO) 8.1 Summary The population recombination rate is estimated within the nextflow script analysis_het.nf (located under $BASE_DIR/nf/analysis_het/), which runs on the all BP data set. Below is an overview of the steps involved in the analysis. (The green dot indicates the genotype input, red arrows depict output that is exported for further use.) 8.2 Details of analysis_het.nf 8.2.1 Data preparation The nextflow script starts by opening the genotype data. Here we actually just do this to get a inventory of all genotyped samples - because of this we use the phased version (because it is the smaller file with the same samples) even though we are going to use the all BP data set to actually estimate the heterozygosity. The sample names are extracted from the genotype file and are fed into a channel. The all BP data set is split by sample, the allele counts for the sample are extracted and converted into a custom format. The heterozygosity is computed along non-operlapping 50 kb windows. Finally, we are done with heterozygosity. "],
["analysis-vi-demographic-history.html", "9 Analysis VI (Demographic History) 9.1 Summary 9.2 Details of analysis_msmc.nf", " 9 Analysis VI (Demographic History) 9.1 Summary The demographic history rate is inferred within the nextflow script analysis_msmc.nf (located under $BASE_DIR/nf/analysis_msmc/), which runs on the SNPs only data set. Below is an overview of the steps involved in the inference. (The green dot indicates the genotype input, red arrows depict output that is exported for further use.) 9.2 Details of analysis_msmc.nf 9.2.1 Data preparation The first part of the scripts includes a large block of preparation work. In this initial block, the data masks are being generated based on the samples coverage statistics combined with the locations of idels and the reference genomes mappability. The whole script is opened by a creating a channel for the linkage groups since the coverage statistics are being created on a linkage group basis. Then the phased genotype data is opened (for later use in msmc). To extract the sequencing depth for each individual, the unphased genotypes are opened as well (as this information is lost during phasing). The outgroups are removed from the data set and the depth is reported for each individual. The depth information is fed into a channel so that the information is accessible for nextflow. Next, a channel is created from all the original .bam files from the mapped sequences (git 1.6). The previously created depth information is attached to the bam channel… … and the genotype data and individual linkage groups are added. Now, the data is split by individual and all the additional information is passed on to the masking (git 8.10) as well as to the production of the the individuals segregating sites (git 8.11). The individual coverage statistics are then being queried using the individuals average depth to create the coverage mask for each individual. For each individual, the segregating sites are created. 9.2.1 Grouping of individuals At this point, the data masks are prepared and the samples can be assigned to their respective groups for their demographic history and and cross-coalescence rate inference. The results of the random assignment are being fed into a channel. Since this script uses an unorthodox way of making the results of the data preparation available to all following processes by exporting them back to the root folder, the following dummy process is installed to wait for the data preparation to finish before proceeding with the workflow. To set up the msmc2 runs, the sample grouping is waiting for the dummy process to finish. Then, the specific msmc2 input files are compiled from the combined masks of the involved samples (for each linkage group individually). The input files of all linkage group are collected for each sample grouping. And finally msmc2 is executed to infer the demographic history of the involved samples. The process cross-coalescence rate is similar (with git 8.19 being the equivalent of git 8.13). So, again the grouping information in fed into a channel. The groups wait for the data preparation to finish. The msmc2 input files are being compiled based on the involved samples (for each linkage group). The input files of all linkage group are collected for each sample grouping. And msmc2 is executed to infer the cross-coalescence rate of the involved samples. Finally, we are done with the inference of the demographic history. "],
["analysis-vii-mokeyflowers.html", "10 Analysis VII (Mokeyflowers) 10.1 Summary 10.2 Details of analysis_stankowski_etal_2019.nf", " 10 Analysis VII (Mokeyflowers) 10.1 Summary The genome wide average differentiation of the study by Stankowski et al. (2019) is computed within the nextflow script analysis_stankowski_etal_2019.nf (located under $BASE_DIR/nf/analysis_stankowski_etal_2019/). (The green dot indicates the genotype input, red arrows depict output that is exported for further use.) 10.2 Details of analysis_stankowski_etal_2019.nf 10.2.1 Data import The nextflow script starts by opening the genotype data and the assignment of samples to populations. Next, we crate an inventory of all populations within the genotype file and create a cross of all possible combinations. For every species pair, we compute the genome wide differentiation using VCFtools. Finally we collect all VCFtools log files, extract the genome wide average FST value and compile a summary table. The summary table can now be used to create supplementary Figure 1. "]
]
